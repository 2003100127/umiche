{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome_to_the_umiche_site","title":"Welcome to the UMIche site!","text":"<p>A comprehensive platform for UMI-associated analysis.</p> <p>Features</p> <ul> <li> UMI method repertoire - comprehensive methods for UMI deduplication</li> <li> UMI deduplication pipeline - pipelines for UMI deduplication effects in demultiplexing</li> <li> UMI count simulation - train and simulate scRNA-seq UMI count matrix</li> <li> UMI visualisation - graph representation and plots of UMI deduplication-related data</li> </ul>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>umiche.graph</code> - UMI graph with nodes and edges</li> <li><code>umiche.dedup</code> - UMI deduplication methods</li> <li><code>umiche.homotrimer</code> - Collapsing or splitting homotrimer UMIs</li> <li><code>umiche.trim</code> - Extraction of UMIs and other read components</li> <li><code>umiche.io</code> - File I/O </li> <li><code>umiche.mat</code> - UMI count matrix simulation</li> <li><code>umiche.pipeline</code> - UMI deduplication pipeline one-stop</li> <li><code>umiche.plot</code> - Plot UMIs in graphs or lines</li> </ul>"},{"location":"#platform","title":"Platform","text":"<ul> <li> PyPI <code>pip install umiche</code></li> <li> Conda <code>conda install jianfeng_sun::umiche</code></li> <li> Docker <code>docker pull 2003100127/umiche</code></li> <li> Github <code>git clone https://github.com/2003100127/umiche.git</code></li> </ul>"},{"location":"about/","title":"About","text":"<p>We are a computational team at the University of Oxford to develop mathematical models and artificial intelligent tools to better interpret biology.</p> <p>UMIche is a UMI-centric platform designed to support the development and verification of UMI  collapsing methods, using both external and internal simulated or experimental data across multiple sequencing contexts, such as bulk and single-cell levels. It is highly modular, featuring Python interfaces to facilitate seamless integration into other computational programs and analysis pipelines. UMIche comprises main four modules: UMI collapsing method implementation, UMI collapsing pipeline implementation, UMI count matrix implementation, and visualisation of UMI metrics (Fig. 1). The core of UMIche offers multifaceted functionalities for UMI deduplication at the single locus, bulk, single-cell levels.</p> Fig 1. UMIche platform.  <p>Abstract</p> <p>UMIche is designed to run at the platform level by harmonising data formats from different sequencing protocols, calculating deduplication counts, exploring features of different methods, visualising analysis results, and etc. We particularly implemented the majority vote and set cover optimisation programs for homotrimer UMI collapsing. To better support the UMI-centric analysis, we further provide a deep learning-based framework for on-demand simulation of UMI count matrices at the single-cell level.</p>"},{"location":"changelog/","title":"Changelog","text":"<ul> <li>0.1.0 -&gt; 1st release (but with abnormal running status)</li> <li>0.1.1 (04.29.2025) -&gt; running with reproducibility and tested on mcverse</li> <li>0.1.2 (05.19.2025) -&gt; placeholder due to imcomplete configration for TMR analysis</li> <li>0.1.3 (05.19.2025) -&gt; add TMR analysis and relevant module</li> <li>0.1.4 (*) -&gt; TBD</li> <li>0.1.5 (05.24.2025) -&gt; Tresor anchor analysis; remove pysam installation constrains for Windows users</li> </ul>"},{"location":"contact/","title":"Contact","text":"<ul> <li> Developer: Dr. Jianfeng Sun</li> <li> Affiliation: Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences (NDORMS), Headington, Oxford OX3 7LD, University of Oxford.</li> <li> Email: Jianfeng.sun@ndorms.ox.ac.uk; jianfeng.sunmt@gmail.com</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#system_requirement","title":"System Requirement","text":"<p>UMIche is advised to be installed within a conda environment, which makes it easier to work on multiple platforms, such as  Windows (partial),  Mac, and  Linux. Owing to the exclusivity of Pysam to the Linux and Mac environments, UMIche does not work with BAM-related analysis in the Windows system.</p> <p>Note</p> <p>Please note that starting from version <code>0.1.5</code>, pysam is no longer a required dependency when installing umiche. This means you can install umiche on any operating system. If you need to use pysam, you should install umiche on a non-Windows system and then install pysam separately. Please refer to the documentation  for instructions on how to install it.</p>"},{"location":"installation/#pypi_highly_recommended_see_the_latest_version","title":"PyPI (highly recommended, see the latest version)","text":"<p>umiche homepage on PyPI</p> <p>Note</p> <p>Please make sure to use the latest version of umiche, as earlier versions may contain bugs. If you do not include the <code>--upgrade</code> flag during installation, you might encounter issues.</p> <pre><code># create a conda environment\nconda create --name umiche python=3.11\n\n# activate the conda environment\nconda activate umiche\n\n# the latest version\npip install umiche --upgrade\n</code></pre>"},{"location":"installation/#conda","title":"Conda","text":"<p>umiche homepage on Anaconda</p> <pre><code># create a conda environment\nconda create --name umiche python=3.11\n\n# activate the conda environment\nconda activate umiche\n\n# the latest version\nconda install jianfeng_sun::umiche\n</code></pre>"},{"location":"installation/#docker","title":"Docker","text":"<p>umiche homepage on Docker</p> <pre><code>docker pull 2003100127/umiche\n</code></pre>"},{"location":"installation/#github","title":"Github","text":"<p>umiche homepage on Github</p> <pre><code># create a conda environment\nconda create --name umiche python=3.11\n\n# activate the conda environment\nconda activate umiche\n\n# create a folder\nmkdir project\n\n# go to the folder\ncd project\n\n# fetch UMIche repository with the latest version\ngit clone https://github.com/2003100127/umiche.git\n\n# enter this repository\ncd umiche\n\n# do the following command\npip install .\n# or\npython setup.py install\n</code></pre>"},{"location":"reproducibility/","title":"Reproducibility","text":"<p>Please refer to mcverse to reproduce the experiments conducted in the paper titled \"UMIche: A UMI-centric analysis platform for enhancing molecular quantification accuracy in bulk and single-cell sequencing\".</p>"},{"location":"tmranalysis/","title":"Tmranalysis","text":"<p>summary</p> <p>\"Sequencing meets cryptography: quadratic substitution error suppression through homotrimer redundancy\"</p> <p>The homotrimer unique molecular identifier (UMI), a simple yet powerful design in which each nucleotide is tripled (e.g., A \u2192 AAA), has demonstrated strong empirical performance in reducing sequencing errors<sup>1</sup>. </p> <p>We provide a theoretical foundation for this approach by framing it through the lens of information theory and error-correcting codes, specifically the triple modular redundancy (TMR) and the (3,1) repetition code used in cryptography. Unlike binary errors, nucleotide substitutions involve one of three alternative bases, introducing a unique probabilistic challenge. </p> <p>We derive an analytical model that accounts for both deterministic majority voting and stochastic tie-breaking, and show that the probability of decoding error scales quadratically with the per-base substitution rate. Compared to conventional UMIs and classical binary redundancy models, homotrimer UMIs provide high resilience under sequencing error conditions. The theoretical framework support the robustness of homotrimer redundancy and offer a new way for optimising UMI design, bridging principles from cryptography and molecular biology.</p>"},{"location":"tmranalysis/#theoretical_derivation_of_error_probabilities","title":"Theoretical Derivation of Error Probabilities","text":""},{"location":"tmranalysis/#1_tmr","title":"1. TMR","text":"<p>We provide a conceptual analogy between repetition code error correction in cryptography and nucleotide-level error correction in sequencing using TMR.</p> Fig 1. Overview of TMR applications in cryptography and sequencing."},{"location":"tmranalysis/#2_binary_31_repetition_code","title":"2. Binary (3,1) Repetition Code","text":"<p>In digital communication, the canonical characters Alice and Bob are often used to represent the sender and receiver of a message. Suppose Alice wishes to transmit the bit 0. To protect against transmission errors, she applies TMR to encode 0 as 000, also known as the (3,1) repetition code. Due to potential noise in the channel, Bob may receive one of eight possible 3-bit combinations. As shown in Fig. 2, Bob applies majority voting to decode them into the original bit: if two or more of the received bits agree, that value is interpreted as the intended message. In this context, if a single bit is flipped regardless of its position, the voting mechanism can correctly recover the original message 0. Although half of the possible voting outcomes may lead to an erroneous result, the actual probability of incorrect interpretation remains low.</p> <p>A classic (3,1) repetition code scheme is illustrated in a binary system, where Alice sends a bit 0 encoded as 000, and Bob decodes received messages using majority voting. A probability model quantifies the probability of correct or incorrect interpretation based on the number and pattern of bit flips.</p> Fig 2. TMR in cryptography. <p>If each bit has an independent probability \\(p\\) of flipping, then the probability \\(p_\\text{rc_block}\\) of a decoding error after applying majority voting is:</p> \\[ P_\\text{rc_block} = 3p^2(1 - p) + p^3 = 3p^2 - 2p^3 \\]"},{"location":"tmranalysis/#3_homotrimer_block_error_probability","title":"3. Homotrimer Block Error Probability","text":"<p>We extend this logic to sequencing, where each base is encoded as a homotrimer (e.g., AAA). Unlike binary systems, DNA sequencing operates in a four-base system with substitution errors (e.g., A \u2192 C, G, or T). The majority-vote decoding is retained when possible, and collapsing is performed by random selection when no majority exists (e.g., ACG). A probability model is derived to quantify correct vs. incorrect block interpretation as a function of per-base error rate \ud835\udc5d, accounting for the combinatorics of 1, 2, and 3 substitution error scenarios.</p> Fig 3. TMR in sequencing. <p>Therefore, the probability \\(p_\\text{ht_block}\\) of a mistakenly decoded nucleotide from a homotrimer block is:</p> \\[ p_\\text{ht_block} = \\frac{7}{3}p^2(1-p) + p^3 = \\frac{7}{3}p^2 - \\frac{4}{3}p^3 \\] <p>Here, \\(p\\) represents the per-base substitution error rate.</p>"},{"location":"tmranalysis/#3_example_substitution_error_rate_of_10-5","title":"3. Example: Substitution error rate of \\(10^{-5}\\)","text":"<p>Given \\(p = 0.00001\\), the homotrimer block decoding error becomes:</p> \\[ p_{\\text{ht_block}} = \\frac{7}{3}(0.00001)^2 - \\frac{4}{3}(0.00001)^3 = \\frac{7}{3} \\cdot 10^{-10} - \\frac{4}{3} \\cdot 10^{-15} = 2.3333 \\times 10^{-10} - 1.3333 \\times 10^{-15} \\approx 2.3333200000e-10 \\] <p>This demonstrates the quadratic suppression of substitution errors enabled by homotrimer redundancy.</p>"},{"location":"tmranalysis/#error_rate_calculation_using_umiche","title":"Error rate calculation using UMIche","text":"<p>Let\u2019s start with some preparation. We can use UMIche to calculate UMI error rates under various scenarios as follows. We initialize an error rate (\\(p=0.00001\\)) representing the probability of an error occurring at a single nucleotide.</p> <pre><code>import umiche as uc\n\nht_tmr = uc.homotrimer.tmr(error_rate=0.00001)\n</code></pre>"},{"location":"tmranalysis/#1_block_error_rate","title":"1. Block error rate","text":"<p>We consider a building block to be erroneous if majority voting fails to correctly identify the original nucleotide. Based on this definition, we can calculate the probability of such an error.</p> <pre><code>ht_tmr.homotrimer_block_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>20/05/2025 23:53:43 logger: =========&gt;homotrimer block error rate: 2.333320000000001e-10\n2.333320000000001e-10\n</code></pre> <p>Comparably, we can get the possibility of the decoding error within a (3,1) repetition code block.</p> <pre><code>ht_tmr.bitflip_block_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>20/05/2025 23:56:07 logger: =========&gt;binary repetition code block error rate is 2.9999800000000003e-10\n2.9999800000000003e-10\n</code></pre>"},{"location":"tmranalysis/#2_umi_error_rate","title":"2. UMI error rate","text":"<p>We consider a UMI to be erroneous if any single building block is decoded incorrectly, without relying on external computational methods or tools for error correction. Based on this assumption, we can calculate the overall probability of UMI failure.</p> <pre><code>ht_tmr.homotrimer_umi_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>21/05/2025 00:05:29 logger: ======&gt;homotrimer UMI error rate:\n21/05/2025 00:05:29 logger: =========&gt;homotrimer block error rate: 2.333320000000001e-10\n21/05/2025 00:05:29 logger: =========&gt;number of building blocks in a UMI is 12\n2.799984244461484e-09\n</code></pre> <p>For the classical model, we can also calculate an error rate.</p> <pre><code>ht_tmr.bitflip_code_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>21/05/2025 00:05:29 logger: ======&gt;binary repetition code error rate:\n21/05/2025 00:05:29 logger: =========&gt;binary repetition code block error rate is 2.9999800000000003e-10\n21/05/2025 00:05:29 logger: =========&gt;number of building blocks in a UMI is 12\n3.5999763170480037e-09\n</code></pre> <p>In contrast, the traditional UMI exhibits a much higher error rate, which can be calculated as follows.</p> <pre><code>ht_tmr.homotrimer_umi_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>21/05/2025 00:09:10 logger: ======&gt;monomer UMI error rate:\n21/05/2025 00:09:10 logger: =========&gt;number of building blocks in a UMI is 12\n0.00011999340021939808\n</code></pre>"},{"location":"tmranalysis/#systematic_investigation_of_error_rate_changes","title":"Systematic investigation of error rate changes","text":""},{"location":"tmranalysis/#1_probability_of_correct_synthesis_per_block","title":"1. Probability of correct synthesis per block","text":"<p>To see how different UMI designs handle sequencing errors, we looked at how the probability of correct synthesis changes with the single-base error rate \ud835\udc5d. We compared three setups: a single base without any redundancy, a classic binary repetition code where each bit is repeated three times and decoded by majority voting, and our homotrimer design, where each nucleotide is repeated three times and decoded in a similar way.</p> <p>We used our tool, UMIche, to model these scenarios and generate the following plot. The grey line shows what happens with a single base\u2014accuracy drops quickly as the error rate increases. The green line represents the binary repetition code, which does a better job but still struggles at higher error rates. The red line is the homotrimer block, and as you can see, it consistently performs the best across all values of \ud835\udc5d.</p> <p>This means the homotrimer approach is more robust to substitution errors, especially in noisy environments, and that makes it a strong choice for improving sequencing accuracy.</p> <pre><code>import numpy as np\n\nerror_rates= np.linspace(0.00001, 0.5, 500)\n\nuc.plot.prob_correct(error_rate=error_rates, num_nt=1)\n</code></pre> Fig 4. Probability of correct synthesis for a single nucleotide, a binary repetition code block of 3 bits, and a homotrimer block as a function of a single-base error rate \ud835\udc5d."},{"location":"tmranalysis/#2_probability_of_incorrect_synthesis_per_umi","title":"2. Probability of incorrect synthesis per UMI","text":"<p>Then we took it a step further and looked at the UMI level. We compared three full-length UMI designs: the standard 12-base UMI with no redundancy, a 36-base binary repetition code, and our 36-base homotrimer UMI. Just like at the block level, the homotrimer UMI came out on top: it had the lowest chance of decoding errors across all error rates. So this tells us that homotrimers aren't just theoretically better: they actually hold up really well in real-world conditions where substitution errors are common.</p> <pre><code>uc.plot.prob_incorrect(error_rate=error_rates, num_nt=12)\n</code></pre> Fig 5. Probability of incorrect synthesis for a 12-bp standard UMI, a 36-bit binary (3, 1) repetition code, and a 36-bp homotrimer UMI."},{"location":"tmranalysis/#visual_summary","title":"Visual Summary","text":"<ul> <li>Homotrimer encoding reduces sequencing errors compared to monomer UMIs and classical binary repetition codes.</li> <li>Majority voting or stochastic tie-breaking in homotrimers provides enhanced robustness to substitution errors.</li> </ul> <ol> <li> <p>Sun, J., Philpott, M., Loi, D. et al. Correcting PCR amplification errors in unique molecular identifiers to generate accurate numbers of sequencing molecules. Nat Methods 21, 401\u2013405 (2024). https://doi.org/10.1038/s41592-024-02168-y\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/IO/","title":"IO","text":""},{"location":"tutorial/IO/#read_bam_files","title":"Read BAM files","text":"<p>A BAM file can be fed into UMIche as shown below.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nbam = uc.io.read_bam(\n    bam_fpn=\"/mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\",\n    verbose=True,\n)\n</code></pre></p> <p>If you know where each read is related or originates with a certain tag, you can extract only this proportion of reads from the BAM file by <code>tags=['PO']</code>.</p> <p> <code>Python</code> <pre><code>print(bam.todf(tags=['PO']))\n</code></pre></p> <p> <code>console</code> <pre><code>31/07/2024 01:36:42 logger: ===&gt;reading the bam file... /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\n31/07/2024 01:36:42 logger: ===&gt;reading BAM time: 0.03s\n31/07/2024 01:36:42 logger: =========&gt;start converting bam to df...\n31/07/2024 01:36:42 logger: =========&gt;time to df: 0.033s\n        id  ... PO\n0        0  ...  1\n1        1  ...  1\n2        2  ...  1\n3        3  ...  1\n4        4  ...  1\n...    ...  ... ..\n6944  6944  ...  1\n6945  6945  ...  1\n6946  6946  ...  1\n6947  6947  ...  1\n6948  6948  ...  1\n\n[6949 rows x 13 columns]\n</code></pre></p>"},{"location":"tutorial/IO/#read_deduplication_files","title":"Read deduplication files","text":"<p>We build a powerful module <code>uc.io.stat</code> for processing deduplicated UMI counts stored in file <code>{method}_dedup.txt</code>. This file can be obtained by running the UMIche pipelines or solely the deduplication methods. It can handle files of multiple sequencing conditions and multiple methods.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nstat_data = uc.io.stat(\n    scenarios={\n        'pcr_nums': 'PCR cycle',\n        'pcr_errs': 'PCR error',\n        'seq_errs': 'Sequencing error',\n        'ampl_rates': 'Amplification rate',\n        'umi_lens': 'UMI length',\n        'seq_deps': 'Sequencing depth',\n    },\n    methods={\n        # 'unique': 'Unique',\n        # 'cluster': 'Cluster',\n        # 'adjacency': 'Adjacency',\n        'directional': 'Directional',\n        # 'dbscan_seq_onehot': 'DBSCAN',\n        # 'birch_seq_onehot': 'Birch',\n        # 'aprop_seq_onehot': 'Affinity Propagation',\n        'mcl': 'MCL',\n        # 'mcl_val': 'MCL-val',\n        # 'mcl_ed': 'MCL-ed',\n    },\n    param_fpn=to('data/params.yml'),\n    verbose=True,\n)\nprint(stat_data)\n</code></pre></p> <p> <code>console</code> <pre><code>31/07/2024 01:31:24 logger: ======&gt;key 1: work_dir\n31/07/2024 01:31:24 logger: =========&gt;value: /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/mclumi/\n31/07/2024 01:31:24 logger: ======&gt;key 2: trimmed\n31/07/2024 01:31:24 logger: =========&gt;value: {'fastq': {'fpn': 'None', 'trimmed_fpn': 'None'}, 'umi_1': {'len': 10}, 'seq': {'len': 100}, 'read_struct': 'umi_1'}\n31/07/2024 01:31:24 logger: ======&gt;key 3: fixed\n31/07/2024 01:31:24 logger: =========&gt;value: {'pcr_num': 8, 'pcr_err': 1e-05, 'seq_err': 0.001, 'ampl_rate': 0.85, 'seq_dep': 400, 'umi_num': 50, 'permutation_num': 2, 'umi_unit_pattern': 1, 'umi_unit_len': 10, 'seq_sub_spl_rate': 0.333, 'sim_thres': 3}\n31/07/2024 01:31:24 logger: ======&gt;key 4: varied\n31/07/2024 01:31:24 logger: =========&gt;value: {'pcr_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'pcr_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.05], 'seq_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1], 'ampl_rates': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'umi_lens': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], 'umi_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], 'seq_deps': [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000]}\n31/07/2024 01:31:24 logger: ======&gt;key 5: dedup\n31/07/2024 01:31:24 logger: =========&gt;value: {'dbscan_eps': 1.5, 'dbscan_min_spl': 1, 'birch_thres': 1.8, 'birch_n_clusters': 'None', 'hdbscan_min_spl': 3, 'aprop_preference': 'None', 'aprop_random_state': 0, 'ed_thres': 1, 'mcl_fold_thres': 1.6, 'iter_num': 100, 'inflat_val': [1.1, 2.7, 3.6], 'exp_val': 2}\n31/07/2024 01:31:24 logger: ======&gt;scenario: PCR cycle\n31/07/2024 01:31:24 logger: =========&gt;method: Directional\n31/07/2024 01:31:24 logger: =========&gt;method: MCL\n31/07/2024 01:31:24 logger: ======&gt;scenario: PCR error\n31/07/2024 01:31:24 logger: =========&gt;method: Directional\n31/07/2024 01:31:24 logger: =========&gt;method: MCL\n31/07/2024 01:31:24 logger: ======&gt;scenario: Sequencing error\n31/07/2024 01:31:24 logger: =========&gt;method: Directional\n31/07/2024 01:31:24 logger: =========&gt;method: MCL\n31/07/2024 01:31:24 logger: ======&gt;scenario: Amplification rate\n31/07/2024 01:31:24 logger: =========&gt;method: Directional\n31/07/2024 01:31:24 logger: =========&gt;method: MCL\n31/07/2024 01:31:24 logger: ======&gt;scenario: UMI length\n31/07/2024 01:31:24 logger: =========&gt;method: Directional\n31/07/2024 01:31:24 logger: =========&gt;method: MCL\n31/07/2024 01:31:24 logger: ======&gt;scenario: Sequencing depth\n31/07/2024 01:31:24 logger: =========&gt;method: Directional\n31/07/2024 01:31:24 logger: =========&gt;method: MCL\n    pn0  pn1  pn2  pn3  ...  max-mean          scenario       method  metric\n0   0.0  0.0  0.0  0.0  ...       0.0         PCR cycle  Directional       1\n1   0.0  0.0  0.0  0.0  ...       0.0         PCR cycle  Directional       2\n2   0.0  0.0  0.0  0.0  ...       0.0         PCR cycle  Directional       3\n3   0.0  0.0  0.0  0.0  ...       0.0         PCR cycle  Directional       4\n4   0.0  0.0  0.0  0.0  ...       0.0         PCR cycle  Directional       5\n..  ...  ...  ...  ...  ...       ...               ...          ...     ...\n4   0.0  0.0  0.0  0.0  ...       0.0  Sequencing depth          MCL     800\n5   0.0  0.0  0.0  0.0  ...       0.0  Sequencing depth          MCL    1000\n6   0.0  0.0  0.0  0.0  ...       0.0  Sequencing depth          MCL    2000\n7   0.0  0.0  0.0  0.0  ...       0.0  Sequencing depth          MCL    3000\n8   0.0  0.0  0.0  0.0  ...       0.0  Sequencing depth          MCL    5000\n\n[158 rows x 19 columns]\n</code></pre></p>"},{"location":"tutorial/IO/#read_inflation_and_expansion_files","title":"Read Inflation and expansion files","text":"<p>Fold change of deduplication with respective to different inflation and expansion values.</p> <p> <code>Python</code> <pre><code>print(stat_data.df_inflat_exp)\n</code></pre></p> <p> <code>console</code></p> <code>inflation</code><code>expansion</code> <pre><code>PCR cycle  PCR error  ...  UMI length  Sequencing depth\n1.10       0.00       0.14  ...        0.02               0.0\n1.45       0.04       0.16  ...        0.02               0.0\n1.80       0.04       0.26  ...        0.02               0.0\n2.15       0.04       0.46  ...        0.02               0.0\n2.50       0.06       0.50  ...        0.02               0.0\n2.85       0.12       0.58  ...        0.02               0.0\n3.20       0.16       0.78  ...        0.02               0.0\n3.55       0.18       0.90  ...        0.02               0.0\n3.90       0.20       0.90  ...        0.02               0.0\n4.25       0.20       0.90  ...        0.02               0.0\n4.60       0.22       0.90  ...        0.02               0.0\n4.95       0.22       0.90  ...        0.02               0.0\n5.30       0.22       0.92  ...        0.02               0.0\n5.65       0.22       1.02  ...        0.02               0.0\n6.00       0.22       1.04  ...        0.02               0.0\n\n[15 rows x 6 columns]\n</code></pre> <pre><code>PCR cycle  PCR error  ...  UMI length  Sequencing depth\n2       0.06       0.56  ...        0.02               0.0\n3       0.04       0.22  ...        0.02               0.0\n4       0.04       0.16  ...        0.02               0.0\n5       0.04       0.16  ...        0.02               0.0\n6       0.04       0.16  ...        0.02               0.0\n7       0.04       0.16  ...        0.02               0.0\n8       0.04       0.16  ...        0.02               0.0\n9       0.04       0.16  ...        0.02               0.0\n\n[8 rows x 6 columns]\n</code></pre>"},{"location":"tutorial/IO/#umi_trajectory_files","title":"UMI trajectory files","text":"<p>Merged UMI nodes (<code>apv</code>)</p>"},{"location":"tutorial/IO/#read_umi_trajectory_files","title":"Read UMI trajectory files","text":"<p> <code>Python</code> <pre><code>print(stat_data.df_trace_cnt)\n</code></pre></p> <p> <code>console</code></p> <code>approved</code><code>disapproved</code> <pre><code>metric  diff_origin  ...          scenario       method\n0      1.0          0.0  ...         PCR cycle  Directional\n1      2.0          0.0  ...         PCR cycle  Directional\n2      3.0          0.0  ...         PCR cycle  Directional\n3      4.0          0.0  ...         PCR cycle  Directional\n4      5.0          0.0  ...         PCR cycle  Directional\n..     ...          ...  ...               ...          ...\n4    800.0          0.0  ...  Sequencing depth          MCL\n5   1000.0          0.0  ...  Sequencing depth          MCL\n6   2000.0          0.0  ...  Sequencing depth          MCL\n7   3000.0          0.0  ...  Sequencing depth          MCL\n8   5000.0          0.0  ...  Sequencing depth          MCL\n\n[158 rows x 9 columns]\n</code></pre> <pre><code>metric  diff_origin  ...          scenario       method\n0       1          0.0  ...         PCR cycle  Directional\n1       2          0.0  ...         PCR cycle  Directional\n2       3          0.0  ...         PCR cycle  Directional\n3       4          0.0  ...         PCR cycle  Directional\n4       5          0.0  ...         PCR cycle  Directional\n..    ...          ...  ...               ...          ...\n4     800          0.0  ...  Sequencing depth  Directional\n5    1000          0.0  ...  Sequencing depth  Directional\n6    2000          0.0  ...  Sequencing depth  Directional\n7    3000          0.0  ...  Sequencing depth  Directional\n8    5000          0.0  ...  Sequencing depth  Directional\n\n[79 rows x 9 columns]\n</code></pre>"},{"location":"tutorial/YAML-configuration/","title":"YAML configuration","text":"<p>In UMIche, all calculation methods rely on a <code>YAML</code> file for parameter initialisation.</p> <p>It defines 5 sections</p> <ul> <li> <code>work_dir</code> - working directory</li> <li> <code>trimmed</code> - trimming FastQ reads and extracting barcodes and UMIs</li> <li> <code>fixed</code> - single-valued simulation parameters</li> <li> <code>varied</code> - varying-valued simulation parameters</li> <li> <code>dedup</code> - UMI deduplication parameters</li> </ul>"},{"location":"tutorial/YAML-configuration/#monomer_umi_pipeline_settings","title":"Monomer UMI pipeline settings","text":"<pre><code>work_dir: /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/mclumi/\n#work_dir: D:/Document/Programming/Python/umiche/umiche/data/simu/general/seq_errs/\n# work_dir data/simu/tree/trimer/\n# work_dir data/simu/monomer/pcr8/\n# work_dir data/simu/trimer/pcr8/\n# work_dir data/simu/dimer/pcr8/\n# work_dir data/simu/dimer/treepcr22_250/\n# work_dir data/simu/dimer/pcr8_mono24/\n\ntrimmed:\n  fastq:\n    fpn: None\n    trimmed_fpn: None\n\n  umi_1:\n    len: 10\n\n  seq:\n    len: 100\n\n  read_struct: 'umi_1'\n\n\nfixed:\n  pcr_num: 8\n  pcr_err: 0.00001\n  seq_err: 0.001\n  ampl_rate: 0.85\n  seq_dep: 400\n  umi_num: 50\n  permutation_num: 2\n  umi_unit_pattern: 1\n  umi_unit_len: 10\n  seq_sub_spl_rate: 0.333\n  sim_thres: 3\n\n\nvaried:\n  pcr_nums: [ # pcr_nums_err_2d_spl0.33\n    1, 2, 3,\n    4, 5, 6, 7,\n    8, 9, 10, 11, 12,\n    13, 14, 15, 16,\n#    17, 18\n#    17, 18, 19, 20,\n  ]\n  pcr_errs: [\n    0.00001,\n    0.000025,\n    0.00005,\n    0.000075,\n    0.0001,\n    0.00025,\n    0.0005,\n    0.00075,\n    0.001,\n    0.0025,\n    0.005,\n    0.0075,\n    0.01,\n#    0.025,\n    0.05,\n#    0.075,\n#    0.1,\n#    0.2,\n#    0.3,\n  ]\n  seq_errs: [\n    0.00001,\n    0.000025,\n    0.00005,\n    0.000075,\n    0.0001,\n    0.00025,\n    0.0005,\n    0.00075,\n    0.001,\n    0.0025,\n    0.005,\n    0.0075,\n    0.01,\n    0.025,\n    0.05,\n    0.075,\n    0.1,\n#    0.2,\n#    0.3,\n  ]\n  ampl_rates: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n  umi_lens: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n#  umi_lens: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n#  umi_nums: [50, 250, 450, 650, 850, 1050]\n  umi_nums: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n  seq_deps: [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000 ]\n#  seq_deps: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\n\ndedup:\n  dbscan_eps: 1.5 # 1.5\n  dbscan_min_spl: 1\n  birch_thres: 1.8 # 1.8\n  birch_n_clusters: None\n  hdbscan_min_spl: 3\n  aprop_preference: None\n  aprop_random_state: 0\n\n  ed_thres: 1\n  mcl_fold_thres: 1.6 # 1.6\n  iter_num: 100\n\n#  inflat_val: 2.7 # 1.1 2.7\n#  exp_val: 2 # 2 3\n#\n  inflat_val: [1.1, 2.7, 3.6]\n  exp_val: 2\n\n#  exp_val: [2, 3, 4]\n\n  # mcl_ed trace!!!\n#  ed_thres: 1\n#  mcl_fold_thres: 2 # 1.6\n#  inflat_val: 2.7 # 1.1 2.7\n#  exp_val: 2 # 2 3\n#  iter_num: 100\n\n  # pcr_nums\n  # mcl_inflat: 2.3\n  # mcl_exp: 2\n  # mcl_fold_thres: 1\n</code></pre>"},{"location":"tutorial/YAML-configuration/#homotrimer_umi_pipeline_settings","title":"Homotrimer UMI pipeline settings","text":"<pre><code>#work_dir: d:/Document/Programming/Python/umiche/umiche/data/simu/umiche/trimer/\nwork_dir: /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umiche/trimer/\n\ntrimmed:\n  fastq:\n    fpn: None\n    trimmed_fpn: None\n\n  umi_1:\n    len: 36\n\n  seq:\n    len: 100\n\n  read_struct: 'umi_1'\n\n\nfixed:\n  pcr_num: 8\n  pcr_err: 0.00001\n  seq_err: 0.001\n  ampl_rate: 0.85\n  seq_dep: 400\n  umi_num: 50\n  permutation_num: 10\n  umi_unit_pattern: 3\n  umi_unit_len: 12\n  seq_sub_spl_rate: 0.333\n  sim_thres: 3\n\n\nvaried:\n  pcr_nums: [ # pcr_nums_err_2d_spl0.33\n    1, 2, 3,\n    4, 5, 6, 7,\n    8, 9, 10, 11, 12,\n    13, 14, 15, 16,\n#    17, 18\n#    17, 18, 19, 20,\n  ]\n  pcr_errs: [\n    0.00001,\n    0.000025,\n    0.00005,\n    0.000075,\n    0.0001,\n    0.00025,\n    0.0005,\n    0.00075,\n    0.001,\n    0.0025,\n    0.005,\n    0.0075,\n    0.01,\n#    0.025,\n    0.05,\n#    0.075,\n#    0.1,\n#    0.2,\n#    0.3,\n  ]\n  seq_errs: [\n    0.00001,\n    0.000025,\n    0.00005,\n    0.000075,\n    0.0001,\n    0.00025,\n    0.0005,\n    0.00075,\n    0.001,\n    0.0025,\n    0.005,\n    0.0075,\n    0.01,\n    0.025,\n    0.05,\n    0.075,\n    0.1,\n    0.2,\n#    0.3,\n  ]\n  ampl_rates: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n  umi_lens: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n#  umi_lens: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n#  umi_nums: [50, 250, 450, 650, 850, 1050]\n  umi_nums: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n  seq_deps: [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000 ]\n#  seq_deps: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\n\ndedup:\n  dbscan_eps: 1.5 # 1.5\n  dbscan_min_spl: 1\n  birch_thres: 1.8 # 1.8\n  birch_n_clusters: None\n  hdbscan_min_spl: 3\n  aprop_preference: None\n  aprop_random_state: 0\n\n  ed_thres: 1\n  mcl_fold_thres: 1.6 # 1.6\n  inflat_val: 2.7 # 1.1 2.7\n  exp_val: 2 # 2 3\n  iter_num: 100\n\n  # mcl_ed trace!!!\n#  ed_thres: 1\n#  mcl_fold_thres: 2 # 1.6\n#  inflat_val: 2.7 # 1.1 2.7\n#  exp_val: 2 # 2 3\n#  iter_num: 100\n\n  # pcr_nums\n  # mcl_inflat: 2.3\n  # mcl_exp: 2\n  # mcl_fold_thres: 1\n</code></pre>"},{"location":"tutorial/tmranalysis/","title":"Tmranalysis","text":"<p>summary</p> <p>\"Sequencing meets cryptography: quadratic substitution error suppression through homotrimer redundancy\". Please check a preprint here .</p> <p>The homotrimer unique molecular identifier (UMI), a simple yet powerful design in which each nucleotide is tripled (e.g., A \u2192 AAA), has demonstrated strong empirical performance in reducing sequencing errors<sup>1</sup>. </p> <p>We provide a theoretical foundation for this approach by framing it through the lens of information theory and error-correcting codes, specifically the triple modular redundancy (TMR) and the (3,1) repetition code used in cryptography. Unlike binary errors, nucleotide substitutions involve one of three alternative bases, introducing a unique probabilistic challenge. </p> <p>We derive an analytical model that accounts for both deterministic majority voting and stochastic tie-breaking, and show that the probability of decoding error scales quadratically with the per-base substitution rate. Compared to conventional UMIs and classical binary redundancy models, homotrimer UMIs provide high resilience under sequencing error conditions. The theoretical framework support the robustness of homotrimer redundancy and offer a new way for optimising UMI design, bridging principles from cryptography and molecular biology.</p>"},{"location":"tutorial/tmranalysis/#theoretical_derivation_of_error_probabilities","title":"Theoretical Derivation of Error Probabilities","text":""},{"location":"tutorial/tmranalysis/#1_tmr","title":"1. TMR","text":"<p>We provide a conceptual analogy between repetition code error correction in cryptography and nucleotide-level error correction in sequencing using TMR.</p> Fig 1. Overview of TMR applications in cryptography and sequencing."},{"location":"tutorial/tmranalysis/#2_binary_31_repetition_code","title":"2. Binary (3,1) Repetition Code","text":"<p>In digital communication, the canonical characters Alice and Bob are often used to represent the sender and receiver of a message. Suppose Alice wishes to transmit the bit 0. To protect against transmission errors, she applies TMR to encode 0 as 000, also known as the (3,1) repetition code. Due to potential noise in the channel, Bob may receive one of eight possible 3-bit combinations. As shown in Fig. 2, Bob applies majority voting to decode them into the original bit: if two or more of the received bits agree, that value is interpreted as the intended message. In this context, if a single bit is flipped regardless of its position, the voting mechanism can correctly recover the original message 0. Although half of the possible voting outcomes may lead to an erroneous result, the actual probability of incorrect interpretation remains low.</p> <p>A classic (3,1) repetition code scheme is illustrated in a binary system, where Alice sends a bit 0 encoded as 000, and Bob decodes received messages using majority voting. A probability model quantifies the probability of correct or incorrect interpretation based on the number and pattern of bit flips.</p> Fig 2. TMR in cryptography. <p>If each bit has an independent probability \\(p\\) of flipping, then the probability \\(p_\\text{rc_block}\\) of a decoding error after applying majority voting is:</p> \\[ P_\\text{rc_block} = 3p^2(1 - p) + p^3 = 3p^2 - 2p^3 \\]"},{"location":"tutorial/tmranalysis/#3_homotrimer_block_error_probability","title":"3. Homotrimer Block Error Probability","text":"<p>We extend this logic to sequencing, where each base is encoded as a homotrimer (e.g., AAA). Unlike binary systems, DNA sequencing operates in a four-base system with substitution errors (e.g., A \u2192 C, G, or T). The majority-vote decoding is retained when possible, and collapsing is performed by random selection when no majority exists (e.g., ACG). A probability model is derived to quantify correct vs. incorrect block interpretation as a function of per-base error rate \ud835\udc5d, accounting for the combinatorics of 1, 2, and 3 substitution error scenarios.</p> Fig 3. TMR in sequencing. <p>Therefore, the probability \\(p_\\text{ht_block}\\) of a mistakenly decoded nucleotide from a homotrimer block is:</p> \\[ p_\\text{ht_block} = \\frac{7}{3}p^2(1-p) + p^3 = \\frac{7}{3}p^2 - \\frac{4}{3}p^3 \\] <p>Here, \\(p\\) represents the per-base substitution error rate.</p>"},{"location":"tutorial/tmranalysis/#3_example_substitution_error_rate_of_10-5","title":"3. Example: Substitution error rate of \\(10^{-5}\\)","text":"<p>Given \\(p = 0.00001\\), the homotrimer block decoding error becomes:</p> \\[ p_{\\text{ht_block}} = \\frac{7}{3}(0.00001)^2 - \\frac{4}{3}(0.00001)^3 = \\frac{7}{3} \\cdot 10^{-10} - \\frac{4}{3} \\cdot 10^{-15} = 2.3333 \\times 10^{-10} - 1.3333 \\times 10^{-15} \\approx 2.3333200000e-10 \\] <p>This demonstrates the quadratic suppression of substitution errors enabled by homotrimer redundancy.</p>"},{"location":"tutorial/tmranalysis/#error_rate_calculation_using_umiche","title":"Error rate calculation using UMIche","text":"<p>Let\u2019s start with some preparation. We can use UMIche to calculate UMI error rates under various scenarios as follows. We initialize an error rate (\\(p=0.00001\\)) representing the probability of an error occurring at a single nucleotide.</p> <pre><code>import umiche as uc\n\nht_tmr = uc.homotrimer.tmr(error_rate=0.00001)\n</code></pre>"},{"location":"tutorial/tmranalysis/#1_block_error_rate","title":"1. Block error rate","text":"<p>We consider a building block to be erroneous if majority voting fails to correctly identify the original nucleotide. Based on this definition, we can calculate the probability of such an error.</p> <pre><code>ht_tmr.homotrimer_block_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>20/05/2025 23:53:43 logger: =========&gt;homotrimer block error rate: 2.333320000000001e-10\n2.333320000000001e-10\n</code></pre> <p>Comparably, we can get the possibility of the decoding error within a (3,1) repetition code block.</p> <pre><code>ht_tmr.bitflip_block_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>20/05/2025 23:56:07 logger: =========&gt;binary repetition code block error rate is 2.9999800000000003e-10\n2.9999800000000003e-10\n</code></pre>"},{"location":"tutorial/tmranalysis/#2_umi_error_rate","title":"2. UMI error rate","text":"<p>We consider a UMI to be erroneous if any single building block is decoded incorrectly, without relying on external computational methods or tools for error correction. Based on this assumption, we can calculate the overall probability of UMI failure.</p> <pre><code>ht_tmr.homotrimer_umi_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>21/05/2025 00:05:29 logger: ======&gt;homotrimer UMI error rate:\n21/05/2025 00:05:29 logger: =========&gt;homotrimer block error rate: 2.333320000000001e-10\n21/05/2025 00:05:29 logger: =========&gt;number of building blocks in a UMI is 12\n2.799984244461484e-09\n</code></pre> <p>For the classical model, we can also calculate an error rate.</p> <pre><code>ht_tmr.bitflip_code_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>21/05/2025 00:05:29 logger: ======&gt;binary repetition code error rate:\n21/05/2025 00:05:29 logger: =========&gt;binary repetition code block error rate is 2.9999800000000003e-10\n21/05/2025 00:05:29 logger: =========&gt;number of building blocks in a UMI is 12\n3.5999763170480037e-09\n</code></pre> <p>In contrast, the traditional UMI exhibits a much higher error rate, which can be calculated as follows.</p> <pre><code>ht_tmr.homotrimer_umi_error\n</code></pre> <p>It outputs the following information.</p> <pre><code>21/05/2025 00:09:10 logger: ======&gt;monomer UMI error rate:\n21/05/2025 00:09:10 logger: =========&gt;number of building blocks in a UMI is 12\n0.00011999340021939808\n</code></pre>"},{"location":"tutorial/tmranalysis/#systematic_investigation_of_error_rate_changes","title":"Systematic investigation of error rate changes","text":""},{"location":"tutorial/tmranalysis/#1_probability_of_correct_synthesis_per_block","title":"1. Probability of correct synthesis per block","text":"<p>To see how different UMI designs handle sequencing errors, we looked at how the probability of correct synthesis changes with the single-base error rate \ud835\udc5d. We compared three setups: a single base without any redundancy, a classic binary repetition code where each bit is repeated three times and decoded by majority voting, and our homotrimer design, where each nucleotide is repeated three times and decoded in a similar way.</p> <p>We used our tool, UMIche, to model these scenarios and generate the following plot. The grey line shows what happens with a single base\u2014accuracy drops quickly as the error rate increases. The green line represents the binary repetition code, which does a better job but still struggles at higher error rates. The red line is the homotrimer block, and as you can see, it consistently performs the best across all values of \ud835\udc5d.</p> <p>This means the homotrimer approach is more robust to substitution errors, especially in noisy environments, and that makes it a strong choice for improving sequencing accuracy.</p> <pre><code>import numpy as np\n\nerror_rates= np.linspace(0.00001, 0.5, 500)\n\nuc.plot.prob_correct(error_rate=error_rates, num_nt=1)\n</code></pre> Fig 4. Probability of correct synthesis for a single nucleotide, a binary repetition code block of 3 bits, and a homotrimer block as a function of a single-base error rate \ud835\udc5d."},{"location":"tutorial/tmranalysis/#2_probability_of_incorrect_synthesis_per_umi","title":"2. Probability of incorrect synthesis per UMI","text":"<p>Then we took it a step further and looked at the UMI level. We compared three full-length UMI designs: the standard 12-base UMI with no redundancy, a 36-base binary repetition code, and our 36-base homotrimer UMI. Just like at the block level, the homotrimer UMI came out on top: it had the lowest chance of decoding errors across all error rates. So this tells us that homotrimers aren't just theoretically better: they actually hold up really well in real-world conditions where substitution errors are common.</p> <pre><code>uc.plot.prob_incorrect(error_rate=error_rates, num_nt=12)\n</code></pre> Fig 5. Probability of incorrect synthesis for a 12-bp standard UMI, a 36-bit binary (3, 1) repetition code, and a 36-bp homotrimer UMI."},{"location":"tutorial/tmranalysis/#visual_summary","title":"Visual Summary","text":"<ul> <li>Homotrimer encoding reduces sequencing errors compared to monomer UMIs and classical binary repetition codes.</li> <li>Majority voting or stochastic tie-breaking in homotrimers provides enhanced robustness to substitution errors.</li> </ul> <ol> <li> <p>Sun, J., Philpott, M., Loi, D. et al. Correcting PCR amplification errors in unique molecular identifiers to generate accurate numbers of sequencing molecules. Nat Methods 21, 401\u2013405 (2024). https://doi.org/10.1038/s41592-024-02168-y\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/1.%20preprocessing/barcode-extraction/","title":"Barcode extraction","text":"<p>Most sequencing short-read technologies arrange different components of sequencing reads in a fixed form. Cell barcodes and molecular barcodes (i.e., UMIs) are demarcated by certain segments of coordinates in read 1. For these fix-length reads, we designed a trimming method for extracting UMIs, barcodes, or other read components in bulk.</p>"},{"location":"tutorial/1.%20preprocessing/barcode-extraction/#central_aim","title":"Central aim","text":"<pre><code>graph LR\n  A[read sequence] --&gt; |barcodes| B[read name];\n  B --- |FastQ file| A;</code></pre>"},{"location":"tutorial/1.%20preprocessing/barcode-extraction/#analogy_with_other_tools","title":"Analogy with other tools","text":""},{"location":"tutorial/1.%20preprocessing/barcode-extraction/#1_umi-tools","title":"1) UMI-tools","text":"<p>UMI-tools uses <code>--bc-pattern</code> as an attribute to tell its built-in modules <code>extract</code> and <code>whitelist</code> where to obtain cell barcode and molecular barcode in sequencing reads.</p> <p>Mission</p> <p>Its mission is to place cell/molecular barcodes from read sequences to read names in the <code>.fastq</code> file.</p> <p>The barcode composition is defined using either:</p> <ol> <li> <p>Positional notation (start/end coordinates) or</p> </li> <li> <p>Regular expressions (pattern matching) to specify the sequence pattern:</p> <ol> <li> <p>N \u2192 UMI positions, </p> </li> <li> <p>C \u2192 Cell barcode positions, </p> </li> <li> <p>X \u2192 Non-barcode/non-UMI bases (retained in read).\"</p> </li> </ol> <p> <code>Attribute</code> <pre><code>--bc-pattern\n</code></pre></p> <p> <code>Module</code> <pre><code>extract\nwhitelist\n</code></pre></p> </li> </ol>"},{"location":"tutorial/1.%20preprocessing/barcode-extraction/#trim_in_umiche","title":"<code>trim</code> in UMIche","text":"<p>Given a FastQ file, We can specifically extract UMIs with the code below.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nparams = {\n    'umi_1': {\n        'len': 10,\n    },\n    'umi_2': {\n        'len': 4,\n    },\n    'bc_1': {\n        'len': 2,\n    },\n    'read_struct': 'umi_1',\n    # 'read_struct': 'umi_1+seq_1',\n    # 'read_struct': 'bc_1+umi_1+seq_1',\n    'seq_1': {\n        'len': 6,\n    },\n    'fastq': {\n        'fpn': to('data/simu/mclumi/seq_errs/permute_0/seq_err_5.fastq.gz'),\n        'trimmed_fpn': to('data/simu/mclumi/seq_errs/permute_0/seq_err_5_trimmed.fastq.gz'),\n    },\n}\n\ndf = uc.trim.template(params=params)\nprint(df)\n</code></pre></p> <p>The following output contains three dataframes. The first dataframe is converted from raw FastQ reads directly. The second one is with UMIs trimmed from reads. The third one is with the rest of the reads for genomic sequences.</p> <p> <code>console</code> <pre><code>31/07/2024 02:48:51 logger: ===&gt;reading from fastq...\nbefore trimmed          seq_raw                name\n0     TCGAATTCAG      17_2_4_5-pcr-5\n1     TCAGAGTGAC     8_1_5_6_8-pcr-8\n2     GTACCCGATT   1_3_5_6_7_8-pcr-8\n3     ATGGACTTCG  21_2_4_5_6_8-pcr-8\n4     CAAGCAGCTG  47_1_2_4_6_7-pcr-7\n...          ...                 ...\n6944  GCACTTCGAC      44_2_5_7-pcr-7\n6945  TCAGAGTGAC           8_8-pcr-8\n6946  GGCGACGGCA    27_2_3_7_8-pcr-8\n6947  GCACGATCAC      10_1_2_7-pcr-7\n6948  AACGTAAAGG    41_1_3_6_8-pcr-8\n\n[6949 rows x 2 columns]\n31/07/2024 02:48:51 logger: ===&gt;umi structure: umi_1\n31/07/2024 02:48:51 logger: ===&gt;bc positions in the read structure: \n31/07/2024 02:48:51 logger: ===&gt;umi positions in the read structure: 0\n31/07/2024 02:48:51 logger: ===&gt;seq positions in the read structure: \n31/07/2024 02:48:51 logger: ======&gt;finding the starting positions of all UMIs...\n31/07/2024 02:48:51 logger: ======&gt;finding the starting positions of all UMIs...\n31/07/2024 02:48:51 logger: =========&gt;umi_1 starting position: 0\n31/07/2024 02:48:51 logger: ======&gt;finding the starting positions of all genomic sequence...\nUMI: start: 0 end: 10\n31/07/2024 02:48:51 logger: ===&gt;umi_1 has been taken out\nafter trimmed          seq_raw                name       umi_1\n0     TCGAATTCAG      17_2_4_5-pcr-5  TCGAATTCAG\n1     TCAGAGTGAC     8_1_5_6_8-pcr-8  TCAGAGTGAC\n2     GTACCCGATT   1_3_5_6_7_8-pcr-8  GTACCCGATT\n3     ATGGACTTCG  21_2_4_5_6_8-pcr-8  ATGGACTTCG\n4     CAAGCAGCTG  47_1_2_4_6_7-pcr-7  CAAGCAGCTG\n...          ...                 ...         ...\n6944  GCACTTCGAC      44_2_5_7-pcr-7  GCACTTCGAC\n6945  TCAGAGTGAC           8_8-pcr-8  TCAGAGTGAC\n6946  GGCGACGGCA    27_2_3_7_8-pcr-8  GGCGACGGCA\n6947  GCACGATCAC      10_1_2_7-pcr-7  GCACGATCAC\n6948  AACGTAAAGG    41_1_3_6_8-pcr-8  AACGTAAAGG\n\n[6949 rows x 3 columns]\n31/07/2024 02:48:51 logger: ===&gt;start saving in gz format...\n31/07/2024 02:48:51 logger: ['seq_raw', 'name', 'umi_1', 'seq_1']\n31/07/2024 02:48:52 logger: ===&gt;trimmed UMIs have been saved in gz format.\n         seq_raw                name       umi_1 seq_1 seq\n0     TCGAATTCAG      17_2_4_5-pcr-5  TCGAATTCAG     B   B\n1     TCAGAGTGAC     8_1_5_6_8-pcr-8  TCAGAGTGAC     B   B\n2     GTACCCGATT   1_3_5_6_7_8-pcr-8  GTACCCGATT     B   B\n3     ATGGACTTCG  21_2_4_5_6_8-pcr-8  ATGGACTTCG     B   B\n4     CAAGCAGCTG  47_1_2_4_6_7-pcr-7  CAAGCAGCTG     B   B\n...          ...                 ...         ...   ...  ..\n6944  GCACTTCGAC      44_2_5_7-pcr-7  GCACTTCGAC     B   B\n6945  TCAGAGTGAC           8_8-pcr-8  TCAGAGTGAC     B   B\n6946  GGCGACGGCA    27_2_3_7_8-pcr-8  GGCGACGGCA     B   B\n6947  GCACGATCAC      10_1_2_7-pcr-7  GCACGATCAC     B   B\n6948  AACGTAAAGG    41_1_3_6_8-pcr-8  AACGTAAAGG     B   B\n\n[6949 rows x 5 columns]\n</code></pre></p> <p>Note</p> <p>In the above example, our sequencing reads are simulated with only UMIs and <code>read_struct</code> is the read structure where all read components are present in reads sequentially. If <code>umi_1</code> is specified in the structure, then its length has to be specified.</p>"},{"location":"tutorial/1.%20preprocessing/barcode-tagging/","title":"Barcode tagging","text":""},{"location":"tutorial/1.%20preprocessing/barcode-tagging/#analogy_with_other_tools","title":"Analogy with other tools","text":""},{"location":"tutorial/1.%20preprocessing/barcode-tagging/#1_umi-tools","title":"1) UMI-tools","text":""},{"location":"tutorial/1.%20preprocessing/barcode-tagging/#read_groupping","title":"Read groupping","text":"<p>The <code>--per-cell</code> and <code>--per-gene</code> attributes are used to tell UMI-tools to group reads within their respective cells and genes. But what they eactly used for read groupping is needed to be specified separately via <code>--cell-tag</code> and <code>--gene-tag</code>, which use a 2-character tag (e.g., <code>XT</code>) found in the <code>tag</code> field of each read in the <code>.bam</code> file.</p> <ol> <li> <p>For <code>--per-gene</code>, it must be used in combination with <code>--gene-tag</code>.</p> </li> <li> <p>For <code>--per-cell</code>, it can optionally be used in combination with <code>--cell-tag</code>.</p> </li> </ol> <pre><code>--per-gene\n--gene-tag\n</code></pre> <pre><code>--gene-tag\n--cell-tag\n--umi-tag\n</code></pre> <p>gene</p> <p>If you have used <code>--gene-tag</code>, then you also need to use <code>--assigned-status-tag</code> too, which is a bool parameter, specified in <code>tag</code> region as well and first introduced by FeatureCounts.</p> <pre><code>--assigned-status-tag=XS\n</code></pre>"},{"location":"tutorial/1.%20preprocessing/barcode-tagging/#umi_retrieval","title":"UMI retrieval","text":"<p>It uses the <code>--umi-tag</code> attribute to extract a UMI per read, with its respective tag stored in a <code>.bam</code> file. For example, to extract UMIs tagged with the <code>MB</code> tag, it is used in combination with the <code>--extract-umi-method</code> attribute. </p> <pre><code>--extract-umi-method=tag --umi-tag=MB\n</code></pre> <p>When this is not available, it uses <code>--umi-separator</code> to get UMIs from read names. For example, to extract UMIs separated with <code>_</code>, it does</p> <pre><code>--umi-separator=_\n</code></pre>"},{"location":"tutorial/m1.%20dedup%20approach/1.%20UMI-graph/","title":"1. UMI graph","text":"<p>In this tutorial, I would walk you through basic operations on a graph constructed by unique UMIs observed at a single genomic locus.</p>"},{"location":"tutorial/m1.%20dedup%20approach/1.%20UMI-graph/#umi_and_graph_nodes","title":"UMI and graph nodes","text":"<p>UMIs are imagined as nodes in a graph. There are two UMI graphs from UMI-tools<sup>1</sup> and mclUMI<sup>2</sup>.</p> <p> UMI-tools graph: 6 unique UMIs observed at a locus from raw reads. Its adjacent list:</p> <p> <code>Python</code> <pre><code>graph_adj_umitools = {\n    'A': ['B', 'C', 'D'],\n    'B': ['A', 'C'],\n    'C': ['A', 'B'],\n    'D': ['A', 'E', 'F'],\n    'E': ['D'],\n    'F': ['D'],\n}\n</code></pre></p> <p> mclUMI graph: 7 unique UMIs observed at a locus from raw reads. Its adjacent list:</p> <p> <code>Python</code> <pre><code>graph_adj_mclumi = {\n    'A': ['B', 'C', 'D'],\n    'B': ['A', 'C'],\n    'C': ['A', 'B'],\n    'D': ['A', 'E', 'F'],\n    'E': ['D', 'G'],\n    'F': ['D', 'G'],\n    'G': ['E', 'F'],\n}\n</code></pre></p> <p>For <code>graph_adj_mclumi</code>, the graph is visually represented this way.</p> Fig 1. A 7-node UMI graph"},{"location":"tutorial/m1.%20dedup%20approach/1.%20UMI-graph/#build_a_umi_graph","title":"Build a UMI graph","text":"<p> <code>Python</code> <pre><code>import umiche as uc\n\nadj = uc.graph.adjacency(\n    graph_adj=graph_adj_mclumi,\n)\nprint(adj.graph)\n</code></pre></p> <p> <code>console</code> <pre><code>{'A': ['B', 'C', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['A', 'E', 'F'], 'E': ['D', 'G'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n</code></pre></p> <p>If you want to turn it to a graph with digital nodes.</p> <p> <code>Python</code> <pre><code>print(adj.graph_mapped)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 12:02:10 logger: ===&gt;the graph is being mapped\n30/07/2024 12:02:10 logger: ======&gt;key map: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\n30/07/2024 12:02:10 logger: ======&gt;the graph is a dict\n30/07/2024 12:02:10 logger: ======&gt;the mapped graph: {0: [1, 2, 3], 1: [0, 2], 2: [0, 1], 3: [0, 4, 5], 4: [3, 6], 5: [3, 6], 6: [4, 5]}\n{0: [1, 2, 3], 1: [0, 2], 2: [0, 1], 3: [0, 4, 5], 4: [3, 6], 5: [3, 6], 6: [4, 5]}\n</code></pre></p> <p>Python <code>dictionary</code>, which is the same as the form of the graph adjacent list.</p> <p> <code>Python</code> <pre><code>print(adj.dict())\n</code></pre></p> <p> <code>console</code> <pre><code>{'A': ['B', 'C', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['A', 'E', 'F'], 'E': ['D', 'G'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n</code></pre></p> <p>Python <code>set</code></p> <p> <code>Python</code> <pre><code>print(adj.set())\n</code></pre></p> <p> <code>console</code> <pre><code>{'A': {'B', 'C', 'D'}, 'B': {'A', 'C'}, 'C': {'A', 'B'}, 'D': {'A', 'E', 'F'}, 'E': {'G', 'D'}, 'F': {'G', 'D'}, 'G': {'E', 'F'}}\n</code></pre></p> <p>Python <code>list</code></p> <p> <code>Python</code> <pre><code>print(adj.list())\n</code></pre></p> <p> <code>console</code> <pre><code>[['B', 'C', 'D'], ['A', 'C'], ['A', 'B'], ['A', 'E', 'F'], ['D', 'G'], ['D', 'G'], ['E', 'F']]\n</code></pre></p> <p>Python <code>numpy ndarray</code>: adjacency matrix</p> <p> <code>Python</code> <pre><code>print(adj.list())\n</code></pre></p> <p> <code>console</code> <pre><code>[[0. 1. 1. 1. 0. 0. 0.]\n [1. 0. 1. 0. 0. 0. 0.]\n [1. 1. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 1. 1. 0.]\n [0. 0. 0. 1. 0. 0. 1.]\n [0. 0. 0. 1. 0. 0. 1.]\n [0. 0. 0. 0. 1. 1. 0.]]\n</code></pre></p> <p>Edge list: symmetrical matrix containing repeated edges</p> <p> <code>Python</code> <pre><code>print(adj.to_edge_list())\n</code></pre></p> <p> <code>console</code> <pre><code>[('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('D', 'A'), ('D', 'E'), ('D', 'F'), ('E', 'D'), ('E', 'G'), ('F', 'D'), ('F', 'G'), ('G', 'E'), ('G', 'F')]\n</code></pre></p> <p>Edge list: triangular matrix extracted from symmetrical matrix. You need to set <code>rr</code> as <code>True</code>.</p> <p> <code>Python</code> <pre><code>print(adj.to_edge_list(\n    rr=True,\n))\n</code></pre></p> <p> <code>console</code> <pre><code>[('C', 'B'), ('C', 'A'), ('B', 'A'), ('G', 'E'), ('E', 'D'), ('G', 'F'), ('D', 'A'), ('F', 'D')]\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/1.%20UMI-graph/#umi_edges","title":"UMI edges","text":"<p>A graph can be represented by an edge list as illustrated above. We put the above edge list <code>to_edge_list(rr=False)</code> to <code>uc.graph.edge</code> and we will be able to operate on the edges.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\neg = uc.graph.edge(\n    graph_edge_list=adj.to_edge_list(rr=False),\n)\nprint(eg.graph)\nprint(eg.nodes)\nprint(eg.key_mapped)\n</code></pre></p> <p> <code>console</code> <pre><code>graph: [('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('D', 'A'), ('D', 'E'), ('D', 'F'), ('E', 'D'), ('E', 'G'), ('F', 'D'), ('F', 'G'), ('G', 'E'), ('G', 'F')]\nkey_mapped: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\nnodes: ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n</code></pre></p> <p>If we want to get non-redundant edges in the edge list, we can do</p> <p> <code>Python</code> <pre><code>print(eg.rvredanduncy)\n</code></pre></p> <p> <code>console</code> <pre><code>[('G', 'E'), ('F', 'D'), ('E', 'D'), ('C', 'A'), ('B', 'A'), ('D', 'A'), ('G', 'F'), ('C', 'B')]\n</code></pre></p> <p>Then, we can still output the properties of the edge-based graph.</p> <p> <code>Python</code> <pre><code>eg.graph = eg.rvredanduncy\nprint(eg.graph)\nprint(eg.key_mapped)\nprint(eg.rvredanduncy)\n</code></pre></p> <p> <code>console</code> <pre><code>[('G', 'E'), ('F', 'D'), ('E', 'D'), ('C', 'A'), ('B', 'A'), ('D', 'A'), ('G', 'F'), ('C', 'B')]\n{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\n[('G', 'E'), ('F', 'D'), ('E', 'D'), ('C', 'A'), ('B', 'A'), ('D', 'A'), ('G', 'F'), ('C', 'B')]\n</code></pre></p> <p>We can convert it back the adjacency list and a new graph with digits mapped from the original graph.</p> <p> <code>Python</code> <pre><code>print(eg.to_adj_dict())\nprint(eg.graph_mapped)\n</code></pre></p> <p> <code>console</code> <pre><code>adjacency list: {'A': ['C', 'B', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['F', 'E', 'A'], 'E': ['G', 'D'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n\ngraph_mapped: [(6, 4), (5, 3), (4, 3), (2, 0), (1, 0), (3, 0), (6, 5), (2, 1)]\n</code></pre></p> <ol> <li> <p>Smith T, Heger A, Sudbery I. UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy. Genome Res. 2017 Mar;27(3):491-499. doi: 10.1101/gr.209601.116. Epub 2017 Jan 18. PMID: 28100584; PMCID: PMC5340976.\u00a0\u21a9</p> </li> <li> <p>Jianfeng Sun and Adam Cribss. mclUMI: Markov clustering of unique molecular identifiers allows removing PCR duplicates dynamically. https://github.com/2003100127/mclumi\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/2.%20Unique/","title":"2. Unique","text":"<p>The UMI count deduplicated by the <code>Unique</code> method is interpreted as the number of unique UMIs observed at a single locus. Thus, consider there is a UMI graph containing non-repeated UMI nodes of 7.</p> <pre><code>graph_adj_mclumi = {\n    'A': ['B', 'C', 'D'],\n    'B': ['A', 'C'],\n    'C': ['A', 'B'],\n    'D': ['A', 'E', 'F'],\n    'E': ['D', 'G'],\n    'F': ['D', 'G'],\n    'G': ['E', 'F'],\n}\n</code></pre> <p>Deduplicated UMI count</p> <p>There are 7 nodes, and therefore, the deduplicated count of UMIs from 7 unique UMIs is 7 at the single locus.</p>"},{"location":"tutorial/m1.%20dedup%20approach/3.%20Cluster/","title":"3. Cluster","text":""},{"location":"tutorial/m1.%20dedup%20approach/3.%20Cluster/#connected_components","title":"Connected components","text":"<p> Connected components are a key concept in graph theory, a field within mathematics and computer science that explores the properties of graphs. A graph is made up of vertices (or nodes) and edges that link pairs of vertices. In an undirected graph, a connected component is defined as a maximal subgraph where each pair of vertices is connected by a path. This means that within a connected component, any vertex can be reached from any other vertex through some path, and there are no additional vertices in the overall graph that can be included in this component without losing this property.</p> <p> Breadth-First Search (BFS) is an effective algorithm for finding connected components in an undirected graph. It starts from a given node, explores all its neighbors, then proceeds to explore the neighbors' neighbors, and so on. A queue is used to manage the vertices that need to be explored next. To identify all connected components using BFS, follow these steps:</p> <p>Tip</p> <p> Initialization</p> <ul> <li> <p>Visited List/Set: Maintain a list or set to track which vertices have been visited.</p> </li> <li> <p>Component List: Initialize a list or counter to store the connected components found.</p> </li> </ul> <p> Iteration Over All Vertices</p> <p>For each vertex in the graph, check if it has already been visited. If it hasn't been visited, it marks the beginning of a new connected component.</p> <p> BFS Traversal</p> <ul> <li>Start a BFS from this unvisited vertex.</li> <li>Use a queue to manage the exploration process. Enqueue the starting vertex and mark it as visited.</li> <li>While the queue is not empty:</li> <li>Dequeue a vertex.</li> <li>For each of its neighbors, if they have not been visited, mark them as visited and enqueue them.</li> <li>All vertices visited during this BFS are part of the current connected component.</li> </ul> <p> Repeated operation</p> <p>Repeat the process for every unvisited vertex, thereby identifying new connected components.</p> <p> Output</p> <p>A list of connected components, with each component consisting of a set of vertices that are interconnected.</p>"},{"location":"tutorial/m1.%20dedup%20approach/3.%20Cluster/#programming_bfs_algorithm","title":"Programming BFS algorithm","text":"<p>We programmed a Python function for breadth-First Search for connected components in a UMI graph with two methods, <code>deque</code> and <code>set</code>.</p> <p> The <code>deque</code> method</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ngraph_adj_mclumi = {\n    'A': ['B', 'C', 'D'],\n    'B': ['A', 'C'],\n    'C': ['A', 'B'],\n    'D': ['A', 'E', 'F'],\n    'E': ['D', 'G'],\n    'F': ['D', 'G'],\n    'G': ['E', 'F'],\n}\n\nconnected_components = uc.graph.cc(\n    graph_adj=graph_adj_mclumi,\n    method='deque',\n    verbose=True\n)\nprint(list(connected_components))\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 13:47:17 logger: ======&gt; root A has not been visited\n30/07/2024 13:47:17 logger: ======&gt; a queue built by root A is deque(['A'])\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['A'])\n30/07/2024 13:47:17 logger: =========&gt; node: A\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['B', 'C', 'D'])\n30/07/2024 13:47:17 logger: =========&gt; node: B\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['C', 'D'])\n30/07/2024 13:47:17 logger: =========&gt; node: C\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['D'])\n30/07/2024 13:47:17 logger: =========&gt; node: D\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['E', 'F'])\n30/07/2024 13:47:17 logger: =========&gt; node: E\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['F', 'G'])\n30/07/2024 13:47:17 logger: =========&gt; node: F\n30/07/2024 13:47:17 logger: =========&gt; a queue built by each root node deque(['G'])\n30/07/2024 13:47:17 logger: =========&gt; node: G\n30/07/2024 13:47:17 logger: =========&gt; visited nodes {'A', 'B', 'D', 'F', 'E', 'G', 'C'}\n30/07/2024 13:47:17 logger: =========&gt; root B has been visited\n30/07/2024 13:47:17 logger: =========&gt; root C has been visited\n30/07/2024 13:47:17 logger: =========&gt; root D has been visited\n30/07/2024 13:47:17 logger: =========&gt; root E has been visited\n30/07/2024 13:47:17 logger: =========&gt; root F has been visited\n30/07/2024 13:47:17 logger: =========&gt; root G has been visited\n[['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n</code></pre></p> <p> The <code>deque</code> function</p> <p> <code>Python</code> <pre><code>def deque(\n        graph : Dict,\n):\n    visited = set()\n    for root, nbrs in graph.items():\n        if root not in visited:\n            visited.add(root)\n            component = []\n            queue = deque([root])\n            while queue:\n                node = queue.popleft()\n                component.append(node)\n                for nbr in graph[node]:\n                    if nbr not in visited:\n                        visited.add(nbr)\n                        queue.append(nbr)\n            yield component\n        else:\n            continue\n</code></pre></p> <p> In addition, there is a <code>set</code> method.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nconnected_components = uc.graph.cc(\n    graph_adj=graph_adj_mclumi,\n    method='set',\n    verbose=True\n)\nprint(connected_components)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 13:47:17 logger: ======&gt; root A has not been visited\n30/07/2024 13:47:17 logger: ======&gt; a queue built by root A is ['A']\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['A']\n30/07/2024 13:47:17 logger: ======&gt; node: A\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['B', 'C', 'D']\n30/07/2024 13:47:17 logger: ======&gt; node: B\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['C', 'D']\n30/07/2024 13:47:17 logger: ======&gt; node: C\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['D']\n30/07/2024 13:47:17 logger: ======&gt; node: D\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['E', 'F']\n30/07/2024 13:47:17 logger: ======&gt; node: E\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['F', 'G']\n30/07/2024 13:47:17 logger: ======&gt; node: F\n30/07/2024 13:47:17 logger: ======&gt; a queue built by each root node ['G']\n30/07/2024 13:47:17 logger: ======&gt; node: G\n30/07/2024 13:47:17 logger: ======&gt; visited nodes {'A', 'B', 'D', 'F', 'E', 'G', 'C'}\n30/07/2024 13:47:17 logger: =========&gt;root B has been visited\n30/07/2024 13:47:17 logger: =========&gt;root C has been visited\n30/07/2024 13:47:17 logger: =========&gt;root D has been visited\n30/07/2024 13:47:17 logger: =========&gt;root E has been visited\n30/07/2024 13:47:17 logger: =========&gt;root F has been visited\n30/07/2024 13:47:17 logger: =========&gt;root G has been visited\n[['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n</code></pre></p> <p> The <code>set</code> function</p> <p> <code>Python</code> <pre><code>def set(\n        graph : Dict,\n) -&gt; List:\n    visited = set()\n    components = []\n    for root, nbrs in graph.items():\n        if root not in visited:\n            visited.add(root)\n            component = []\n            queue = [root]\n            while queue:\n                node = queue.pop(0)\n                component.append(node)\n                for nbr in graph[node]:\n                    if nbr not in visited:\n                        visited.add(nbr)\n                        queue.append(nbr)\n            components.append(component)\n        else:\n            print('=========&gt;root {} has been visited'.format(root))\n    return components\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/3.%20Cluster/#cluster_for_umi_deduplication","title":"<code>Cluster</code> for UMI deduplication","text":"<p>There are two methods to get the connected components from a UMI graph, a <code>deque</code> method based on our customised code and a <code>networkx</code> method using <code>NetworkX</code> (installed via <code>pip install pip install networkx</code>). The number of connected components are reported as the <code>Cluster</code> method in UMI-tools<sup>1</sup>.</p> <p> The <code>deque</code> method.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ncc_cluster = uc.dedup.cluster(\n    graph=graph_adj_mclumi,\n    method='deque',\n)\nprint(cc_cluster)\n</code></pre></p> <p> <code>console</code> <pre><code>{0: ['A', 'B', 'C', 'D', 'E', 'F', 'G']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There is only one cluster 0, and therefore, the deduplicated count of UMIs from 7 unique UMIs is 1 at the single locus.</p> <p> The <code>deque</code> function</p> <p> <code>Python</code> <pre><code>def cc(\n        graph_adj,\n):\n    connected_components = list(gbfscc().deque(graph_adj))\n    return {i: cc for i, cc in enumerate(connected_components)}\n</code></pre></p> <p> The <code>networkx</code> method. This method requires a graph as input represented by an edge list.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nadj = uc.graph.adjacency(\n    graph_adj=graph_adj_mclumi,\n)\n\ncc_cluster = uc.dedup.cluster(\n    graph=adj.to_edge_list(rr=True),\n    method='networkx',\n)\nprint(cc_cluster)\n</code></pre></p> <p> <code>console</code> <pre><code>{0: NodeView(('B', 'A', 'D', 'C', 'F', 'G', 'E'))}\n</code></pre></p> <p> The <code>networkx</code> function</p> <p> <code>Python</code> <pre><code>def ccnx(\n        edge_list,\n):\n    import networkx as nx\n    G = nx.Graph()\n    for edge in edge_list:\n        G.add_edge(edge[0], edge[1])\n    return {i: G.subgraph(cc).nodes() for i, cc in enumerate(nx.connected_components(G))}\n</code></pre></p> <ol> <li> <p>Smith T, Heger A, Sudbery I. UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy. Genome Res. 2017 Mar;27(3):491-499. doi: 10.1101/gr.209601.116. Epub 2017 Jan 18. PMID: 28100584; PMCID: PMC5340976.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/4.%20Adjacency/","title":"4. Adjacency","text":"<p>Within connected component, subcomponents can be further compartmentalised. The number of subcomponents with its central node (the highest count) one edge away from the rest of nodes is treated as the deduplicated molecule count, leading to the <code>Adjacency</code> method in UMI-tools<sup>1</sup>.</p> Fig 1. A 7-node UMI graph <p>The <code>Adjacency</code> method draws on the information about the count of UMIs. In the example graph, the counts of the 7 unique UMIs are given in <code>node_val_sorted</code>.</p> <pre><code>node_val_sorted = pd.Series({\n    'A': 120,\n    'D': 90,\n    'E': 50,\n    'G': 5,\n    'B': 2,\n    'C': 2,\n    'F': 1,\n})\n</code></pre> <p>It also requires the connected components as input because those are what the <code>Adjacency</code> method classifies further into connected subcomponents. </p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nccs = uc.dedup.cluster(\n    graph=graph_adj,\n    method='deque',\n)\n\ndedup_res = uc.dedup.adjacency(\n    connected_components=ccs,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    graph_adj=graph_adj\n)\ndedup_count = dedup_res['count']\ndedup_clusters = dedup_res['clusters']\nprint(\"deduplicated count:\\n{}\".format(dedup_count))\nprint(\"deduplicated clusters:\\n{}\".format(dedup_clusters))\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 15:44:54 logger: ======&gt;connected_components: ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n30/07/2024 15:44:54 logger: =========&gt;subcomponents: {'A': ['B']}\n30/07/2024 15:44:54 logger: =========&gt;subcomponents: {'A': ['B', 'C']}\n30/07/2024 15:44:54 logger: =========&gt;subcomponents: {'A': ['B', 'C', 'D']}\n30/07/2024 15:44:54 logger: ======&gt;The ccurent ele popping out: A {'C', 'B', 'A', 'D'}\n30/07/2024 15:44:54 logger: =========&gt;subcomponents: {'A': ['B', 'C', 'D'], 'D': ['E']}\n30/07/2024 15:44:54 logger: =========&gt;subcomponents: {'A': ['B', 'C', 'D'], 'D': ['E', 'F']}\n30/07/2024 15:44:54 logger: ======&gt;The ccurent ele popping out: D {'F', 'E', 'B', 'C', 'A', 'D'}\n30/07/2024 15:44:54 logger: =========&gt;subcomponents: {'A': ['B', 'C', 'D'], 'D': ['A', 'E', 'F'], 'E': ['G']}\n30/07/2024 15:44:54 logger: ======&gt;The ccurent ele popping out: E {'G', 'F', 'E', 'B', 'C', 'A', 'D'}\ndeduplicated count:\n3\ndeduplicated clusters:\n{'cc_0': {'node_A': ['A', 'B', 'C'], 'node_D': ['D', 'F'], 'node_E': ['E', 'G']}}\ndeduplicated clusters decomposed:\n{0: ['A', 'B', 'C'], 1: ['D', 'F'], 2: ['E', 'G']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There are 3 connected subcomponents (<code>node_A</code>, <code>node_D</code>, and <code>node_E</code>) in connected component <code>cc_0</code> and therefore, the deduplicated count of UMIs from 7 unique UMIs is 3 at the single locus.</p> <p>We implemented the method based on the example 6-node UMI graph from the UMI-tools paper. </p> <p>Abstract</p> <p>The method aims to accurately resolve complex networks by analyzing node counts. It begins by removing the most abundant node and all nodes connected to it. If not all nodes in the network are accounted for, the next most abundant node and its neighbors are removed. This process is repeated until all nodes in the network are addressed. The total number of steps required to resolve the networks at a given locus corresponds to the estimated number of unique molecules.</p> <p> <code>Python</code> <pre><code>def adjacency(\n        connected_components,\n        df_umi_uniq_val_cnt,\n        graph_adj,\n):\n    tcl = []\n    cc_subs = {}\n    for i, cc in connected_components.items():\n        # print('cc: ', cc)\n        step, cc_sub = adjacency_search(df_umi_uniq_val_cnt=df_umi_uniq_val_cnt, cc=cc, graph_adj=graph_adj)\n        tcl.append(step)\n        cc_subs['cc_' + str(i)] = cc_sub\n        # print(self.umi_tools(cc_sorted))\n    return {\n        'count': sum(tcl),\n        'clusters': cc_subs,\n    }\n\ndef adjacency_search(\n        df_umi_uniq_val_cnt,\n        cc,\n        graph_adj,\n):\n    cc_umi_sorted = df_umi_uniq_val_cnt.loc[df_umi_uniq_val_cnt.index.isin(cc)].sort_values(ascending=False).to_dict()\n    ### @@ cc_umi_sorted\n    # {'A': 456, 'E': 90, 'D': 72, 'B': 2, 'C': 2, 'F': 1}\n    cc_sorted = [*cc_umi_sorted.keys()]\n    ### @@ cc_sorted\n    # ['A', 'E', 'D', 'B', 'C', 'F']\n    visited = set()\n    step = 1\n    subcomponents = {}\n    cc_set = set(cc_sorted)\n    while cc_sorted:\n        e = cc_sorted.pop(0)\n        subcomponents[e] = []\n        for node in graph_adj[e]:\n            if node not in visited:\n                subcomponents[e].append(node)\n                # print(subcomponents)\n        ### @@ e, subcomponents[e]\n        # A ['B', 'C', 'D']\n        # E []\n        # D ['F']\n        visited.add(e)\n        ### @@ e, visited\n        # A {'A'}\n        # E {'B', 'A', 'E', 'C', 'D'}\n        # D {'B', 'A', 'E', 'C', 'D'}\n        visited.update(graph_adj[e])\n        ### @@ e, visited\n        # A {'B', 'D', 'A', 'C'}\n        # E {'B', 'A', 'E', 'C', 'D'}\n        # D {'B', 'F', 'A', 'E', 'C', 'D'}\n        subcomponents[e] = graph_adj[e]\n        ### @@ e, subcomponents[e]\n        # A ['B', 'C', 'D']\n        # E ['D']\n        # D ['A', 'E', 'F']\n        # print('the ccurent ele popping out: {} {}'.format(e, visited))\n        if visited == cc_set:\n            # print(step)\n            break\n        else:\n            step += 1\n    ### @@ subcomponents\n    # {'A': ['B', 'C', 'D'], 'E': ['D'], 'D': ['A', 'E', 'F']}\n    vertex = [*subcomponents.keys()]\n    ### @@ vertex\n    # ['A', 'E', 'D']\n    cc_sub = {}\n    for k, v in subcomponents.items():\n        cc_sub['node_' + str(k)] = [k]\n        for i in v:\n            if i not in vertex:\n                cc_sub['node_' + str(k)].append(i)\n    return step, cc_sub\n</code></pre></p> <ol> <li> <p>Smith T, Heger A, Sudbery I. UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy. Genome Res. 2017 Mar;27(3):491-499. doi: 10.1101/gr.209601.116. Epub 2017 Jan 18. PMID: 28100584; PMCID: PMC5340976.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/5.%20Directional/","title":"5. Directional","text":"<p>Since <code>Cluster</code> and <code>Adjacency</code> often led to an underestimated number and an overestimated number of unique UMIs, the <code>Directional</code> method was developed to seek for a more balanced estimation between somewhat of the two extremes. Its deduplication process can coarsely be described as a directed edge-visiting strategy, that is, merging node B by node A if the count of node A is at least two-fold greater than that of node B. The <code>Directional</code> method in the UMI-tools suite is reported to gain the highest accuracy in identifying PCR duplicates<sup>1</sup>.</p> <p>Similar to the <code>Adjacency</code> method, <code>Directional</code> also draws on the information about the count of UMIs. In the example graph, the counts of the 7 unique UMIs are given in <code>node_val_sorted</code>.</p> <pre><code>node_val_sorted = pd.Series({\n    'A': 120,\n    'D': 90,\n    'E': 50,\n    'G': 5,\n    'B': 2,\n    'C': 2,\n    'F': 1,\n})\n</code></pre> <p> <code>Python</code> <pre><code>import umiche as uc\n\nccs = uc.dedup.cluster(\n    graph=graph_adj,\n    method='deque',\n)\n\ndedup_res = directional(\n    connected_components=ccs,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    graph_adj=graph_adj,\n    verbose=True,\n)\ndedup_count = dedup_res['count']\ndedup_clusters = dedup_res['clusters']\nprint(\"deduplicated count:\\n{}\".format(dedup_count))\nprint(\"deduplicated clusters:\\n{}\".format(dedup_clusters))\ndedup_clusters_dc = decompose(dedup_clusters)\nprint(\"deduplicated clusters decomposed:\\n{}\".format(dedup_clusters_dc))\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'A'}\n30/07/2024 15:52:33 logger: {'A'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: B\n30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'A', 'B'}\n30/07/2024 15:52:33 logger: {'A', 'B'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: A\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: C\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: C\n30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'A', 'B', 'C'}\n30/07/2024 15:52:33 logger: {'A', 'B', 'C'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: A\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: B\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: D\n30/07/2024 15:52:33 logger: remaining: {'D', 'F', 'G', 'E'}\n30/07/2024 15:52:33 logger: disapproval [['B', 'C'], ['A', 'D']]\n30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'D'}\n30/07/2024 15:52:33 logger: {'D'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: A\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: E\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: F\n30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'D', 'F'}\n30/07/2024 15:52:33 logger: {'D', 'F'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: D\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: G\n30/07/2024 15:52:33 logger: remaining: {'G', 'E'}\n30/07/2024 15:52:33 logger: disapproval [['D', 'E'], ['F', 'G']]\n30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'E'}\n30/07/2024 15:52:33 logger: {'E'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: D\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: G\n30/07/2024 15:52:33 logger: ======&gt;visited UMI nodes: {'G', 'E'}\n30/07/2024 15:52:33 logger: {'G', 'E'}\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: E\n30/07/2024 15:52:33 logger: =========&gt;the neighbor: F\n30/07/2024 15:52:33 logger: remaining: set()\n30/07/2024 15:52:33 logger: disapproval []\ndeduplicated count:\n3\ndeduplicated clusters:\n{'cc_0': {'node_A': ['A', 'B', 'C'], 'node_D': ['D', 'F'], 'node_E': ['G', 'E']}}\ndeduplicated clusters decomposed:\n{0: ['A', 'B', 'C'], 1: ['D', 'F'], 2: ['G', 'E']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There are 3 connected subcomponents (<code>node_A</code>, <code>node_D</code>, and <code>node_E</code>) in connected component <code>cc_0</code> and therefore, the deduplicated count of UMIs from 7 unique UMIs is 3 at the single locus.</p> <p>In addition, if we want to explore if merged UMIs are of the same or different origin, we can also output the statistics.</p> <p> <code>Python</code> <pre><code># the same orgin\nprint(dedup_res['apv'])\n\n# the different origin\nprint(dedup_res['disapv'])\n</code></pre></p> <p> <code>console</code> <pre><code>{'cc_0': {'node_A': [['A', 'B'], ['A', 'C']], 'node_D': [['D', 'F']], 'node_E': [['E', 'G']]}}\n{'cc_0': {'node_A': [['B', 'C'], ['A', 'D']], 'node_D': [['D', 'E'], ['F', 'G']], 'node_E': []}}\n</code></pre></p> <p>We implemented the <code>Directional</code> method in UMIche.</p> <p> <code>Python</code> <pre><code>def directional(\n    connected_components,\n    df_umi_uniq_val_cnt,\n    graph_adj,\n):\n    cc_sub_cnt = []\n    cc_subs = {}\n    cc_apvs = {}\n    cc_disapvs = {}\n    for i, cc in connected_components.items():\n\n        cc_sub, apv_node_nbr, disapv_node_nbr = umi_tools_(\n            df_umi_uniq_val_cnt=df_umi_uniq_val_cnt,\n            cc=cc,\n            graph_adj=graph_adj,\n        )\n        cc_sub_cnt.append(len(cc_sub))\n        cc_subs['cc_' + str(i)] = cc_sub\n        cc_apvs['cc_' + str(i)] = apv_node_nbr\n        cc_disapvs['cc_' + str(i)] = disapv_node_nbr\n    # print(sum(cc_sub_cnt))\n    # print(cc_subs)\n    # print(cc_apvs)\n    # print(cc_disapvs)\n    if self.heterogeneity:\n        return (\n            sum(cc_sub_cnt),\n            cc_subs,\n            cc_apvs,\n            cc_disapvs,\n        )\n    else:\n        return {\n            \"count\": sum(cc_sub_cnt),\n            \"clusters\": cc_subs,\n            \"apv\": cc_apvs,\n            \"disapv\": cc_disapvs,\n        }\n\ndef directional_search(\n    df_umi_uniq_val_cnt,\n    cc,\n    graph_adj,\n):\n    cc_node_sorted = df_umi_uniq_val_cnt.loc[df_umi_uniq_val_cnt.index.isin(cc)].sort_values(ascending=False).to_dict()\n    ### @@ cc_umi_sorted\n    # {'A': 456, 'E': 90, 'D': 72, 'B': 2, 'C': 2, 'F': 1}\n    nodes = [*cc_node_sorted.keys()]\n    # print(nodes)\n    ### @@ cc_sorted\n    # ['A', 'E', 'D', 'B', 'C', 'F']\n    node_cp = nodes.copy()\n    node_set_remaining = set(node_cp)\n    ### @@ node_set_remaining\n    # {'C', 'F', 'E', 'B', 'D', 'A'}\n    cc_sub = {}\n    apv_node_nbr = {}\n    disapv_node_nbr = {}\n    while nodes:\n        e = nodes.pop(0)\n        if e in node_set_remaining:\n            seen, apv, disapv = dfs(\n                node=e,\n                node_val_sorted=cc_node_sorted,\n                node_set_remaining=node_set_remaining,\n                graph_adj=graph_adj,\n            )\n            ### @@ e, seen\n            # A {'C', 'D', 'F', 'A', 'B'}\n            # E {'E'}\n            cc_sub['node_' + str(e)] = list(seen)\n            apv_node_nbr['node_' + str(e)] = apv\n            disapv_node_nbr['node_' + str(e)] = disapv\n            node_set_remaining = node_set_remaining - seen\n            self.console.print('remaining: {}'.format(node_set_remaining))\n            self.console.print('disapproval {}'.format(disapv))\n            ### @@ print('disapproval {}'.format(disapv))\n            # disapproval []\n            # disapproval [[183, 103]]\n            # disapproval [[131, 4], [131, 147]]\n            # ...\n            # disapproval [[133, 194]]\n            # disapproval []\n        else:\n            continue\n    ### @@ disapv_node_nbr\n    # {'node_0': []}\n    # {'node_36': [[183, 103]]}\n    # {'node_29': [[131, 4], [131, 147]], 'node_4': []}\n    # {'node_7': []}\n    # {'node_28': [[8, 57]]}\n    # ...\n    # {'node_59': [[133, 194]]}\n    # {'node_63': []}\n    return cc_sub, apv_node_nbr, disapv_node_nbr\n</code></pre></p> <p>The <code>directional_search</code> function utilises a depth-first search (DFS) algorithm for travesing the UMI graph. It is written this way</p> <p> <code>Python</code> <pre><code>def dfs(\n        node,\n        node_val_sorted,\n        node_set_remaining,\n        graph_adj,\n):\n    visited = set()\n    approval = []\n    disapproval = []\n    g = graph_adj\n    def search(node):\n        visited.add(node)\n        self.console.print('======&gt;visited UMI nodes: {}'.format(visited))\n        self.console.print(visited)\n        for neighbor in g[node]:\n            self.console.print('=========&gt;the neighbor: {}'.format(neighbor))\n            if neighbor not in visited:\n                if neighbor in node_set_remaining:\n                    if node_val_sorted[node] &gt;= 2 * node_val_sorted[neighbor] - 1:\n                        approval.append([node, neighbor])\n                        search(neighbor)\n                    else:\n                        disapproval.append([node, neighbor])\n    search(node)\n    ### @@ approval\n    # {'cc_0': {'node_A': [['A', 'B'], ['A', 'C'], ['A', 'D'], ['D', 'F']], 'node_E': []}}\n    ### @@ disapproval\n    # {'cc_0': {'node_A': [['B', 'C'], ['D', 'E']], 'node_E': []}}\n    return visited, approval, disapproval\n</code></pre></p> <ol> <li> <p>Smith T, Heger A, Sudbery I. UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy. Genome Res. 2017 Mar;27(3):491-499. doi: 10.1101/gr.209601.116. Epub 2017 Jan 18. PMID: 28100584; PMCID: PMC5340976.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/6.%20Markov-clustering/","title":"6. Markov clustering","text":"<p>We developed mclUMI for automatic detection of UMI clusters by the Markov clustering algorithm<sup>1</sup> without the need for calculating UMI counts, leading to the MCL method. It has two derivatives (MCL-ed and MCL-val) by considering the information about UMI counts.</p>"},{"location":"tutorial/m1.%20dedup%20approach/6.%20Markov-clustering/#mcl","title":"<code>MCL</code>","text":"<p>The <code>MCL</code> method collapses UMIs without the information about the count of UMIs.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nccs = uc.dedup.cluster(\n    graph=graph_adj,\n    method='deque',\n)\n\ndedup_res = uc.dedup.mcl(\n    inflat_val=1.6,\n    exp_val=2,\n    iter_num=100,\n    connected_components=ccs,\n    graph_adj=graph_adj,\n    verbose=True,\n)\n\nprint(dedup_res.columns)\nprint(\"vertices: {}\".format(dedup_res.loc[0, 'cc_vertices']))\nprint(dedup_res.loc[0, 'graph_cc_adj'])\nprint(dedup_res.loc[0, 'nt_to_int_map'])\nprint(dedup_res.loc[0, 'int_to_nt_map'])\n</code></pre></p> <p>It outputs some general information about the inputs and UMI graphs.</p> <p> <code>console</code> <pre><code>Index(['cc_vertices', 'graph_cc_adj', 'nt_to_int_map', 'int_to_nt_map',\n       'cc_adj_mat', 'mcl_clusters', 'clusters', 'clust_num', 'edge_list',\n       'apv'],\n      dtype='object')\nvertices: ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n{'A': ['B', 'C', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['A', 'E', 'F'], 'E': ['D', 'G'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\n{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G'}\n</code></pre></p> <p>Then, we can see the MCL clustered results.</p> <pre><code>print(dedup_res.loc[0, 'cc_adj_mat'])\nprint(dedup_res.loc[0, 'mcl_clusters'])\nprint(dedup_res.loc[0, 'clusters'])\nprint(dedup_res.loc[0, 'clust_num'])\n\nprint(dedup_res['clusters'].values)\ndedup_clusters_dc = decompose_mcl(list_nd=dedup_res['clusters'].values)\nprint(\"deduplicated clusters decomposed (mcl):\\n{}\".format(dedup_clusters_dc))\n</code></pre> <p>We can visually get a feeling of the three nodes 'A', 'B', 'C' being represented in a small cluster while 'D', 'E', 'F', 'G' being represented in another small cluster from its adjacency matrix <code>dedup_res.loc[0, 'cc_adj_mat']</code>.</p> <p> <code>console</code> <pre><code>[[0. 1. 1. 1. 0. 0. 0.]\n [1. 0. 1. 0. 0. 0. 0.]\n [1. 1. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 1. 1. 0.]\n [0. 0. 0. 1. 0. 0. 1.]\n [0. 0. 0. 1. 0. 0. 1.]\n [0. 0. 0. 0. 1. 1. 0.]]\n[(0, 1, 2), (3, 4, 5, 6)]\n[['A', 'B', 'C'], ['D', 'E', 'F', 'G']]\n2\n[list([['A', 'B', 'C'], ['D', 'E', 'F', 'G']])]\ndeduplicated clusters decomposed (mcl):\n{0: ['A', 'B', 'C'], 1: ['D', 'E', 'F', 'G']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There are 2 connected subcomponents (0 and 1) and therefore, the deduplicated count of UMIs from 7 unique UMIs is 2 at the single locus.</p>"},{"location":"tutorial/m1.%20dedup%20approach/6.%20Markov-clustering/#mcl-val","title":"<code>MCL-val</code>","text":"<p>There exist edges between representative nodes from different Markov clusters if their count difference is roughly within t-fold, yielding a UMI count-based merging strategy (referred to as MCL-val).</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf_mcl_val = uc.dedup.mcl_val(\n    df_mcl_ccs=dedup_res,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    thres_fold=2,\n)\nprint(df_mcl_val)\ndedup_count = df_mcl_val['count'].values[0]\ndedup_clusters = df_mcl_val['clusters'].values[0]\nprint(\"deduplicated count (mcl_val):\\n{}\".format(dedup_count))\nprint(\"deduplicated clusters (mcl_val):\\n{}\".format(dedup_clusters))\n\ndf_mcl_val = decompose_mcl(list_nd=df_mcl_val['clusters'].values)\nprint(\"deduplicated clusters decomposed (mcl_val):\\n{}\".format(df_mcl_val))\n</code></pre></p> <p>Setting <code>thres_fold</code> as 2, we get the results that MCL-val keeps two clusters separated.</p> <p> <code>console</code> <pre><code>{'count': 0    2\nName: mscmv_val_len, dtype: int64, 'clusters': 0    [[A], [D]]\nName: mscmv_val_clusters, dtype: object, 'apv': 0    []\nName: mscmv_val_apv, dtype: object, 'disapv': 0    [[A, D]]\nName: mscmv_val_disapv, dtype: object}\ndeduplicated count (mcl_val):\n2\ndeduplicated clusters (mcl_val):\n[['A'], ['D']]\ndeduplicated clusters decomposed (mcl_val):\n{0: ['A'], 1: ['D']}\n</code></pre></p> <p>However, if we change Setting <code>thres_fold</code> as 1, we get a new UMI deduplication result, 1.</p> <p> <code>Python</code> <pre><code>{'count': 0    1\nName: mscmv_val_len, dtype: int64, 'clusters': 0    [[A, D]]\nName: mscmv_val_clusters, dtype: object, 'apv': 0    [[A, D]]\nName: mscmv_val_apv, dtype: object, 'disapv': 0    []\nName: mscmv_val_disapv, dtype: object}\ndeduplicated count (mcl_val):\n1\ndeduplicated clusters (mcl_val):\n[['A', 'D']]\ndeduplicated clusters decomposed (mcl_val):\n{0: ['A', 'D']}\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/6.%20Markov-clustering/#mcl-ed","title":"<code>MCL-ed</code>","text":"<p>There exist edges between representative nodes from different Markov clusters if they are within a minimal edit distance k, leading to a distance-based merging strategy (referred to as MCL-ed). This method needs UMI sequences as input because it will measure the edit distance between UMIs. Thus, we generate the sequences of the 7 unique UMIs.</p> <pre><code>int_to_umi_dict = {\n    'A': 'AGATCTCGCA',\n    'B': 'AGATCCCGCA',\n    'C': 'AGATCACGCA',\n    'D': 'AGATCGCGCA',\n    'E': 'AGATCGCGGA',\n    'F': 'AGATCGCGTA',\n    'G': 'TGATCGCGAA',\n}\n</code></pre> <p>A threshold of edit distance can be set through <code>thres_fold</code>. In this case, we set it as 1, which means that if the representative UMI in each cluster is &gt;1 edit distance away from the other one, they will be considered as two PCR-amplified UMIs originating from the same root.</p> <p>Then, we can use <code>uc.dedup.mcl_ed</code> to obtain the edit distance-based collapsing results.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf_mcl_ed = uc.dedup.mcl_ed(\n    df_mcl_ccs=dedup_res,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    thres_fold=1,\n    int_to_umi_dict=int_to_umi_dict,\n)\ndedup_count = df_mcl_ed['count']\ndedup_clusters = df_mcl_ed['clusters']\nprint('approval: {}'.format(df_mcl_ed['apv']))\n\nprint(\"deduplicated count (mcl_ed):\\n{}\".format(dedup_count))\nprint(\"deduplicated clusters (mcl_ed):\\n{}\".format(dedup_clusters))\n\ndf_mcl_ed = decompose_mcl(list_nd=df_mcl_ed['clusters'].values)\nprint(\"deduplicated clusters decomposed (mcl_ed):\\n{}\".format(df_mcl_ed))\n</code></pre></p> <p>By setting <code>thres_fold</code> as 1, <code>MCL-ed</code> merges two Markov clusters.</p> <p> <code>console</code> <pre><code>approval: 0    [[A, D]]\nName: mscmv_ed_apv, dtype: object\ndeduplicated count (mcl_ed):\n0    1\nName: mscmv_ed_len, dtype: int64\ndeduplicated clusters (mcl_ed):\n0    [[A, D]]\nName: mscmv_ed_clusters, dtype: object\ndeduplicated clusters decomposed (mcl_ed):\n{0: ['A', 'D']}\n</code></pre></p> <p>However, if we change Setting <code>thres_fold</code> as 0, the two clusters are reckoned differently.</p> <p> <code>Python</code> <pre><code>approval: 0    []\nName: mscmv_ed_apv, dtype: object\ndeduplicated count (mcl_ed):\n0    2\nName: mscmv_ed_len, dtype: int64\ndeduplicated clusters (mcl_ed):\n0    [[A], [D]]\nName: mscmv_ed_clusters, dtype: object\ndeduplicated clusters decomposed (mcl_ed):\n{0: ['A'], 1: ['D']}\n</code></pre></p> <ol> <li> <p>Satuluri V, Parthasarathy S, Ucar D. Markov clustering of protein interaction networks with improved balance and scalability. Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology [Internet]. New York, NY, USA: Association for Computing Machinery; 2010. p. 247\u201356. Available from: https://doi.org/10.1145/1854776.1854812\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/7.%20DBSCAN/","title":"7. DBSCAN","text":"<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)<sup>1</sup> is a widely-used clustering algorithm in data mining and machine learning, designed to identify clusters in a dataset. Unlike traditional methods like k-means, which require the number of clusters to be specified beforehand, DBSCAN is a density-based algorithm capable of discovering clusters of any shape and effectively handling noise and outliers.</p> <p>We implemented a DBSCAN-based UMI deduplication method taking one-hot encoded UMI representation as input, which can be accessed via <code>uc.dedup.dbscan</code>. Thus, it also needs the information on UMI sequences.</p> <pre><code>int_to_umi_dict = {\n    'A': 'AGATCTCGCA',\n    'B': 'AGATCCCGCA',\n    'C': 'AGATCACGCA',\n    'D': 'AGATCGCGCA',\n    'E': 'AGATCGCGGA',\n    'F': 'AGATCGCGTA',\n    'G': 'TGATCGCGAA',\n}\n</code></pre> <p>For UMI deduplication, we can use the code below.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf = uc.dedup.dbscan(\n    connected_components=ccs,\n    graph_adj=graph_adj,\n    # df_umi_uniq_val_cnt=node_val_sorted,\n    int_to_umi_dict=int_to_umi_dict,\n    dbscan_eps=1.5,\n    dbscan_min_spl=1,\n)\nfor i, col in enumerate(df.columns):\n    print(\"{}.{}: \\n{}\".format(i+1, col, df[col].values[0]))\ndf_decomposed = decompose_mcl(list_nd=df['clusters'].values)\nprint(\"deduplicated clusters decomposed:\\n{}\".format(df_decomposed))\n</code></pre></p> <p>Parameter illustration</p> <p><code>dbscan_eps</code>: The maximum distance that defines the neighborhood of a sample; two samples are considered neighbors if the distance between them is less than or equal to this value.</p> <p><code>dbscan_min_spl</code>: The minimum number of samples (or total weight) required within a neighborhood for a point to qualify as a core point.</p> <p>From the output, the one-hot representation can be seen. For example, <code>AGATCTCGCA</code> is encoded as <code>[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]</code>. All sequences are encoded as</p> <pre><code>[[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n[0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0]]\n</code></pre> <p> <code>console</code> <pre><code>1.cc_vertices: \n['A', 'B', 'C', 'D', 'E', 'F', 'G']\n2.umi: \n['AGATCTCGCA', 'AGATCCCGCA', 'AGATCACGCA', 'AGATCGCGCA', 'AGATCGCGGA', 'AGATCGCGTA', 'TGATCGCGAA']\n3.onehot: \n[array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0])]\n4.clustering_clusters: \n[array([0, 0, 0, 0, 0, 0, 1])]\n5.clusters: \n[['A', 'B', 'C', 'D', 'E', 'F'], ['G']]\n6.clust_num: \n2\n7.graph_cc_adj: \n{'A': ['B', 'C', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['A', 'E', 'F'], 'E': ['D', 'G'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n8.edge_list: \n[('D', 'A'), ('F', 'D'), ('C', 'B'), ('G', 'F'), ('B', 'A'), ('C', 'A'), ('E', 'D'), ('G', 'E')]\n9.apv: \n[['D', 'A'], ['F', 'D'], ['C', 'B'], ['G', 'F'], ['B', 'A'], ['C', 'A'], ['E', 'D'], ['G', 'E']]\ndeduplicated clusters decomposed:\n{0: ['A', 'B', 'C', 'D', 'E', 'F'], 1: ['G']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There are 2 connected subcomponents (0 and 1) and therefore, the deduplicated count of UMIs from 7 unique UMIs is 2 at the single locus.</p> <ol> <li> <p>Schubert E, Sander J, Ester M, Kriegel HP, Xu X. DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN. ACM Trans Database Syst [Internet]. 2017;42. Available from: https://doi.org/10.1145/3068335\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/8.%20BIRCH/","title":"8. BIRCH","text":"<p>BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)<sup>1</sup> is a hierarchical clustering algorithm optimized for efficiently managing large datasets. Its scalable and incremental design enables it to process data in a single pass and adapt dynamically to new data without needing to reprocess the entire dataset. This makes it particularly effective for clustering large volumes of data.</p> <p>Feature</p> <p>BIRCH constructs a Clustering Feature Tree (CF Tree) to efficiently represent the dataset. This tree comprises nodes, each containing a concise summary of the data, referred to as Clustering Features (CFs).</p> <p>We can use <code>uc.dedup.birch</code> to perform the collapsing of UMIs using UMI sequences as input.</p> <pre><code>int_to_umi_dict = {\n    'A': 'AGATCTCGCA',\n    'B': 'AGATCCCGCA',\n    'C': 'AGATCACGCA',\n    'D': 'AGATCGCGCA',\n    'E': 'AGATCGCGGA',\n    'F': 'AGATCGCGTA',\n    'G': 'TGATCGCGAA',\n}\n</code></pre> <p>For UMI deduplication, we can use the code below.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf = uc.dedup.birch(\n    connected_components=ccs,\n    graph_adj=graph_adj,\n    int_to_umi_dict=int_to_umi_dict,\n    birch_thres=1.8,\n    birch_n_clusters=None,\n)\n# print(df)\nfor i, col in enumerate(df.columns):\n    print(\"{}.{}: \\n{}\".format(i + 1, col, df[col].values[0]))\ndf_decomposed = decompose_mcl(list_nd=df['clusters'].values)\nprint(\"deduplicated clusters decomposed:\\n{}\".format(df_decomposed))\n</code></pre></p> <p>Parameter illustration</p> <p><code>birch_thres</code>: The threshold radius determines that the subcluster formed by merging a new sample with the nearest subcluster must not exceed this value.</p> <p><code>birch_n_clusters</code>: This parameter specifies the number of clusters to be formed during the final clustering step, where the subclusters at the leaves are considered as individual samples.</p> <p> <code>console</code> <pre><code>1.cc_vertices: \n['A', 'B', 'C', 'D', 'E', 'F', 'G']\n2.umi: \n['AGATCTCGCA', 'AGATCCCGCA', 'AGATCACGCA', 'AGATCGCGCA', 'AGATCGCGGA', 'AGATCGCGTA', 'TGATCGCGAA']\n3.onehot: \n[array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0])]\n4.clustering_clusters: \n[array([0, 0, 0, 0, 0, 0, 0])]\n5.clusters: \n[['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n6.clust_num: \n1\n7.graph_cc_adj: \n{'A': ['B', 'C', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['A', 'E', 'F'], 'E': ['D', 'G'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n8.edge_list: \n[('C', 'B'), ('F', 'D'), ('G', 'F'), ('B', 'A'), ('D', 'A'), ('E', 'D'), ('C', 'A'), ('G', 'E')]\n9.apv: \n[['C', 'B'], ['F', 'D'], ['G', 'F'], ['B', 'A'], ['D', 'A'], ['E', 'D'], ['C', 'A'], ['G', 'E']]\ndeduplicated clusters decomposed:\n{0: ['A', 'B', 'C', 'D', 'E', 'F', 'G']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There is 1 connected subcomponent containing all the 7 UMIs (<code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code>, <code>F</code>, <code>G</code>) and therefore, the deduplicated count of UMIs is 1 at the single locus.</p> <ol> <li> <p>Zhang T, Ramakrishnan R, Livny M. BIRCH: A New Data Clustering Algorithm and Its Applications. Data Min Knowl Discov [Internet]. 1997;1:141\u201382. Available from: https://doi.org/10.1023/A:1009783824328\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/9.%20Afinity-propagation/","title":"9. Afinity propagation","text":"<p>Affinity Propagation<sup>1</sup> is a clustering technique that selects representative exemplars from a set of data points and forms clusters around them. Unlike traditional methods like k-means, which require the number of clusters to be predetermined, Affinity Propagation automatically determines the number of clusters based on the data. This method is especially advantageous for discovering clusters without specifying their count and is compatible with various distance metrics, including non-Euclidean ones.</p> <p>Our implemented affinity propagation clustering of UMIs is a parameter-free method, which also takes UMI sequences as input. </p> <pre><code>int_to_umi_dict = {\n    'A': 'AGATCTCGCA',\n    'B': 'AGATCCCGCA',\n    'C': 'AGATCACGCA',\n    'D': 'AGATCGCGCA',\n    'E': 'AGATCGCGGA',\n    'F': 'AGATCGCGTA',\n    'G': 'TGATCGCGAA',\n}\n</code></pre> <p>We can use <code>uc.dedup.affinity_propagation</code> to perform the collapsing of UMIs.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf = uc.dedup.affinity_propagation(\n    connected_components=ccs,\n    graph_adj=graph_adj,\n    int_to_umi_dict=int_to_umi_dict,\n)\n# print(df)\nfor i, col in enumerate(df.columns):\n    print(\"{}.{}: \\n{}\".format(i + 1, col, df[col].values[0]))\ndf_decomposed = decompose_mcl(list_nd=df['clusters'].values)\nprint(\"deduplicated clusters decomposed:\\n{}\".format(df_decomposed))\n</code></pre></p> <p> <code>console</code> <pre><code>1.cc_vertices: \n['A', 'B', 'C', 'D', 'E', 'F', 'G']\n2.umi: \n['AGATCTCGCA', 'AGATCCCGCA', 'AGATCACGCA', 'AGATCGCGCA', 'AGATCGCGGA', 'AGATCGCGTA', 'TGATCGCGAA']\n3.onehot: \n[array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]), array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0])]\n4.clustering_clusters: \n[array([0, 1, 0, 1, 2, 2, 3])]\n5.clusters: \n[['A', 'C'], ['B', 'D'], ['E', 'F'], ['G']]\n6.clust_num: \n4\n7.graph_cc_adj: \n{'A': ['B', 'C', 'D'], 'B': ['A', 'C'], 'C': ['A', 'B'], 'D': ['A', 'E', 'F'], 'E': ['D', 'G'], 'F': ['D', 'G'], 'G': ['E', 'F']}\n8.edge_list: \n[('G', 'F'), ('B', 'A'), ('F', 'D'), ('C', 'A'), ('E', 'D'), ('C', 'B'), ('G', 'E'), ('D', 'A')]\n9.apv: \n[['G', 'F'], ['B', 'A'], ['F', 'D'], ['C', 'A'], ['E', 'D'], ['C', 'B'], ['G', 'E'], ['D', 'A']]\ndeduplicated clusters decomposed:\n{0: ['A', 'C'], 1: ['B', 'D'], 2: ['E', 'F'], 3: ['G']}\n</code></pre></p> <p>Deduplicated UMI count</p> <p>There are 4 connected subcomponents after running and therefore, the deduplicated count of UMIs from 7 unique UMIs is 4 at the single locus.</p> <ol> <li> <p>Shang F, Jiao LC, Shi J, Wang F, Gong M. Fast affinity propagation clustering: A multilevel approach. Pattern Recognit [Internet]. 2012;45:474\u201386. Available from: https://www.sciencedirect.com/science/article/pii/S0031320311002007\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/1.-Splitting-UMIs/","title":"1. Splitting UMIs","text":""},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/1.-Splitting-UMIs/#splitting_umis","title":"Splitting UMIs","text":"<p>We designed two strategies for splitting homotrimer UMIs into monomer UMIs.</p> <p>Abstract</p> <ol> <li> <p>The first strategy involves splitting monomer UMIs from homotrimer UMIs by resolving nucleotide heterogeneity obviated in each trimer block using majority vote. This can be accessed via ``.</p> </li> <li> <p>The second strategy, spALL, splits monomer UMIs without addressing nucleotide heterogeneity conflicts in trimer blocks. This can be accessed via ``.</p> </li> </ol> <p>Consider a homotrimer UMI sequence without an error on it.</p> <pre><code>trimer = AAACCCGGGTTTGGGAAATTTGGGCCCCCC\n</code></pre> <p>To split it into monomer UMIs with the majority vote, we can do</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nsplitter = uc.homotrimer.collapse()\n\nprint(splitter.split_by_mv('AAACCCGGGTTTGGGAAATTTGGGCCCCCC', recur_len=3))\n</code></pre></p> <p> <code>console</code> <pre><code>{'ACGTGATGCC'}\n</code></pre></p> <p>We can add some errors to the <code>trimer</code> sequence. For example, we can split the following 4 erroneous UMIs.</p> <p> <code>Python</code> <pre><code>print(splitter.split_to_mv('AAATCCGGATTTCGGAAATTTGGGCACCCC', recur_len=3))\nprint(splitter.split_to_mv('AAATCCGGATTTCGGAAATTTGGGCCCCCC', recur_len=3))\nprint(splitter.split_to_mv('AAATCCGGATTTGGGAAATTTGGGCCCCCC', recur_len=3))\nprint(splitter.split_to_mv('AAATCCGGGTTTGGGAAATTTGGGCCCCCC', recur_len=3))\n</code></pre></p> <p>Using majority vote, all four UMI sequences are all split into the same UMI. This is because errors added to the sequences per trimer block. Thus, conflicts within trimer blocks can be addressed.</p> <p> <code>console</code> <pre><code>{'ACGTGATGCC'}\n{'ACGTGATGCC'}\n{'ACGTGATGCC'}\n{'ACGTGATGCC'}\n</code></pre></p> <p>But if we use the <code>split_to_all</code> strategy, the errors lead to different numbers of split UMIs for 4 homotrimer UMIs, respectively.</p> <p> <code>Python</code> <pre><code>print(splitter.split_to_all('AAATCCGGATTTCGGAAATTTGGGCACCCC', recur_len=3))\nprint(splitter.split_to_all('AAATCCGGATTTCGGAAATTTGGGCCCCCC', recur_len=3))\nprint(splitter.split_to_all('AAATCCGGATTTGGGAAATTTGGGCCCCCC', recur_len=3))\nprint(splitter.split_to_all('AAATCCGGGTTTGGGAAATTTGGGCCCCCC', recur_len=3))\n</code></pre></p> <p> <code>console</code> <pre><code>{'ATGTCATGCC', 'ACATGATGAC', 'ATATCATGAC', 'ACGTCATGCC', 'ACATGATGCC', 'ACGTCATGAC', 'ATGTCATGAC', 'ATATCATGCC', 'ACATCATGCC', 'ATATGATGAC', 'ATGTGATGAC', 'ATGTGATGCC', 'ACATCATGAC', 'ACGTGATGAC', 'ACGTGATGCC', 'ATATGATGCC'}\n{'ATGTCATGCC', 'ACGTCATGCC', 'ACATGATGCC', 'ATATCATGCC', 'ACATCATGCC', 'ATATGATGCC', 'ACGTGATGCC', 'ATGTGATGCC'}\n{'ACATGATGCC', 'ACGTGATGCC', 'ATATGATGCC', 'ATGTGATGCC'}\n{'ACGTGATGCC', 'ATGTGATGCC'}\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/1.-Splitting-UMIs/#homotrimer_block_conflicts","title":"Homotrimer block conflicts","text":"<p>We can address conflicts within a homotrimer block in two ways.</p> <ol> <li>Randomly picking out a base from each block</li> <li>Sequentially picking out a base from each block</li> <li>Voting the most-common base from each block</li> </ol> <p> <code>Python</code> <pre><code>print(splitter.majority_vote('AAATCCGGGTTTGGGAAATTTGGGCCCCCC', recur_len=3))\nprint(splitter.take_by_order('AAATCCGGGTTTGGGAAATTTGGGCCCCCC', pos=0, recur_len=3))\n</code></pre></p> <p>Using <code>take_by_order</code>, we can sequentially pick out 1st base (<code>pos</code> as 0) from each block. While using <code>majority_vote</code>, the error in the 2nd block can be addressed by voting the most-common base <code>C</code>.</p> <p> <code>console</code> <pre><code>ACGTGATGCC\nATGTGATGCC\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/1.-Splitting-UMIs/#majority_vote","title":"Majority vote","text":"<p>We can better understand how we split a homotrimer UMI with or without conflicts in blocks like below.</p> <p> <code>Python</code> <pre><code>print(splitter.vote('AAA', recur_len=3))\nprint(splitter.vote('TAA', recur_len=3))\nprint(splitter.vote('TGA', recur_len=3))\n</code></pre></p> <p> <code>console</code> <pre><code>{'A'}\n{'A'}\n{'T', 'G', 'A'}\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/2.-Majority-vote/","title":"2. Majority vote","text":""},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/2.-Majority-vote/#majority_vote_function","title":"Majority vote function","text":"<p>The Python code implemented to perform the majority vote for UMI deduplication is shown below.</p> <p> <code>Python</code> <pre><code>def track(\n        multimer_list,\n        recur_len,\n):\n    multimer_umi_to_mono_umi_map = {multimer_umi: self.collapse.majority_vote(\n        umi=multimer_umi,\n        recur_len=recur_len,\n    ) for multimer_umi in multimer_list}\n    mono_umi_to_multimer_umi_map = {self.collapse.majority_vote(\n        umi=multimer_umi,\n        recur_len=recur_len,\n    ): multimer_umi for multimer_umi in multimer_list}\n    uniq_multimer_cnt = len(multimer_umi_to_mono_umi_map)\n    shortlisted_multimer_umi_list = [*mono_umi_to_multimer_umi_map.values()]\n    dedup_cnt = len(shortlisted_multimer_umi_list)\n    print('=========&gt;# of shortlisted multimer UMIs: {}'.format(len(shortlisted_multimer_umi_list)))\n    print('=========&gt;dedup cnt: {}'.format(dedup_cnt))\n    return dedup_cnt, uniq_multimer_cnt, shortlisted_multimer_umi_list\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/2.-Majority-vote/#umi_deduplication","title":"UMI deduplication","text":"<p>We use a BAM file containing reads demarked with trimer UMIs. It contains a total of 6949 reads observed at a single locus. To read it, we do</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nbam = uc.io.read_bam(\n    bam_fpn=\"/mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\",\n    verbose=True,\n)\ndf_bam = bam.todf(tags=['PO'])\nprint(df_bam)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 20:28:17 logger: ===&gt;reading the bam file... /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\n30/07/2024 20:28:17 logger: ===&gt;reading BAM time: 0.00s\n30/07/2024 20:28:17 logger: =========&gt;start converting bam to df...\n30/07/2024 20:28:17 logger: =========&gt;time to df: 0.030s\n        id  ... PO\n0        0  ...  1\n1        1  ...  1\n2        2  ...  1\n3        3  ...  1\n4        4  ...  1\n...    ...  ... ..\n6944  6944  ...  1\n6945  6945  ...  1\n6946  6946  ...  1\n6947  6947  ...  1\n6948  6948  ...  1\n\n[6949 rows x 13 columns]\n</code></pre></p> <p>Then, we extract only trimer sequences from them.</p> <p> <code>Python</code> <pre><code>trimer_list = df_bam.query_name.apply(lambda x: x.split('_')[1]).values\nprint(trimer_list)\n</code></pre></p> <p> <code>console</code> <pre><code>['GGGTTTGTGACCCCCTGTAAATTTCCCCGGAAAGTG'\n 'GGGAAATTTTTTGTTCTCAAAGGGCAAGGGAAATTT'\n 'TTTGGGAACAAAGGGTTTAGGTTTCGGAAAAAATTT' ...\n 'GGGAAAAAAGGGAACAGATATAAATTTTTTTTTCCC'\n 'TTTATTAAAGGAAAATTAGGGAAACTTTTTAAATTT'\n 'AAAGGGAAACCCAAATTTGGGTTTTCGTTTCCTTTT']\n</code></pre></p> <p>Using the trimers as input, we can perform deduplication with set cover.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\n(\n    dedup_cnt,\n    uniq_multimer_cnt,\n    shortlisted_multimer_umi_list,\n) = uc.dedup.majority_vote(\n    multimer_list=trimer_list,\n    recur_len=3,\n    verbose=True\n)\nprint(dedup_cnt)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 20:49:14 logger: ===&gt;reading the bam file... /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\n30/07/2024 20:49:14 logger: ===&gt;reading BAM time: 0.00s\n30/07/2024 20:49:14 logger: =========&gt;start converting bam to df...\n30/07/2024 20:49:14 logger: =========&gt;time to df: 0.059s\n30/07/2024 20:49:14 logger: =========&gt;# of shortlisted multimer UMIs: 1501\n30/07/2024 20:49:14 logger: =========&gt;dedup cnt: 1501\n</code></pre></p> <p>The <code>shortlisted_multimer_umi_list</code> contains all trimer UMIs left after deduplication.</p> <pre><code>['GGGTTTGGGCCCCCCTTTGAATTTACCCGGAAAGGG',\n 'AGGAAATTCTTTTCTCCCAAAGGGAAAGGGAAATTT', \n 'TTTGGGAAAAAAGGGTTAGGGTTTGGGAAAAAATTT', \n 'TTTGGGAAAAAAAAAAAAGGAGGCAAACCCGGGTTT', \n 'AGGCCCGGGAAAAAAGTGAAATGGGGCAAAGGAGGG', \n 'TTTCCCCACTTTCACTTTAAAGGATTTGGGCCCCCC', \n 'TGTCCCCCCAAATTTCCACCCAAACTATTTGGGCTC',\n...\n]\n</code></pre> <p>Deduplicated UMI count</p> <p>The result shows that 6949 reads are deduplicated as 1501.</p>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/3.-Set-coverage/","title":"3. Set coverage","text":""},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/3.-Set-coverage/#greedy_algorithm","title":"Greedy algorithm","text":"<p>The Greedy Algorithm for Set Cover is a heuristic technique used to approximate solutions for the Set Cover Problem, a challenging combinatorial optimization problem known for its NP-hardness. The objective of the Set Cover Problem is to cover a universe of elements with the fewest possible number of subsets from a given collection.</p> <p>Given a universe \ud835\udc48 of \ud835\udc5b elements and a collection \ud835\udc46 of m subsets of U, the task is to identify the smallest subcollection of \ud835\udc46 such that every element in \ud835\udc48 is included in at least one subset from this subcollection.</p> <p>Greedy algorithm</p> <ol> <li> <p>Initialization</p> <ul> <li>Begin with an empty set of chosen subsets.</li> <li>Maintain a set to track which elements have been covered.</li> </ul> </li> <li> <p>Iterative selection</p> <ul> <li>While there are still uncovered elements in \ud835\udc48: For each subset in \ud835\udc46, determine how many uncovered elements it includes.<ul> <li>Select the subset that covers the most uncovered elements.</li> <li>Add this subset to the list of selected subsets.</li> <li>Update the set of covered elements to reflect the addition of the new subset.</li> </ul> </li> </ul> </li> <li> <p>Termination</p> <p>Continue the process until all elements in \ud835\udc48 are covered. The result is a collection of subsets that provides an approximate solution to the Set Cover Problem.</p> </li> </ol>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/3.-Set-coverage/#set_cover_function","title":"Set cover function","text":"<p>The Python code is implemented to perform set cover for UMI deduplication, as given below.</p> <p> <code>Python</code> <pre><code>def greedy(\n        multimer_list,\n        recur_len,\n        split_method='split_to_all',\n):\n    if split_method == 'split_to_all':\n        split_func = collapse.split_to_all\n    else:\n        split_func = collapse.split_by_mv\n    umi_dict = {multimer_umi: split_func(\n        umi=multimer_umi,\n        recur_len=recur_len,\n    ) for multimer_umi in multimer_list}\n    # print(umi_dict)\n    monomer_umi_lens = []\n    multimer_umi_lens = []\n    merged_mono_umi_dict = {}\n    trimer_umi_to_id_map = {trimer_umi: k for k, trimer_umi in enumerate(umi_dict.keys())}\n    trimer_id_to_umi_map = {k: trimer_umi for k, trimer_umi in enumerate(umi_dict.keys())}\n    # print(trimer_umi_to_id_map)\n    # print(trimer_id_to_umi_map)\n    # @@ [*umi_dict.keys()]\n    # ['GGGTTTGTGACCCCCTGTAAATTTCCCCGGAAAGTG',\n    # 'GGGAAATTTTTTGTTCTCAAAGGGCAAGGGAAATTT',\n    # ...,\n    # 'AAAGGGAAACCCAAATTTGGGTTTTCGTTTCCTTTT',]\n    mono_umi_set_list = [*umi_dict.values()]\n    # print(mono_umi_set_list)\n    # @@ mono_umi_set_list\n    # [{'GTGCCTATCGAG', 'GTGACGATCGAT', ..., 'GTGCCGATCGAT'},\n    # {'GATTGCAGAGAT', 'GATTTTAGCGAT', ..., 'GATTGCAGCGAT'},\n    # ...,\n    # {'AGACATGTGTTT', 'AGACATGTCTTT', ..., 'AGACATGTTTCT'}]\n    mono_umi_set_list_remaining = umi_dict\n    num_steps = 0\n    is_empty_set_overlap = False\n    while not is_empty_set_overlap:\n        # It addresses how many trimer UMIs monomer UMIs can account for\n        mono_umi_to_trimer_id_dict = {}\n        for multimer_umi, mono_umi_set in mono_umi_set_list_remaining.items():\n            for mono_umi in mono_umi_set:\n                if mono_umi in mono_umi_to_trimer_id_dict:\n                    mono_umi_to_trimer_id_dict[mono_umi].append(trimer_umi_to_id_map[multimer_umi])\n                else:\n                    mono_umi_to_trimer_id_dict[mono_umi] = [trimer_umi_to_id_map[multimer_umi]]\n        # @@ mono_umi_to_trimer_id_dict\n        # {'GGATTCGGGACT': [5022, 6458], ..., 'TAAAAAGATTAT': [6890], 'TAAAATGACTAT': [6890]}\n        monomer_umi_lens.append(len(mono_umi_to_trimer_id_dict))\n        monomer_umi_to_cnt_map = {k: len(v) for k, v in mono_umi_to_trimer_id_dict.items()}\n        # @@ monomer_umi_to_cnt_map\n        # {'GGATTCGGGACT': 2, ..., 'TAAAAAGACTAT': 1, 'TAAAATGATTAT': 1}\n        if monomer_umi_to_cnt_map:\n            monomer_umi_max = max(monomer_umi_to_cnt_map, key=monomer_umi_to_cnt_map.get)\n        else:\n            break\n        print(monomer_umi_max)\n        # TTAGATGATTAT\n        # ...\n        # TTTTAAGCTGTC\n        # TCCTCTAGTGCC\n        if monomer_umi_to_cnt_map[monomer_umi_max] &gt; 1:\n            multimer_umi_ids = mono_umi_to_trimer_id_dict[monomer_umi_max]\n            multimer_umi_lens.append(len(multimer_umi_ids) - 1)\n\n            # important!!\n            # @@ this is where we keep one trimer UMI\n            merged_mono_umi_dict[monomer_umi_max] = trimer_id_to_umi_map[mono_umi_to_trimer_id_dict[monomer_umi_max][0]]\n\n            for multimer_umi_id in multimer_umi_ids:\n                mono_umi_set_list_remaining.pop(trimer_id_to_umi_map[multimer_umi_id], None)\n            num_steps += 1\n            is_empty_set_overlap = False\n        else:\n            is_empty_set_overlap = True\n\n    multimer_umi_solved_by_sc = [*merged_mono_umi_dict.values()]\n    multimer_umi_not_solved = [*mono_umi_set_list_remaining.keys()]\n    shortlisted_multimer_umi_list = multimer_umi_solved_by_sc + multimer_umi_not_solved\n    print('=========&gt;# of shortlisted multimer UMIs solved by set cover: {}'.format(len(multimer_umi_solved_by_sc)))\n    print('=========&gt;# of shortlisted multimer UMIs not solved by set cover: {}'.format(len(multimer_umi_not_solved)))\n    print('=========&gt;# of shortlisted multimer UMIs: {}'.format(len(shortlisted_multimer_umi_list)))\n    dedup_cnt = len(mono_umi_set_list) - sum(multimer_umi_lens)\n    print('=========&gt;dedup cnt: {}'.format(dedup_cnt))\n    return dedup_cnt, multimer_umi_solved_by_sc, multimer_umi_not_solved, shortlisted_multimer_umi_list, monomer_umi_lens, multimer_umi_lens\n</code></pre></p>"},{"location":"tutorial/m1.%20dedup%20approach/homotrimer-UMI/3.-Set-coverage/#umi_deduplication","title":"UMI deduplication","text":"<p>We use a BAM file containing reads demarked with trimer UMIs. It contains a total of 6949 reads observed at a single locus. To read it, we do</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nbam = uc.io.read_bam(\n    bam_fpn=\"/mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\",\n    verbose=True,\n)\ndf_bam = bam.todf(tags=['PO'])\nprint(df_bam)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 20:28:17 logger: ===&gt;reading the bam file... /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umi/trimer/seq_errs/permute_0/trimmed/seq_err_17.bam\n30/07/2024 20:28:17 logger: ===&gt;reading BAM time: 0.00s\n30/07/2024 20:28:17 logger: =========&gt;start converting bam to df...\n30/07/2024 20:28:17 logger: =========&gt;time to df: 0.030s\n        id  ... PO\n0        0  ...  1\n1        1  ...  1\n2        2  ...  1\n3        3  ...  1\n4        4  ...  1\n...    ...  ... ..\n6944  6944  ...  1\n6945  6945  ...  1\n6946  6946  ...  1\n6947  6947  ...  1\n6948  6948  ...  1\n\n[6949 rows x 13 columns]\n</code></pre></p> <p>Then, we extract only trimer sequences from them.</p> <p> <code>Python</code> <pre><code>trimer_list = df_bam.query_name.apply(lambda x: x.split('_')[1]).values\nprint(trimer_list)\n</code></pre></p> <p> <code>console</code> <pre><code>['GGGTTTGTGACCCCCTGTAAATTTCCCCGGAAAGTG'\n 'GGGAAATTTTTTGTTCTCAAAGGGCAAGGGAAATTT'\n 'TTTGGGAACAAAGGGTTTAGGTTTCGGAAAAAATTT' ...\n 'GGGAAAAAAGGGAACAGATATAAATTTTTTTTTCCC'\n 'TTTATTAAAGGAAAATTAGGGAAACTTTTTAAATTT'\n 'AAAGGGAAACCCAAATTTGGGTTTTCGTTTCCTTTT']\n</code></pre></p> <p>Using the trimers as input, we can perform deduplication with set cover.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\n(\n    dedup_cnt,\n    multimer_umi_solved_by_sc,\n    multimer_umi_not_solved,\n    shortlisted_multimer_umi_list,\n    monomer_umi_lens,\n    multimer_umi_lens,\n) = uc.dedup.set_cover(\n    multimer_list=trimer_list,\n    recur_len=3,\n    split_method='split_to_all',\n)\nprint(dedup_cnt)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 20:40:55 logger: =========&gt;# of shortlisted multimer UMIs solved by set cover: 71\n30/07/2024 20:40:55 logger: =========&gt;# of shortlisted multimer UMIs not solved by set cover: 111\n30/07/2024 20:40:55 logger: =========&gt;# of shortlisted multimer UMIs: 182\n30/07/2024 20:40:55 logger: =========&gt;dedup cnt: 182\n</code></pre></p> <p>Deduplicated UMI count</p> <p>The result shows that 6949 reads are deduplicated as 182.</p>"},{"location":"tutorial/m2.%20pipeline/anchor/","title":"Anchor","text":"<p>The <code>uc.pipeline.anchor</code> function is responsible for analysis of read capture efficiency using anchor-interposed beads. The anchor oligonucleotide design was proposed by this research . In mcverse, we applied Tresor to generate a bunch of reads with which we used the anchor pipeline to calculate the percentage of caaptured reads. </p> <p>hands-on analysis</p> <p>Please see details  about a hands-on analysis using the pipeline.</p> <p>This section demonstrates only the essential components and minimal runnable example. We first define the required parameters in a <code>.yml</code> file, which are fully parsed by the relevant modules within UMIche.</p> <p>It is important to note that <code>work_dir</code> should point to the directory where simulated reads have already been generated by Tresor  under a specific error scenario.</p> <p>We need to define a series of varying <code>criteria</code> under the <code>varied</code> attribute. Typically, we run 10 permutation tests to reduce the impact of random noise. As this is a user-defined sequencing experiment, in principle, you can specify an unlimited number of <code>seq</code> values under the <code>anchor</code> attribute.</p> <pre><code>work_dir: ../../data/tresor/anchor/simu/pcr_del/\n\nfixed:\n  permutation_num: 10\n\nvaried:\n    criteria: [\n        1e-05,\n        2.5e-05,\n        5e-05,\n        7.5e-05,\n        0.0001,\n        0.00025,\n        0.0005,\n        0.00075,\n        0.001,\n        0.0025,\n        0.005,\n        0.0075,\n        0.01,\n        0.025,\n        0.05,\n        0.075,\n        0.1,\n        # 0.2,\n        # 0.3,\n    ]\n\nanchor:\n    seq1: 'BAGC'\n</code></pre> <p>Then, please specify the scenario in the function, again. </p> <pre><code>pct_reads, pct_anchor_reads = uc.pipeline.anchor(\n    scenario='pcr_del',\n    param_fpn='../../data/tresor/anchor/params_anchor.yml',\n)\n</code></pre> <p>It will calculate % of reads captured without an anchor.</p> <pre><code>{0: [0.9952, 0.9962, 0.9954, 0.9944, 0.9894, 0.9852, 0.9798, 0.9706, 0.9392, 0.8288, 0.771, 0.6808, 0.6126, 0.3122, 0.142, 0.0572, 0.0282], 1: [0.998, 0.9892, 0.9956, 0.9734, 0.978, 0.987, 0.953, 0.9536, 0.9346, 0.9046, 0.8136, 0.7044, 0.5678, 0.3194, 0.094, 0.0358, 0.0324], 2: [0.9972, 0.9956, 0.9956, 0.9948, 0.9702, 0.9614, 0.9672, 0.9506, 0.9042, 0.851, 0.7844, 0.6466, 0.5246, 0.2814, 0.0998, 0.061, 0.0232], 3: [0.9868, 0.9954, 0.994, 0.9956, 0.995, 0.9866, 0.936, 0.9348, 0.95, 0.8762, 0.7068, 0.6174, 0.6248, 0.27, 0.1254, 0.0608, 0.0286], 4: [0.9974, 0.9942, 0.9956, 0.994, 0.988, 0.9632, 0.9546, 0.9348, 0.9498, 0.8892, 0.7662, 0.666, 0.5966, 0.2898, 0.1236, 0.059, 0.0212], 5: [0.9964, 0.996, 0.9922, 0.9936, 0.9914, 0.9666, 0.975, 0.9622, 0.9758, 0.84, 0.7312, 0.6896, 0.5448, 0.2692, 0.0928, 0.0482, 0.0236], 6: [0.996, 0.994, 0.9926, 0.9812, 0.993, 0.9804, 0.9444, 0.9674, 0.9678, 0.8592, 0.782, 0.6528, 0.6428, 0.282, 0.118, 0.0584, 0.0386], 7: [0.9966, 0.9962, 0.9958, 0.9932, 0.9914, 0.9822, 0.9346, 0.9442, 0.9306, 0.8476, 0.8036, 0.697, 0.6026, 0.3232, 0.1032, 0.0376, 0.0272], 8: [0.997, 0.995, 0.9928, 0.9938, 0.9922, 0.988, 0.984, 0.9636, 0.9426, 0.9036, 0.7962, 0.6374, 0.5552, 0.2944, 0.0866, 0.0422, 0.0256], 9: [0.9962, 0.991, 0.9956, 0.9928, 0.9928, 0.9856, 0.9802, 0.9528, 0.9366, 0.9044, 0.7412, 0.7106, 0.5484, 0.3078, 0.1202, 0.0418, 0.0264]}\n</code></pre> <p>Also, it will calculate % of reads captured with an anchor.</p> <pre><code>{0: [0.981, 0.9402, 0.9622, 0.8878, 0.8486, 0.6536, 0.4906, 0.3792, 0.287, 0.0862, 0.0192, 0.003, 0.0008, 0.0, 0.0, 0.0, 0.0], 1: [0.9938, 0.9716, 0.9328, 0.8968, 0.868, 0.7362, 0.5006, 0.393, 0.2776, 0.0768, 0.0072, 0.003, 0.0018, 0.0, 0.0, 0.0, 0.0], 2: [0.994, 0.974, 0.917, 0.9164, 0.8914, 0.6954, 0.5458, 0.3634, 0.297, 0.0644, 0.009, 0.0026, 0.001, 0.0, 0.0, 0.0, 0.0], 3: [0.9766, 0.969, 0.9596, 0.8992, 0.9014, 0.6024, 0.448, 0.3616, 0.2564, 0.0862, 0.0062, 0.0038, 0.0006, 0.0002, 0.0, 0.0, 0.0], 4: [0.9898, 0.9668, 0.9068, 0.8852, 0.849, 0.7704, 0.546, 0.3638, 0.318, 0.0866, 0.016, 0.0038, 0.0014, 0.0002, 0.0, 0.0, 0.0], 5: [0.9952, 0.9688, 0.9598, 0.9166, 0.8304, 0.6936, 0.4916, 0.4086, 0.316, 0.0704, 0.013, 0.0046, 0.0014, 0.0, 0.0, 0.0, 0.0], 6: [0.9678, 0.981, 0.927, 0.928, 0.865, 0.7012, 0.5252, 0.4332, 0.2636, 0.092, 0.0126, 0.0022, 0.0006, 0.0, 0.0, 0.0, 0.0], 7: [0.9836, 0.9562, 0.9226, 0.8764, 0.876, 0.726, 0.5374, 0.3614, 0.3218, 0.058, 0.0146, 0.0042, 0.0014, 0.0, 0.0, 0.0, 0.0], 8: [0.9778, 0.9614, 0.9366, 0.9046, 0.8822, 0.7604, 0.5196, 0.3692, 0.2664, 0.0648, 0.0108, 0.003, 0.0014, 0.0, 0.0, 0.0, 0.0], 9: [0.9972, 0.9696, 0.966, 0.908, 0.9056, 0.7056, 0.5192, 0.3878, 0.2856, 0.067, 0.013, 0.0036, 0.0018, 0.0, 0.0, 0.0, 0.0]}\n</code></pre>"},{"location":"tutorial/m2.%20pipeline/heterogeneity/","title":"Heterogeneity","text":""},{"location":"tutorial/m2.%20pipeline/heterogeneity/#1_umi_trajectory","title":"1. UMI trajectory","text":"<p>The <code>uc.pipeline.heterogeneity</code> function allows us to research trajectories of UMIs across PCR amplification cycles.</p> <p>Motivation</p> <p>To merge UMI nodes, the UMI-tools' Directional method launches a series of directed visits from a UMI node with the highest count to UMIs with a lower count. Ideally, when a node attempts to merge a neighbouring node into a cluster, reads attached with the two UMIs are derived from the same original molecule. </p> <p>This can help understand if any two merged UMIs in a UMI graph (for deduplication) have the same origin.</p> <p>To gain an understanding of whether the directed visits in the directional method are beneficial to merge UMIs from the same origin, we labelled each amplified read the identifier of its original molecule to keep track of the source during the read simulation process. There are 4 possible situations, UMIs that are merged and derived from the same origin, UMIs that are merged but derived from a different origin, UMIs that are not merged but derived from the same origin, and UMIs that are not merged and derived from a different origin.</p> Fig 1. Schematic of merging UMIs into a cluster <p>Tip</p> <p>Annotating UMIs with their origin is conducive to study their connectivity in a network in which a merge indicates an estimated association between two UMIs.</p> <p>To generate the relationship between UMIs from any two adjacent PCR cycles, we can use the code below.</p> <p> Python <pre><code>import umiche as uc\n\nuc.pipeline.heterogeneity(\n    # scenario='pcr_nums',\n    # scenario='pcr_errs',\n    scenario='seq_errs',\n    # scenario='ampl_rates',\n    # scenario='umi_lens',\n    # scenario='seq_deps',\n    # scenario='umi_nums',\n\n    # method='unique',\n    # method='cluster',\n    # method='adjacency',\n    method='directional',\n    # method='mcl',\n    # method='mcl_val',\n    # method='mcl_ed',\n    # method='mcl_cc_all_node_umis',\n    # method='dbscan_seq_onehot',\n    # method='birch_seq_onehot',\n    # method='aprop_seq_onehot',\n    # method='hdbscan_seq_onehot',\n    # method='set_cover',\n\n    is_trim=False,\n    is_tobam=False,\n    is_dedup=True,\n    is_sv=True,\n\n    param_fpn=to('data/params.yml'),\n)\n</code></pre></p>"},{"location":"tutorial/m2.%20pipeline/heterogeneity/#2_spell-out","title":"2. Spell-out","text":"<p>The UMIche's pipeline is designed to work on top of the output from Tresor. Thus, we might need to run the <code>heterogeneity</code> pipeline with raw FastQ reads. The UMI sequences are tagged with 3'/5' end of reads. The first thing we need to do is to extract UMI sequences and typically append them to the end of the name of each FastQ read. To do so, we need to set the following parameters and run the <code>uc.pipeline.heterogeneity</code> command.</p> <pre><code>is_trim=True,\nis_tobam=False,\nis_dedup=False,\n</code></pre> <p>Then, because the format of input to mclUMI is BAM, we need to convert the FastQ file to a BAM file. Please note that this is not to map reads to a genome but is simply a conversion from FastQ to BAM. To do so, we need to claim the following parameters and re-run the <code>uc.pipeline.heterogeneity</code> command.</p> <pre><code>is_trim=False,\nis_tobam=True,\nis_dedup=False,\n</code></pre> <p>Please notice that if you have mapped reads in BAM, you can omit this step.</p> <p>Then, we re-run the <code>uc.pipeline.heterogeneity</code> command to mute <code>is_trim</code> and <code>is_dedup</code>, but open the deduplication channel with <code>is_dedup=True</code>. We can see the following output if we conduct the deduplication on simulated reads from two rounds of permutation tests. But if we set <code>verbose=True</code>, we will be prompted with more output in the console.</p> <p> Output <pre><code>29/07/2024 23:47:04 logger: ======&gt;key 1: work_dir\n29/07/2024 23:47:04 logger: =========&gt;value: /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/mclumi/\n29/07/2024 23:47:04 logger: ======&gt;key 2: trimmed\n29/07/2024 23:47:04 logger: =========&gt;value: {'fastq': {'fpn': 'None', 'trimmed_fpn': 'None'}, 'umi_1': {'len': 10}, 'seq': {'len': 100}, 'read_struct': 'umi_1'}\n29/07/2024 23:47:04 logger: ======&gt;key 3: fixed\n29/07/2024 23:47:04 logger: =========&gt;value: {'pcr_num': 8, 'pcr_err': 1e-05, 'seq_err': 0.001, 'ampl_rate': 0.85, 'seq_dep': 400, 'umi_num': 50, 'permutation_num': 2, 'umi_unit_pattern': 1, 'umi_unit_len': 10, 'seq_sub_spl_rate': 0.333, 'sim_thres': 3}\n29/07/2024 23:47:04 logger: ======&gt;key 4: varied\n29/07/2024 23:47:04 logger: =========&gt;value: {'pcr_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'pcr_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.05], 'seq_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05], 'ampl_rates': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'umi_lens': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], 'umi_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], 'seq_deps': [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000]}\n29/07/2024 23:47:04 logger: ======&gt;key 5: dedup\n29/07/2024 23:47:04 logger: =========&gt;value: {'dbscan_eps': 1.5, 'dbscan_min_spl': 1, 'birch_thres': 1.8, 'birch_n_clusters': 'None', 'hdbscan_min_spl': 3, 'aprop_preference': 'None', 'aprop_random_state': 0, 'ed_thres': 1, 'mcl_fold_thres': 1.6, 'iter_num': 100, 'inflat_val': 2.7, 'exp_val': 2}\n===&gt;dedup method: directional\n===&gt;No.0 permutation\nNo.0-&gt;1e-05 for seq_errs dedup cnt: 50\nNo.1-&gt;2.5e-05 for seq_errs dedup cnt: 50\nNo.2-&gt;5e-05 for seq_errs dedup cnt: 50\nNo.3-&gt;7.5e-05 for seq_errs dedup cnt: 50\nNo.4-&gt;0.0001 for seq_errs dedup cnt: 50\nNo.5-&gt;0.00025 for seq_errs dedup cnt: 50\nNo.6-&gt;0.0005 for seq_errs dedup cnt: 50\nNo.7-&gt;0.00075 for seq_errs dedup cnt: 50\nNo.8-&gt;0.001 for seq_errs dedup cnt: 51\nNo.9-&gt;0.0025 for seq_errs dedup cnt: 52\nNo.10-&gt;0.005 for seq_errs dedup cnt: 54\nNo.11-&gt;0.0075 for seq_errs dedup cnt: 63\nNo.12-&gt;0.01 for seq_errs dedup cnt: 70\nNo.13-&gt;0.025 for seq_errs dedup cnt: 89\nNo.14-&gt;0.05 for seq_errs dedup cnt: 150\n    pn0\n0    50\n1    50\n2    50\n3    50\n4    50\n5    50\n6    50\n7    50\n8    51\n9    52\n10   54\n11   63\n12   70\n13   89\n14  150\n===&gt;No.1 permutation\nNo.0-&gt;1e-05 for seq_errs dedup cnt: 50\nNo.1-&gt;2.5e-05 for seq_errs dedup cnt: 50\nNo.2-&gt;5e-05 for seq_errs dedup cnt: 50\nNo.3-&gt;7.5e-05 for seq_errs dedup cnt: 50\nNo.4-&gt;0.0001 for seq_errs dedup cnt: 50\nNo.5-&gt;0.00025 for seq_errs dedup cnt: 50\nNo.6-&gt;0.0005 for seq_errs dedup cnt: 50\nNo.7-&gt;0.00075 for seq_errs dedup cnt: 50\nNo.8-&gt;0.001 for seq_errs dedup cnt: 51\nNo.9-&gt;0.0025 for seq_errs dedup cnt: 53\nNo.10-&gt;0.005 for seq_errs dedup cnt: 53\nNo.11-&gt;0.0075 for seq_errs dedup cnt: 65\nNo.12-&gt;0.01 for seq_errs dedup cnt: 70\nNo.13-&gt;0.025 for seq_errs dedup cnt: 89\nNo.14-&gt;0.05 for seq_errs dedup cnt: 134\n    pn0  pn1\n0    50   50\n1    50   50\n2    50   50\n3    50   50\n4    50   50\n5    50   50\n6    50   50\n7    50   50\n8    51   51\n9    52   53\n10   54   53\n11   63   65\n12   70   70\n13   89   89\n14  150  134\n</code></pre></p>"},{"location":"tutorial/m2.%20pipeline/heterogeneity/#3_generated_files","title":"3. Generated files","text":"Fig 2. Generated files of the UMI trajectory <p> Homogeneity in origin between every two merged UMIs</p> <code>directional_apv_cnt.txt</code><code>directional_apv_pct.txt</code> <pre><code>diff_origin same_origin total   scenario    method  permutation\n0   0   0   1   directional 0\n0   1   1   2   directional 0\n0   2   2   3   directional 0\n0   3   3   4   directional 0\n0   10  10  5   directional 0\n0   20  20  6   directional 0\n0   35  35  7   directional 0\n0   62  62  8   directional 0\n...\n8   1244    1252    14  directional 9\n9   1438    1447    15  directional 9\n5   1528    1533    16  directional 9\n</code></pre> <pre><code>diff_origin same_origin total   scenario    method  permutation\n0.0 0.0 1   1   directional 0\n0.0 0.02    1   2   directional 0\n0.0 0.04    1   3   directional 0\n0.0 0.06    1   4   directional 0\n0.0 0.18    1   5   directional 0\n0.0 0.36    1   6   directional 0\n0.0 0.5 1   7   directional 0\n0.0 0.6274509803921569  1   8   directional 0\n...\n0.0031283774105759343   0.9968716225894241  1   14  directional 9\n0.002942775441849134    0.9970572245581508  1   15  directional 9\n0.001674603054245462    0.9983253969457546  1   16  directional 9\n</code></pre> <p> Heterogeneity in origin between every two merged UMIs</p> <code>directional_disapv_cnt.txt</code><code>directional_disapv_pct.txt</code> <pre><code>diff_origin same_origin total   scenario    method  permutation\n0   0   0   1   directional 0\n0   0   0   2   directional 0\n0   0   0   3   directional 0\n0   0   0   4   directional 0\n0   0   0   5   directional 0\n0   0   0   6   directional 0\n0   0   0   7   directional 0\n0   0   0   8   directional 0\n...\n5   519 524 14  directional 9\n15  875 890 15  directional 9\n19  1163    1182    16  directional 9\n</code></pre> <pre><code>diff_origin same_origin total   scenario    method  permutation\n0.0 0.0 1   1   directional 0\n0.0 0.0 1   2   directional 0\n0.0 0.0 1   3   directional 0\n0.0 0.0 1   4   directional 0\n0.0 0.0 1   5   directional 0\n0.0 0.0 1   6   directional 0\n0.0 0.0 1   7   directional 0\n0.0 0.0 1   8   directional 0\n...\n0.003969508752117447    0.9960304912478826  1   14  directional 9\n0.006470058266103986    0.9935299417338961  1   15  directional 9\n0.005888901910230868    0.9941110980897692  1   16  directional 9\n</code></pre>"},{"location":"tutorial/m2.%20pipeline/standard/","title":"Standard","text":"<p>The standard pipeline is built to perform UMI deduplication with 11 methods on a regualr basis. However, this pipeline still runs based on the output of the Tresor tool or other simulated reads.  </p> <p>Feature</p> <p>The standard pipeline can give you the hints about how experimental researchers can optimise their sequencing libraries or how computational scientists can design more effective tools for UMI deduplication. Therefore, it will provide you with the statistics during the deduplication as well the deduplicated UMI count under multiple conditions.</p> <p>We show an example of using the <code>set_cover</code> method to deduplicate trimer UMIs. Please be sure of the working directory set to be the root of a batch of simulated reads by <code>Tresor</code> software, where you should be able to see different permutation folders. We can use the below code to do the batch deduplication.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nuc.pipeline.standard(\n    # scenario='pcr_nums',\n    # scenario='pcr_errs',\n    scenario='seq_errs',\n    # scenario='ampl_rates',\n    # scenario='umi_lens',\n\n    # method='unique',\n    # method='cluster',\n    # method='adjacency',\n    # method='directional',\n    # method='mcl',\n    # method='mcl_val',\n    # method='mcl_ed',\n    method='set_cover',\n    # method='majority_vote',\n\n    # is_trim=True,\n    # is_tobam=False,\n    # is_dedup=False,\n\n    # is_trim=False,\n    # is_tobam=True,\n    # is_dedup=False,\n\n    is_trim=False,\n    is_tobam=False,\n    is_dedup=True,\n\n    # @@ for directional on multimer umis deduplicated by set_cover\n    is_collapse_block=False,\n    deduped_method='set_cover',\n    split_method='split_to_all', # split_to_all split_by_mv\n\n    # @@ for directional on multimer umis deduplicated by majority_vote\n    # is_collapse_block=False,\n    # deduped_method='majority_vote',\n    # split_method='',\n\n    # @@ for directional but on monomer umis of set_cover or majority_vote\n    # is_collapse_block=True, # True False\n    # collapse_block_method='take_by_order', # majority_vote take_by_order\n    # deduped_method='set_cover', # majority_vote set_cover\n    # split_method='split_by_mv', # split_to_all split_by_mv\n\n    # @@ for directional but on monomer umis without other methods.\n    # is_collapse_block=True,  # True False\n    # collapse_block_method='majority_vote',  # majority_vote take_by_order\n    # deduped_method='',  # majority_vote set_cover\n    # split_method='',  # split_to_all split_by_mv\n\n    # param_fpn=to('data/params_dimer.yml'),\n    param_fpn=to('data/params_trimer.yml'),\n    # param_fpn=to('data/params.yml'),\n\n    verbose=False, # True False\n)\n</code></pre></p> <p> <code>console</code> <pre><code>30/07/2024 21:25:34 logger: ======&gt;key 1: work_dir\n30/07/2024 21:25:34 logger: =========&gt;value: /mnt/d/Document/Programming/Python/umiche/umiche/data/simu/umiche/trimer/\n30/07/2024 21:25:34 logger: ======&gt;key 2: trimmed\n30/07/2024 21:25:34 logger: =========&gt;value: {'fastq': {'fpn': 'None', 'trimmed_fpn': 'None'}, 'umi_1': {'len': 36}, 'seq': {'len': 100}, 'read_struct': 'umi_1'}\n30/07/2024 21:25:34 logger: ======&gt;key 3: fixed\n30/07/2024 21:25:34 logger: =========&gt;value: {'pcr_num': 8, 'pcr_err': 1e-05, 'seq_err': 0.001, 'ampl_rate': 0.85, 'seq_dep': 400, 'umi_num': 50, 'permutation_num': 10, 'umi_unit_pattern': 3, 'umi_unit_len': 12, 'seq_sub_spl_rate': 0.333, 'sim_thres': 3}\n30/07/2024 21:25:34 logger: ======&gt;key 4: varied\n30/07/2024 21:25:34 logger: =========&gt;value: {'pcr_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'pcr_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.05], 'seq_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.2], 'ampl_rates': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'umi_lens': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], 'umi_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], 'seq_deps': [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000]}\n30/07/2024 21:25:34 logger: ======&gt;key 5: dedup\n30/07/2024 21:25:34 logger: =========&gt;value: {'dbscan_eps': 1.5, 'dbscan_min_spl': 1, 'birch_thres': 1.8, 'birch_n_clusters': 'None', 'hdbscan_min_spl': 3, 'aprop_preference': 'None', 'aprop_random_state': 0, 'ed_thres': 1, 'mcl_fold_thres': 1.6, 'inflat_val': 2.7, 'exp_val': 2, 'iter_num': 100}\nUMI homopolymer recurring pattern: 3\n===&gt;Permutation number: 0\n============&gt;No.0, dedup cnt: 50.0\n============&gt;No.1, dedup cnt: 50.0\n============&gt;No.2, dedup cnt: 50.0\n============&gt;No.3, dedup cnt: 50.0\n============&gt;No.4, dedup cnt: 50.0\n============&gt;No.5, dedup cnt: 50.0\n============&gt;No.6, dedup cnt: 50.0\n============&gt;No.7, dedup cnt: 50.0\n============&gt;No.8, dedup cnt: 50.0\n============&gt;No.9, dedup cnt: 50.0\n============&gt;No.10, dedup cnt: 50.0\n============&gt;No.11, dedup cnt: 50.0\n============&gt;No.12, dedup cnt: 50.0\n============&gt;No.13, dedup cnt: 51.0\n============&gt;No.14, dedup cnt: 55.0\n============&gt;No.15, dedup cnt: 77.0\n============&gt;No.16, dedup cnt: 115.0\n============&gt;No.17, dedup cnt: 338.0\n===&gt;Permutation number: 1\n============&gt;No.0, dedup cnt: 50.0\n============&gt;No.1, dedup cnt: 50.0\n============&gt;No.2, dedup cnt: 50.0\n============&gt;No.3, dedup cnt: 50.0\n============&gt;No.4, dedup cnt: 50.0\n============&gt;No.5, dedup cnt: 50.0\n============&gt;No.6, dedup cnt: 50.0\n============&gt;No.7, dedup cnt: 50.0\n============&gt;No.8, dedup cnt: 50.0\n============&gt;No.9, dedup cnt: 50.0\n============&gt;No.10, dedup cnt: 50.0\n============&gt;No.11, dedup cnt: 50.0\n============&gt;No.12, dedup cnt: 50.0\n============&gt;No.13, dedup cnt: 51.0\n============&gt;No.14, dedup cnt: 55.0\n============&gt;No.15, dedup cnt: 76.0\n============&gt;No.16, dedup cnt: 112.0\n...\n</code></pre></p> <p>Note</p> <p>The  <code>params_trimer.yml</code> file configures the parameters for simulated reads and those for deduplication. Please see the relevant page for details.</p>"},{"location":"tutorial/m3.%20count%20matrix/DeepConvCVAE/","title":"DeepConvCVAE","text":""},{"location":"tutorial/m3.%20count%20matrix/DeepConvCVAE/#vae","title":"VAE","text":"<p>A variational autoencoder (VAE)<sup>1</sup> is a generative model in machine learning designed to encode input data into a latent space and subsequently decode it to reconstruct the original input. VAEs excel at generating new data samples that resemble a given dataset and are also valuable for various unsupervised learning tasks.</p> <p>We utilised a variant of the variational autoencoder (VAE), the conditional VAE (CVAE)<sup>2</sup>, to simulate transcriptomics data conditioned on cell types. The CVAE allows for training simulators on highly dimensional, yet sparse single-cell gene expression data at a significantly accelerated speed. Additionally, it integrates various types of neural networks into the parameterized statistical inference process, offering greater flexibility than purely statistical inference methods for denoising multimodally distributed data.</p> <p>Background: scRNA-seq count matrix simulation</p> <p>The recent surge of interest in scRNA-seq stems from its ability to elucidate the correlation between genetics and diseases at the level of individual cell types with greater granularity. To discern the cell-specific gene expression landscapes, numerous scRNA-seq analysis tools have emerged. The reliability of these tools in practical applications hinges on their evaluation against realistically labelled data. Consequently, many simulators have been developed to ensure the availability of ground truth data. Most of these simulations are based on statistical inference techniques, using well-estimated parameters of probability distributions to characterise given scRNA-seq data. However, statistical inference methods have two limitations: long training time and a lack of denoising strategies. Firstly, methods in this category typically lack technical support from computing acceleration technologies, such as graphics processing units (GPUs), particularly when compared to deep learning computing libraries. However, the practical utility of simulated data varies across cell, tissue, and disease types, potentially necessitating model retraining to accommodate a range of application conditions. On the other hand, the inherent variability of complex biological systems often eludes capture by rigid statistical inference methods that rely solely on fixed probability distribution assumptions. This limitation may constrain their ability to effectively denoise data exhibiting multimodal distributions.</p> <p>Point</p> <p>A count matrix can be simulated on-demand.</p>"},{"location":"tutorial/m3.%20count%20matrix/DeepConvCVAE/#programming_cvae","title":"Programming CVAE","text":"<p>We implemented a CVAE framework based on convolutional neural networks.</p> <p>Warning</p> <p>Please note that before using it, we need to install <code>tensorflow</code>.</p> <pre><code>pip install tensorflow\n</code></pre> <p>The following vignette shows the framework.</p> <p> <code>Python</code> <pre><code>class condiConvVAE:\n\n    def __init__(\n            self,\n            input_shape,\n            image_size,\n            label_shape,\n            batch_size,\n            kernel_size,\n            filters,\n            latent_dim,\n            strides,\n            epochs,\n            inputs,\n            y_labels,\n    ):\n        self.image_size = image_size\n        self.input_shape = input_shape\n        self.label_shape = label_shape\n        self.batch_size = batch_size\n        self.kernel_size = kernel_size\n        self.filters = filters\n        self.latent_dim = latent_dim\n        self.strides = strides\n        self.epochs = epochs\n        self.inputs = inputs\n        self.y_labels = y_labels\n\n    def reparameterize(self, params):\n        z_mean, z_log_var = params\n        batch = K.shape(z_mean)[0]\n        dim = K.int_shape(z_mean)[1]\n        epsilon = K.random_normal(shape=(batch, dim))\n        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n    def loss(self, outputs, z_mean, z_log_var):\n        # VAE loss = mse_loss or xent_loss + kl_loss\n        beta = 1.0\n        # rloss = tf.keras.losses.binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n        rloss = tf.keras.losses.mse(K.flatten(self.inputs), K.flatten(outputs))\n        rloss *= self.image_size * self.image_size\n        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n        kl_loss = K.sum(kl_loss, axis=-1)\n        kl_loss *= -0.5 * beta\n        return K.mean(rloss + kl_loss)\n\n    def encoding(self, ):\n        x = tf.keras.layers.Dense(self.image_size * self.image_size)(self.y_labels)\n        x = tf.keras.layers.Reshape((self.image_size, self.image_size, 1))(x)\n        x = tf.keras.layers.concatenate([self.inputs, x])\n        x = tf.keras.layers.Conv2D(self.filters, self.kernel_size, self.strides, padding='same', activation='relu')(x)\n        x = tf.keras.layers.Conv2D(self.filters * 2, self.kernel_size, self.strides, padding='same', activation='relu')(x)\n        # for decoder\n        shape = K.int_shape(x)\n        # generate latent vector Q(z|X)\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(16, activation='relu')(x)\n        z_mean = tf.keras.layers.Dense(self.latent_dim, name='z_mean')(x)\n        z_log_var = tf.keras.layers.Dense(self.latent_dim, name='z_log_var')(x)\n\n        # use reparameterization trick to push the sampling out as input\n        z = tf.keras.layers.Lambda(self.reparameterize, output_shape=(self.latent_dim,), name='z',)([z_mean, z_log_var])\n        encoder = tf.keras.models.Model(inputs=[self.inputs, self.y_labels], outputs=[z_mean, z_log_var, z], name='encoder')\n        return shape, z_mean, z_log_var, encoder\n\n    def decoding(self, shape,):\n        latent_inputs = tf.keras.layers.Input(shape=(self.latent_dim,), name='z_sampling')\n        x = tf.keras.layers.concatenate([latent_inputs, self.y_labels])\n        x = tf.keras.layers.Dense(shape[1]*shape[2]*shape[3], activation='relu')(x)\n        x = tf.keras.layers.Reshape((shape[1], shape[2], shape[3]))(x)\n        x = tf.keras.layers.Conv2DTranspose(self.filters, self.kernel_size, self.strides, padding='same', activation='relu')(x)\n        x = tf.keras.layers.Conv2DTranspose(self.filters / 2, self.kernel_size, self.strides, padding='same', activation='relu')(x)\n        x = tf.keras.layers.Conv2DTranspose(1, self.kernel_size, padding='same', activation='sigmoid', name='decoder_output')(x)\n        decoder = tf.keras.models.Model(inputs=[latent_inputs, self.y_labels], outputs=x, name='decoder')\n        return decoder\n</code></pre></p>"},{"location":"tutorial/m3.%20count%20matrix/DeepConvCVAE/#simulation","title":"Simulation","text":"<p>We have trained a model which we can use to simulate 100 cells (<code>num_dots=100</code>) per cell cluster. The final simulated count matrix will be saved in <code>h5</code> format.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nuc.mat.simu_deepconvcvae(\n    image_size=144,\n    batch_size=32,\n    kernel_size=3,\n    filters=16,\n    latent_dim=2,\n    strides=2,\n    epochs=1,\n    num_dots=100,\n    model_fpn='../data/deepconvcvae/cvae_model50/cvae_model50.tf',\n    sv_fpn='../data/deepconvcvae/cvae_model50/cvae_model50.h5',\n)\n</code></pre></p> <p> <code>console</code> <pre><code>KerasTensor(type_spec=TensorSpec(shape=(None, 144, 144, 1), dtype=tf.float32, name='encoder_input'), name='encoder_input', description=\"created by layer 'encoder_input'\")\nKerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='class_labels'), name='class_labels', description=\"created by layer 'class_labels'\")\n2024-07-30 22:41:02.531703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n===&gt;label 0\n1/1 [==============================] - 0s 292ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n...\n</code></pre></p>"},{"location":"tutorial/m3.%20count%20matrix/DeepConvCVAE/#training","title":"Training","text":"<p>We use 68k PBMCs scRNA-seq data downloaded from https://github.com/10XGenomics/single-cell-3prime-paper/tree/master/pbmc68k_analysis. It has the content.</p> <ul> <li> <code>fresh_68k_pbmc_donor_a_filtered_gene_bc_matrices.tar</code></li> <li> <code>68k_pbmc_barcodes_annotation.tsv</code> </li> </ul> <p>We use an in-house data processing method for filtering cells and genes that are not proper for analysis. This method can be programmatically accessed with <code>uc.mat.data_in</code>. We can use it to read the data and further get the portions for training and testing.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nx_train, y_train, x_test, y_test = uc.mat.data_in(\n    params={\n        'data_fpn': to('data/deepconvcvae/flt.h5'),\n        'data_rv_zero_fpn': to('data/deepconvcvae/flt_rv_zero_col.h5'),\n    },\n    image_size=144,\n)\n</code></pre></p> <p> <code>console</code> <pre><code>Train: (41147,) Test: (27432,)\nTrain: (41147,) Test: (27432,)\nTrain: (41147,) Test: (27432,)\nTrain: (41147,) Test: (27432,)\nTrain: (41147,) Test: (27432,)\n['CD8+ Cytotoxic T', 'CD8+/CD45RA+ Naive Cytotoxic', 'CD4+/CD45RO+ Memory', 'CD19+ B', 'CD4+/CD25 T Reg', 'CD56+ NK', 'CD4+ T Helper2', 'CD4+/CD45RA+/CD25- Naive T', 'CD34+', 'Dendritic', 'CD14+ Monocyte']\n(41147, 144, 144, 1)\n(41147,)\n(27432, 144, 144, 1)\n(27432,)\n</code></pre></p> <p>The following code will allow us to train the ConvCVAE framework. We can actually use any other types of training data, provided that you have tailored it to suit the input format (i.e. <code>numpy.ndarray</code>).</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nuc.mat.train_deepconvcvae(\n    x_train=x_train,\n    y_train=y_train,\n    x_test=x_test,\n    y_test=y_test,\n    image_size=144,\n    batch_size=32,\n    kernel_size=3,\n    filters=16,\n    latent_dim=2,\n    strides=2,\n    epochs=1,\n    sv_fpn='./cvae',\n)\n</code></pre></p> <p> <code>console</code> <pre><code>KerasTensor(type_spec=TensorSpec(shape=(None, 144, 144, 1), dtype=tf.float32, name='encoder_input'), name='encoder_input', description=\"created by layer 'encoder_input'\")\nKerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='class_labels'), name='class_labels', description=\"created by layer 'class_labels'\")\n2024-07-30 23:13:52.493677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nModel: \"ccvae\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 144, 144, 1)]        0         []                            \n\n class_labels (InputLayer)   [(None, 11)]                 0         []                            \n\n encoder (Functional)        [(None, 2),                  917412    ['encoder_input[0][0]',       \n                              (None, 2),                             'class_labels[0][0]']        \n                              (None, 2)]                                                          \n\n decoder (Functional)        (None, 144, 144, 1)          586465    ['encoder[0][2]',             \n                                                                     'class_labels[0][0]']        \n\n dense (Dense)               (None, 20736)                248832    ['class_labels[0][0]']        \n\n reshape (Reshape)           (None, 144, 144, 1)          0         ['dense[0][0]']               \n\n concatenate (Concatenate)   (None, 144, 144, 2)          0         ['encoder_input[0][0]',       \n                                                                     'reshape[0][0]']             \n\n conv2d (Conv2D)             (None, 72, 72, 16)           304       ['concatenate[0][0]']         \n\n conv2d_1 (Conv2D)           (None, 36, 36, 32)           4640      ['conv2d[0][0]']              \n\n flatten (Flatten)           (None, 41472)                0         ['conv2d_1[0][0]']            \n\n dense_1 (Dense)             (None, 16)                   663568    ['flatten[0][0]']             \n\n z_log_var (Dense)           (None, 2)                    34        ['dense_1[0][0]']             \n\n z_mean (Dense)              (None, 2)                    34        ['dense_1[0][0]']             \n\n tf.reshape_1 (TFOpLambda)   (None,)                      0         ['decoder[0][0]']             \n\n tf.reshape (TFOpLambda)     (None,)                      0         ['encoder_input[0][0]']       \n\n tf.__operators__.add (TFOp  (None, 2)                    0         ['z_log_var[0][0]']           \n Lambda)                                                                                          \n\n tf.math.square (TFOpLambda  (None, 2)                    0         ['z_mean[0][0]']              \n )                                                                                                \n\n tf.convert_to_tensor (TFOp  (None,)                      0         ['tf.reshape_1[0][0]']        \n Lambda)                                                                                          \n\n tf.cast (TFOpLambda)        (None,)                      0         ['tf.reshape[0][0]']          \n\n tf.math.subtract (TFOpLamb  (None, 2)                    0         ['tf.__operators__.add[0][0]',\n da)                                                                 'tf.math.square[0][0]']      \n\n tf.math.exp (TFOpLambda)    (None, 2)                    0         ['z_log_var[0][0]']           \n\n tf.math.squared_difference  (None,)                      0         ['tf.convert_to_tensor[0][0]',\n  (TFOpLambda)                                                       'tf.cast[0][0]']             \n\n tf.math.subtract_1 (TFOpLa  (None, 2)                    0         ['tf.math.subtract[0][0]',    \n mbda)                                                               'tf.math.exp[0][0]']         \n\n tf.math.reduce_mean (TFOpL  ()                           0         ['tf.math.squared_difference[0\n ambda)                                                             ][0]']                        \n\n tf.math.reduce_sum (TFOpLa  (None,)                      0         ['tf.math.subtract_1[0][0]']  \n mbda)                                                                                            \n\n tf.math.multiply (TFOpLamb  ()                           0         ['tf.math.reduce_mean[0][0]'] \n da)                                                                                              \n\n tf.math.multiply_1 (TFOpLa  (None,)                      0         ['tf.math.reduce_sum[0][0]']  \n mbda)                                                                                            \n\n tf.__operators__.add_1 (TF  (None,)                      0         ['tf.math.multiply[0][0]',    \n OpLambda)                                                           'tf.math.multiply_1[0][0]']  \n\n tf.math.reduce_mean_1 (TFO  ()                           0         ['tf.__operators__.add_1[0][0]\n pLambda)                                                           ']                            \n\n add_loss (AddLoss)          ()                           0         ['tf.math.reduce_mean_1[0][0]'\n                                                                    ]                             \n\n==================================================================================================\nTotal params: 1503877 (5.74 MB)\nTrainable params: 1503877 (5.74 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n  18/1286 [..............................] - ETA: 2:58 - loss: 3376.0042\n</code></pre></p> <ol> <li> <p>Cemgil, T., Ghaisas, S., Dvijotham, K., Gowal, S., &amp; Kohli, P. (2020). The autoencoding variational autoencoder. Advances in Neural Information Processing Systems, 33, 15077-15087.\u00a0\u21a9</p> </li> <li> <p>Lopez-Martin, M., Carro, B., Sanchez-Esguevillas, A., &amp; Lloret, J. (2017). Conditional variational autoencoder for prediction and feature recovery applied to intrusion detection in iot. Sensors, 17(9), 1967.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/m4.%20visualisation/Anchor-captured-read-efficiency/","title":"Anchor captured read efficiency","text":"<p>We use the following code to generate the plot. This function visualizes the anchor detection efficiency by plotting the percentage of total reads captured and the percentage of anchor-containing reads across different error rates. You can switch the condition argument to analyze other types of error scenarios.</p> <p>We set four conditions: PCR deletion error rate, Sequencing deletion error rate, Sequencing insertion error rate, and PCR insertion error rate.</p> <pre><code>uc.vis.anchor_efficiency(\n    criteria=criteria,\n    quant_captured=pct_reads,\n    quant_anchor_captured=pct_anchor_reads,\n    condition='PCR deletion error rate',\n)\n</code></pre> Fig 1. Percentage of captured reads with or without an anchor. <p>In addition, we can use the following code to visualise it with bars.</p> <pre><code>uc.vis.anchor_efficiency_simple(\n    criteria=criteria,\n    quant_captured=pct_reads,\n    quant_anchor_captured=pct_anchor_reads,\n    condition='PCR deletion error rate',\n    # condition='Sequencing deletion error rate',\n    # condition='Sequencing insertion error rate',\n    # condition='PCR insertion error rate',\n)\n</code></pre> Fig 1. Percentage of captured reads with or without an anchor."},{"location":"tutorial/m4.%20visualisation/Tracing-UMI-trajectory/","title":"Tracing UMI trajectory","text":"<p>We recorded the information about UMI identities as soon as there is a merging operation. We can visualise them using methods alone or in bulk.</p>"},{"location":"tutorial/m4.%20visualisation/Tracing-UMI-trajectory/#one_method_alone","title":"One method alone","text":"<p> read the statistics</p> <p> <code>Python</code> <pre><code>from umiche.deduplicate.io.Stat import Stat as dedupstat\nscenarios = {\n    'pcr_nums': 'PCR cycle',\n    # 'pcr_errs': 'PCR error',\n    # 'seq_errs': 'Sequencing error',\n    # 'ampl_rates': 'Amplification rate',\n    # 'umi_lens': 'UMI length',\n    # 'seq_deps': 'Sequencing depth',\n}\nmethods = {\n    # 'unique': 'Unique',\n    # 'cluster': 'Cluster',\n    # 'adjacency': 'Adjacency',\n    'directional': 'Directional',\n    # 'dbscan_seq_onehot': 'DBSCAN',\n    # 'birch_seq_onehot': 'Birch',\n    # 'aprop_seq_onehot': 'Affinity Propagation',\n    # 'mcl': 'MCL',\n    # 'mcl_val': 'MCL-val',\n    # 'mcl_ed': 'MCL-ed',\n}\ndedupstat11 = dedupstat(\n    scenarios=scenarios,\n    methods=methods,\n    param_fpn=to('data/params.yml'),\n)\n</code></pre></p> <p> draw the plot</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nuc.plot.trace_single(\n    df_apv=dedupstat11.df_trace_cnt['apv'],\n    df_disapv=dedupstat11.df_trace_cnt['disapv'],\n).line_apv_disapv()\n</code></pre></p> Fig 1. Counts of merged or not merged UMIs derived from the same or different origin using the Directional method."},{"location":"tutorial/m4.%20visualisation/Tracing-UMI-trajectory/#multiple_methods_and_panels","title":"Multiple methods and panels","text":"<p> read the statistics</p> <p> <code>Python</code> <pre><code>from umiche.deduplicate.io.Stat import Stat as dedupstat\nscenarios = {\n    'pcr_nums': 'PCR cycle',\n    # 'pcr_errs': 'PCR error',\n    # 'seq_errs': 'Sequencing error',\n    # 'ampl_rates': 'Amplification rate',\n    # 'umi_lens': 'UMI length',\n    # 'seq_deps': 'Sequencing depth',\n}\nmethods = {\n    'unique': 'Unique',\n    'cluster': 'Cluster',\n    'adjacency': 'Adjacency',\n    'directional': 'Directional',\n    'dbscan_seq_onehot': 'DBSCAN',\n    'birch_seq_onehot': 'Birch',\n    'aprop_seq_onehot': 'Affinity Propagation',\n    'mcl': 'MCL',\n    'mcl_val': 'MCL-val',\n    'mcl_ed': 'MCL-ed',\n}\ndedupstat22 = dedupstat(\n    scenarios=scenarios,\n    methods=methods,\n    param_fpn=to('data/params.yml'),\n)\n</code></pre></p> <p> draw the plot</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nuc.plot.trace_multiple(\n    df_apv=dedupstat22.df_trace_cnt['apv'],\n    df_disapv=dedupstat22.df_trace_cnt['disapv'],\n    scenarios=scenarios,\n    methods=methods,\n).line_apv()\n</code></pre></p> Fig 1. Counts of UMIs derived from the same origin to be merged across multiple PCR cycles with different UMI collapsing methods."},{"location":"tutorial/m4.%20visualisation/UMI-dedup-multi-panel/","title":"UMI dedup multi panel","text":"<p>We build a function to handle and plot complex deduplication data with regard to different sequencing conditions and deduplication methods. The final results will be rendered on a multiple-panel lineplot.</p> <p> Deduplication methods</p> <p>We can first define deduplication methods.</p> <pre><code>methods={\n    'unique': 'Unique',\n    'cluster': 'Cluster',\n    'adjacency': 'Adjacency',\n    'directional': 'Directional',\n    'dbscan_seq_onehot': 'DBSCAN',\n    'birch_seq_onehot': 'Birch',\n    'aprop_seq_onehot': 'Affinity Propagation',\n    'mcl': 'MCL',\n    'mcl_val': 'MCL-val',\n    'mcl_ed': 'MCL-ed',\n}\n</code></pre> <p> Sequencing conditions</p> <pre><code>scenarios={\n    'pcr_nums': 'PCR cycle',\n    'pcr_errs': 'PCR error',\n    'seq_errs': 'Sequencing error',\n    'ampl_rates': 'Amplification rate',\n    'umi_lens': 'UMI length',\n    'seq_deps': 'Sequencing depth',\n}\n</code></pre> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf = uc.plot.dedup_multiple(\n    scenarios=scenarios,\n    methods=methods,\n    param_fpn=to('data/params.yml'),\n)\nprint(plot_dm.line())\n</code></pre></p> <p> <code>console</code> <pre><code>31/07/2024 00:20:57 logger: ======&gt;key 1: work_dir\n31/07/2024 00:20:57 logger: =========&gt;value: D:/Document/Programming/Python/umiche/umiche/data/simu/mclumi/\n31/07/2024 00:20:57 logger: ======&gt;key 2: trimmed\n31/07/2024 00:20:57 logger: =========&gt;value: {'fastq': {'fpn': 'None', 'trimmed_fpn': 'None'}, 'umi_1': {'len': 10}, 'seq': {'len': 100}, 'read_struct': 'umi_1'}\n31/07/2024 00:20:57 logger: ======&gt;key 3: fixed\n31/07/2024 00:20:57 logger: =========&gt;value: {'pcr_num': 8, 'pcr_err': 1e-05, 'seq_err': 0.001, 'ampl_rate': 0.85, 'seq_dep': 400, 'umi_num': 50, 'permutation_num': 2, 'umi_unit_pattern': 1, 'umi_unit_len': 10, 'seq_sub_spl_rate': 0.333, 'sim_thres': 3}\n31/07/2024 00:20:57 logger: ======&gt;key 4: varied\n31/07/2024 00:20:57 logger: =========&gt;value: {'pcr_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'pcr_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.05], 'seq_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1], 'ampl_rates': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'umi_lens': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], 'umi_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], 'seq_deps': [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000]}\n31/07/2024 00:20:57 logger: ======&gt;key 5: dedup\n31/07/2024 00:20:57 logger: =========&gt;value: {'dbscan_eps': 1.5, 'dbscan_min_spl': 1, 'birch_thres': 1.8, 'birch_n_clusters': 'None', 'hdbscan_min_spl': 3, 'aprop_preference': 'None', 'aprop_random_state': 0, 'ed_thres': 1, 'mcl_fold_thres': 1.6, 'iter_num': 100, 'inflat_val': [1.1, 2.7, 3.6], 'exp_val': 2}\n31/07/2024 00:20:57 logger: ======&gt;key 1: work_dir\n31/07/2024 00:20:57 logger: =========&gt;value: D:/Document/Programming/Python/umiche/umiche/data/simu/mclumi/\n31/07/2024 00:20:57 logger: ======&gt;key 2: trimmed\n31/07/2024 00:20:57 logger: =========&gt;value: {'fastq': {'fpn': 'None', 'trimmed_fpn': 'None'}, 'umi_1': {'len': 10}, 'seq': {'len': 100}, 'read_struct': 'umi_1'}\n31/07/2024 00:20:57 logger: ======&gt;key 3: fixed\n31/07/2024 00:20:57 logger: =========&gt;value: {'pcr_num': 8, 'pcr_err': 1e-05, 'seq_err': 0.001, 'ampl_rate': 0.85, 'seq_dep': 400, 'umi_num': 50, 'permutation_num': 2, 'umi_unit_pattern': 1, 'umi_unit_len': 10, 'seq_sub_spl_rate': 0.333, 'sim_thres': 3}\n31/07/2024 00:20:57 logger: ======&gt;key 4: varied\n31/07/2024 00:20:57 logger: =========&gt;value: {'pcr_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'pcr_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.05], 'seq_errs': [1e-05, 2.5e-05, 5e-05, 7.5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1], 'ampl_rates': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'umi_lens': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], 'umi_nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], 'seq_deps': [100, 200, 500, 600, 800, 1000, 2000, 3000, 5000]}\n31/07/2024 00:20:57 logger: ======&gt;key 5: dedup\n31/07/2024 00:20:57 logger: =========&gt;value: {'dbscan_eps': 1.5, 'dbscan_min_spl': 1, 'birch_thres': 1.8, 'birch_n_clusters': 'None', 'hdbscan_min_spl': 3, 'aprop_preference': 'None', 'aprop_random_state': 0, 'ed_thres': 1, 'mcl_fold_thres': 1.6, 'iter_num': 100, 'inflat_val': [1.1, 2.7, 3.6], 'exp_val': 2}\n\npn0   pn1   pn2   pn3  ...      max-mean          scenario  method  metric\n0   0.00  0.00  0.00  0.00  ...  0.000000e+00         PCR cycle  Unique       1\n1   0.02  0.02  0.02  0.02  ...  3.321743e-18         PCR cycle  Unique       2\n2   0.04  0.04  0.04  0.04  ...  6.643485e-18         PCR cycle  Unique       3\n3   0.06  0.06  0.06  0.06  ...  7.462728e-03         PCR cycle  Unique       4\n4   0.20  0.18  0.20  0.20  ...  9.860566e-03         PCR cycle  Unique       5\n..   ...   ...   ...   ...  ...           ...               ...     ...     ...\n4   0.00  0.00  0.00  0.00  ...  0.000000e+00  Sequencing depth  MCL-ed     800\n5   0.00  0.00  0.00  0.00  ...  0.000000e+00  Sequencing depth  MCL-ed    1000\n6   0.00  0.00  0.00  0.00  ...  0.000000e+00  Sequencing depth  MCL-ed    2000\n7   0.00  0.00  0.00  0.00  ...  0.000000e+00  Sequencing depth  MCL-ed    3000\n8   0.00  0.00  0.00  0.00  ...  0.000000e+00  Sequencing depth  MCL-ed    5000\n\n[790 rows x 19 columns]\n</code></pre></p> Fig 1. UMI deduplication with multiple methods under multiple conditions"},{"location":"tutorial/m4.%20visualisation/UMI-dedup-single-panel/","title":"UMI dedup single panel","text":"<p>We can use UMIche to draw deduplication performance of Markov clustering versus its two important parameters, <code>inflation</code> and <code>expansion</code>. </p> <p> Tabulate the statistics</p> <p> <code>Python</code> <pre><code>from umiche.io import stat\n\nscenarios = {\n    # 'pcr_nums': 'PCR cycle',\n    # 'pcr_errs': 'PCR error',\n    'seq_errs': 'Sequencing error',\n    # 'ampl_rates': 'Amplification rate',\n    # 'umi_lens': 'UMI length',\n    # 'seq_deps': 'Sequencing depth',\n}\nmethods = {\n    # 'unique': 'Unique',\n    # 'cluster': 'Cluster',\n    # 'adjacency': 'Adjacency',\n    'directional': 'Directional',\n    # 'dbscan_seq_onehot': 'DBSCAN',\n    # 'birch_seq_onehot': 'Birch',\n    # 'aprop_seq_onehot': 'Affinity Propagation',\n    'mcl': 'MCL',\n    'mcl_val': 'MCL-val',\n    'mcl_ed': 'MCL-ed',\n}\ndedupstat = stat(\n    scenarios=scenarios,\n    methods=methods,\n    param_fpn=to('data/params.yml'),\n)\n\ndf_dedup = dedupstat.df_dedup\ndf_dedup_perm_melt = dedupstat.df_dedup_perm_melt\n</code></pre></p> <p> Define <code>uc.plot.dedup_single</code> for better disease biology study.</p> <p> <code>Python</code> <pre><code>import umiche as uc\n\nt = uc.plot.dedup_single(\n    df_dedup=df_dedup,\n    df_dedup_perm_melt=df_dedup_perm_melt,\n)\nt.strip()\n</code></pre></p> Fig 1. Jointgrid plot. <p> <code>Python</code> <pre><code>t.jointgrid()\n</code></pre></p> Fig 2. Strip plot. <p> <code>Python</code> <pre><code>t.stackedbar()\n</code></pre></p> Fig 3. Stackedbar plot."},{"location":"tutorial/m4.%20visualisation/UMI-dedup-trimer/","title":"UMI dedup trimer","text":"<p>We generated a few files for deduplicating homotrimer UMIs. They are shown in the below code section.</p> <pre><code>methods={\n    'directional_dedupby_majority_vote_splitby__collblockby_take_by_order': 'UMI-tools+drMV+cbRAN',\n    'directional_dedupby_majority_vote_splitby__collblockby_majority_vote': 'UMI-tools+drMV+cbMV',\n    'directional_dedupby_set_cover_splitby_split_by_mv_collblockby_take_by_order': 'UMI-tools+drSC+spMV+cbRAN',\n    'directional_dedupby_set_cover_splitby_split_by_mv_collblockby_majority_vote': 'UMI-tools+drSC+spMV+cbMV',\n    'directional_dedupby_set_cover_splitby_split_to_all_collblockby_take_by_order': 'UMI-tools+drSC+spALL+cbRAN',\n    'directional_dedupby_set_cover_splitby_split_to_all_collblockby_majority_vote': 'UMI-tools+drSC+spALL+cbMV',\n}\n</code></pre> <p>We can use <code>uc.plot.dedup_multiple_trimer</code> to directly draw a line plot for understanding their deduplication effects.</p> <p> <code>Python</code> <pre><code>from umiche import uc\n\nuc.plot.dedup_multiple_trimer(\n    scenarios={\n        'seq_errs': 'Sequencing error rate',\n    },\n    methods=methods,\n    param_fpn=to('data/params_trimer.yml'),\n).line()\n</code></pre></p> Fig 1. Comparison of UMI-tools deduplication results between the majority vote and the set cover methods."},{"location":"tutorial/m4.%20visualisation/graph-cluster-example/","title":"Graph cluster example","text":"<p>We understand UMI deduplication processes better in a visiable manner sometimes due to the complexity between a set of UMIs. Starting from building a 6-node UMI graph, we deduplicate UMIs and plot the final graph.</p> <p> build a graph with data from UMI-tools</p> <p> <code>Python</code> <pre><code>graph_adj = {\n    'A': ['B', 'C', 'D'],\n    'B': ['A', 'C'],\n    'C': ['A', 'B'],\n    'D': ['A', 'E', 'F'],\n    'E': ['D'],\n    'F': ['D'],\n}\nprint(\"An adjacency list of a graph:\\n{}\".format(graph_adj))\n\nnode_val_sorted = pd.Series({\n    'A': 456,\n    'E': 90,\n    'D': 72,\n    'B': 2,\n    'C': 2,\n    'F': 1,\n})\nprint(\"Counts sorted:\\n{}\".format(node_val_sorted))\n</code></pre></p> <p> deduplicate UMIs with 6 methods</p> <p> <code>Python</code> <pre><code>### @@@ ******Connected components******\nfrom umiche.deduplicate.method.Cluster import Cluster as umiclust\nccs = umiclust().cc(graph_adj=graph_adj)\nprint(\"Connected components:\\n{}\".format(ccs))\n\n### @@@ ******UMI-tools Adjacency******\nfrom umiche.deduplicate.method.Adjacency import Adjacency as umiadj\nfrom umiche.deduplicate.method.Directional import Directional as umidirec\nfrom umiche.deduplicate.method.MarkovClustering import MarkovClustering as umimcl\ndedup_res_adj = umiadj().umi_tools(\n    connected_components=ccs,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    graph_adj=graph_adj,\n)\ndedup_res_adj_dc = umiadj().decompose(dedup_res_adj['clusters'])\nprint(\"deduplicated clusters (UMI-tools Adjacency):\\n{}\".format(dedup_res_adj_dc))\n\n### @@@ ******UMI-tools Directional******\nfrom umiche.deduplicate.method.Directional import Directional as umidirec\nfrom umiche.deduplicate.method.MarkovClustering import MarkovClustering as umimcl\ndedup_res_direc = umidirec().umi_tools(\n    connected_components=ccs,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    graph_adj=graph_adj,\n)\ndedup_res_direc_dc = umidirec().decompose(dedup_res_direc['clusters'])\nprint(\"deduplicated clusters (UMI-tools Directional):\\n{}\".format(dedup_res_direc_dc))\n\n### @@@ ******MCL******\nfrom umiche.deduplicate.method.MarkovClustering import MarkovClustering as umimcl\nmcl = umimcl(\n    inflat_val=1.6,\n    exp_val=2,\n    iter_num=100,\n)\ndf_mcl = mcl.dfclusters(\n    connected_components=ccs,\n    graph_adj=graph_adj,\n)\ndedup_res_mcl_dc = mcl.decompose(list_nd=df_mcl['clusters'].values)\nprint(\"deduplicated clusters (MCL):\\n{}\".format(dedup_res_mcl_dc))\n\n### @@@ ******MCL mcl_val******\ndf_mcl_val = mcl.maxval_val(\n    df_mcl_ccs=df_mcl,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    thres_fold=2,\n)\ndedup_res_mcl_val_dc = mcl.decompose(list_nd=df_mcl_val['clusters'].values)\nprint(\"deduplicated clusters decomposed (mcl_val):\\n{}\".format(dedup_res_mcl_val_dc))\ndedup_res_mcl_val_dc_full = mcl.get_full_subcc(ccs_dict=dedup_res_mcl_val_dc, mcl_ccs_dict=dedup_res_mcl_dc)\nprint(\"deduplicated clusters decomposed full list(mcl_val):\\n{}\".format(dedup_res_mcl_val_dc_full))\n\n### @@@ ******MCL mcl_ed******\nint_to_umi_dict = {\n    'A': 'ACGT',\n    'B': 'TCGT',\n    'C': 'CCGT',\n    'D': 'ACAT',\n    'E': 'ACAG',\n    'F': 'AAAT',\n}\ndf_mcl_ed = mcl.maxval_ed(\n    df_mcl_ccs=df_mcl,\n    df_umi_uniq_val_cnt=node_val_sorted,\n    thres_fold=1,\n    int_to_umi_dict=int_to_umi_dict,\n)\ndedup_res_mcl_ed_dc = mcl.decompose(list_nd=df_mcl_ed['clusters'].values)\nprint(\"deduplicated clusters decomposed (mcl_ed):\\n{}\".format(dedup_res_mcl_ed_dc))\ndedup_res_mcl_ed_dc_full = mcl.get_full_subcc(ccs_dict=dedup_res_mcl_ed_dc, mcl_ccs_dict=dedup_res_mcl_dc)\nprint(\"deduplicated clusters decomposed full list(mcl_ed):\\n{}\".format(dedup_res_mcl_ed_dc_full))\n</code></pre></p> <p> draw UMI nodes with deduplicated information</p> <p> <code>Python</code> <pre><code>fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(14, 9.5))\np = Graph(graph=graph_adj)\np.color_list = ['cornflowerblue', 'lightcoral', 'mediumseagreen',]\np.draw(ccs, ax=ax[0, 0], title='Cluster')\np.draw(dedup_res_adj_dc, title='Adjacent', ax=ax[0, 1])\np.draw(dedup_res_direc_dc, title='Directional', ax=ax[0, 2])\np.draw(dedup_res_mcl_dc, title='MCL', ax=ax[1, 0])\np.draw(dedup_res_mcl_val_dc_full, title='MCL-val', ax=ax[1, 1])\np.draw(dedup_res_mcl_ed_dc_full, title='MCL-ed', ax=ax[1, 2])\n\nplt.subplots_adjust(\n    top=0.92,\n    bottom=0.04,\n    left=0.04,\n    right=0.98,\n    hspace=0.10,\n    wspace=0.15,\n)\nplt.show()\n</code></pre></p> Fig 1. Graph-based identification of true molecules by collapsing UMIs using the cluster, adjacency, directional, mcl, mcl-ed, and mcl-val algorithms."},{"location":"tutorial/m4.%20visualisation/graph-cluster-true/","title":"Graph cluster true","text":"<p>We extracted a UMI graph from simulated data observed at a single locus.</p> <p>There are three Markov clusters.</p> <pre><code>dedup_cluster = [\n[69, 72, 838, 1221, 97, 210, 249, 315, 324, 374, 457, 658, 727, 760, 771, 933, 1073, 1126, 1198, 1260, 1271, 1307, 1498, 1505, 1541, 1563, 914, 946, 1083, 684, 1288, 1543, 822],\n[1174, 119, 290, 303, 204, 218, 289, 302, 404, 545, 586, 633, 674, 709, 720, 802, 884, 943, 980, 1355, 1436, 1488, 1553, 786, 537, 867, 1649, 1255, 701, 1080, 347, 251],\n[1315, 1549]\n],\n</code></pre> <p>The connected components are </p> <pre><code>ccs = {69: [72, 838, 1221],\n72: [69, 97, 210, 249, 315, 324, 374, 457, 658, 727, 760, 771, 838, 933, 1073, 1126, 1198, 1221, 1260,\n1271, 1307, 1498, 1505, 1541, 1563], 838: [69, 72, 1221], 1221: [69, 72, 838, 914, 946],\n97: [72, 658, 1505], 210: [72, 1083, 1271], 249: [72, 684, 727, 1073], 315: [72, 374, 946, 1126],\n324: [72, 771], 374: [72, 315, 1126], 457: [72, 1288, 1307], 658: [72, 97, 1505], 727: [72, 249, 1073],\n760: [72, 933, 1543], 771: [72, 324], 933: [72, 760], 1073: [72, 249, 727], 1126: [72, 315, 374],\n1198: [72, 1260, 1541], 1260: [72, 1083, 1198, 1541, 1543], 1271: [72, 210], 1307: [72, 457],\n1498: [72, 684, 1563], 1505: [72, 97, 658, 822], 1541: [72, 1198, 1260], 1563: [72, 914, 1498],\n914: [1221, 1563], 946: [315, 1221], 1083: [210, 1260], 684: [249, 1498], 1288: [457],\n1543: [760, 1174, 1260], 822: [1505], 1174: [119, 290, 303, 1315, 1543],\n119: [204, 218, 289, 290, 302, 303, 404, 545, 586, 633, 674, 709, 720, 802, 884, 943, 980, 1174, 1355,\n1436, 1488, 1553], 290: [119, 303, 786, 1174], 303: [119, 290, 1174], 1315: [1174, 1549],\n204: [119, 537, 720, 1488], 218: [119, 302, 674, 867, 1649], 289: [119, 980],\n302: [119, 218, 674, 1255], 404: [119, 802, 943], 545: [119, 701, 1436], 586: [119, 709, 884],\n633: [119, 1080, 1553], 674: [119, 218, 302], 709: [119, 586, 884], 720: [119, 204, 347, 1488],\n802: [119, 404, 537, 943], 884: [119, 586, 701, 709], 943: [119, 404, 802], 980: [119, 251, 289],\n1355: [119, 251, 867], 1436: [119, 545], 1488: [119, 204, 720], 1553: [119, 633, 786], 786: [290, 1553],\n1549: [1315], 537: [204, 802], 867: [218, 1355, 1649], 1649: [218, 867], 1255: [302], 701: [545, 884],\n1080: [633], 347: [720], 251: [980, 1355]},\n</code></pre> <p>The edge list of the UMI graph is</p> <pre><code>edge_list=[(97, 72), (658, 97), (1355, 119), (204, 119), (1649, 218), (1541, 72), (933, 72), (914, 1563),\n(1505, 72), (1221, 69), (404, 119), (1649, 867), (1307, 72), (302, 119), (802, 119), (1543, 760),\n(1126, 72), (709, 586), (786, 1553), (374, 315), (946, 315), (1073, 72), (720, 204), (933, 760),\n(1174, 1543), (210, 72), (771, 72), (537, 204), (727, 249), (1563, 72), (684, 1498), (822, 1505),\n(1549, 1315), (1505, 97), (545, 119), (374, 72), (1553, 119), (1436, 119), (867, 218), (457, 72),\n(1563, 1498), (709, 119), (720, 119), (1260, 1198), (1543, 1260), (218, 119), (251, 1355),\n(701, 545), (1541, 1260), (1488, 204), (1553, 633), (1083, 210), (980, 289), (119, 1174),\n(727, 72), (290, 119), (1073, 727), (1541, 1198), (884, 586), (802, 404), (303, 119), (347, 720),\n(674, 218), (1221, 838), (1221, 72), (914, 1221), (943, 119), (586, 119), (943, 802), (838, 72),\n(1271, 210), (980, 119), (674, 302), (786, 290), (251, 980), (1488, 720), (1488, 119),\n(946, 1221), (1073, 249), (884, 709), (1288, 457), (1126, 315), (943, 404), (537, 802),\n(302, 218), (249, 72), (315, 72), (303, 290), (290, 1174), (884, 119), (633, 119), (771, 324),\n(658, 72), (1505, 658), (1307, 457), (303, 1174), (72, 69), (289, 119), (838, 69), (684, 249),\n(1498, 72), (1315, 1174), (1126, 374), (701, 884), (1255, 302), (674, 119), (1080, 633),\n(1083, 1260), (1436, 545), (1260, 72), (1271, 72), (867, 1355), (760, 72), (1198, 72), (324, 72)]\n</code></pre> <p>The unique UMIs have the counts stored in a <code>umi_uniq_val_cnt-simu.txt</code> file. UMI IDs are listed on the left while their counts are shown on the right.</p> <pre><code>27  122\n119 23.8\n15  118\n13  115\n16  112\n126 109\n32  106\n72  20.8\n80  104\n7   104\n14  103\n47  103\n10  101\n58  100\n...\n</code></pre> <pre><code>import umiche as uc\n\nuc.plot.graph_cluster(\n    umi_uniq_val_cnt_fpn=to('data/simu/mclumi/seq_errs/umi_uniq_val_cnt-simu.txt'),\n    dedup_cluster=dedup_cluster,\n    ccs=ccs,\n    edge_list=edge_list,\n).draw()\n</code></pre> Fig 1. Graph representation of UMI clusters. Each node in the graphs represents a unique UMI. The node size is enlarged as the count of the unique UMIs increases. Different clusters are shown in different colours."},{"location":"tutorial/m4.%20visualisation/homotrimer-umi-error-proba-analysis/","title":"Homotrimer umi error proba analysis","text":"<p>The <code>uc.plot.prob_correct</code> and <code>uc.plot.prob_incorrect</code> functions allows you to plot the probability of correct and incorrect synthesis of homotrimer building blocks and full sequences across varying error rates. Please see this section for details.</p> <ol> <li>correct synthesis of a homotrimer building block (<code>num_nt=1</code>)</li> </ol> <pre><code>import numpy as np\n\nerror_rates= np.linspace(0.00001, 0.5, 500)\n\nuc.plot.prob_correct(error_rate=error_rates, num_nt=1)\n</code></pre> Fig 1. Probability of correct synthesis for a single nucleotide, a binary repetition code block of 3 bits, and a homotrimer block as a function of a single-base error rate \ud835\udc5d. <ol> <li>incorrect synthesis of a whole sequence (<code>num_nt=12</code>)</li> </ol> <pre><code>uc.plot.prob_incorrect(error_rate=error_rates, num_nt=12)\n</code></pre> Fig 2. Probability of incorrect synthesis for a 12-bp standard UMI, a 36-bit binary (3, 1) repetition code, and a 36-bp homotrimer UMI."},{"location":"tutorial/m4.%20visualisation/inflation-expansion/","title":"Inflation expansion","text":"<p>We can use UMIche to draw deduplication performance of Markov clustering versus its two important parameters, <code>inflation</code> and <code>expansion</code>. </p> <p> <code>Python</code> <pre><code>import umiche as uc\n\ndf = uc.plot.inflat_exp(\n    scenarios={\n        'pcr_nums': 'PCR cycle',\n        'pcr_errs': 'PCR error',\n        'seq_errs': 'Sequencing error',\n        'ampl_rates': 'Amplification rate',\n        'umi_lens': 'UMI length',\n        'seq_deps': 'Sequencing depth',\n    },\n    methods={\n        'mcl': 'MCL',\n    },\n    param_fpn=to('data/params.yml'),\n).draw()\n</code></pre></p> Fig 1. Fold change between estimated and actual deduplicated counts with respect to the inflation (a) and expansion (b) parameters of the MCL algorithm."},{"location":"tutorial/output/dedup-count/","title":"Dedup count","text":"<p>The deduplicated UMI counts are saved uniformly in <code>{method}_dedup.txt</code>.</p> <p>The column name means the round of permutation, e.g. <code>pn0</code> means the 1st round of permutation with respect to a certain sequencing condition, e.g. sequencing errors from 0.000001 to 0.1.</p> <pre><code>pn0 pn1 pn2 pn3 pn4 pn5 pn6 pn7 pn8 pn9\n50  50  50  50  50  50  50  50  50  49\n50  50  50  50  50  50  50  50  50  50\n50  50  50  50  50  50  50  50  50  50\n50  50  50  50  50  50  50  50  50  50\n51  51  51  51  51  51  51  51  51  51\n51  51  51  51  51  51  51  51  51  51\n51  51  51  51  51  51  51  52  51  51\n51  51  51  51  51  51  52  51  51  51\n50  50  50  50  50  50  50  50  50  50\n51  51  51  51  51  51  51  51  51  51\n50  50  51  50  51  51  50  51  51  50\n51  51  51  51  51  51  51  51  51  51\n51  51  51  51  51  51  51  51  51  51\n</code></pre> <p>In <code>set_cover_multi_len_split_by_mv_dedup.txt</code>, each cell means in each condition (e.g. at sequencing error rate 0.001) how many times are needed for set covering UMIs, with each element separated by <code>;</code> meaning how many steps are needed to merge UMIs each time.</p> <pre><code>pn0 pn1 pn2 pn3 pn4 pn5 pn6 pn7 pn8 pn9\n1;1;1;1 1;1;1;1 1;1;1   1;1;1   1;1;1   1;1 1;1;1   1;1;1   1;1;1;1 1;1;1\n1;1;1;1;1;1 1;1;1;1;1;1 1;1;1;1;1   2;1;1;1 2;1;1;1 1;1;1;1 1;1;1;1;1   1;1;1;1;1   1;1;1;1;1;1 1;1;1;1;1\n2;1;1;1;1;1;1;1;1;1;1;1 2;1;1;1;1;1;1;1;1;1;1;1 2;1;1;1;1;1;1;1;1;1;1   2;2;1;1;1;1;1;1;1;1 2;2;1;1;1;1;1;1;1;1 2;1;1;1;1;1;1;1;1;1 2;1;1;1;1;1;1;1;1;1;1   \n...\n138;130;126;126;124;120;119;117;115;114;112;112;111;110;110;110;110;109;109;107;107;106;106;105;103;102;100;100;99;97;91;91;90;89;89;86;85;75;73;70;70;63;63;57;56;45;44;41;35;27;5;5;5;5;5;4;4;4;4;4;4;4;4;4;4;4;4;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;3;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;2;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1;1\n</code></pre> <p>Below shows how many unique UMIs are or are not solved by set cover. Their combination means the total count of unique UMIs after set cover deduplication.</p> <code>set_cover_solved_split_by_mv_dedup.txt</code><code>set_cover_not_solved_split_by_mv_dedup.txt</code> <pre><code>pn0 pn1 pn2 pn3 pn4 pn5 pn6 pn7 pn8 pn9\n4.0 4.0 3.0 3.0 3.0 2.0 3.0 3.0 4.0 3.0\n6.0 6.0 5.0 4.0 4.0 4.0 5.0 5.0 6.0 5.0\n12.0    12.0    11.0    10.0    10.0    10.0    11.0    10.0    12.0    11.0\n16.0    16.0    14.0    14.0    13.0    14.0    15.0    14.0    16.0    15.0\n20.0    19.0    18.0    18.0    17.0    18.0    19.0    18.0    20.0    19.0\n32.0    31.0    31.0    31.0    30.0    31.0    31.0    31.0    33.0    32.0\n40.0    39.0    40.0    40.0    39.0    39.0    39.0    39.0    39.0    40.0\n48.0    48.0    48.0    48.0    48.0    48.0    48.0    48.0    48.0    48.0\n49.0    49.0    49.0    49.0    49.0    49.0    49.0    49.0    49.0    49.0\n50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0\n50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0\n50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0\n50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0    50.0\n52.0    51.0    52.0    51.0    51.0    51.0    52.0    53.0    50.0    51.0\n64.0    63.0    60.0    64.0    60.0    62.0    57.0    61.0    55.0    62.0\n91.0    95.0    97.0    97.0    88.0    104.0   88.0    76.0    91.0    87.0\n155.0   154.0   144.0   146.0   161.0   143.0   168.0   156.0   154.0   149.0\n519.0   544.0   571.0   558.0   522.0   551.0   539.0   533.0   555.0   557.0\n</code></pre> <pre><code>pn0 pn1 pn2 pn3 pn4 pn5 pn6 pn7 pn8 pn9\n46.0    46.0    47.0    47.0    47.0    48.0    47.0    47.0    46.0    47.0\n44.0    44.0    45.0    46.0    46.0    46.0    45.0    45.0    44.0    45.0\n38.0    38.0    39.0    40.0    40.0    40.0    39.0    40.0    38.0    39.0\n34.0    34.0    36.0    36.0    37.0    36.0    35.0    36.0    34.0    35.0\n30.0    31.0    32.0    32.0    33.0    32.0    31.0    32.0    30.0    31.0\n18.0    19.0    19.0    19.0    20.0    19.0    19.0    19.0    17.0    18.0\n10.0    11.0    10.0    10.0    11.0    11.0    11.0    11.0    11.0    10.0\n2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n3.0 3.0 1.0 1.0 1.0 2.0 1.0 0.0 1.0 2.0\n4.0 6.0 4.0 4.0 6.0 3.0 2.0 2.0 2.0 4.0\n8.0 7.0 10.0    8.0 5.0 9.0 9.0 7.0 7.0 8.0\n49.0    51.0    56.0    50.0    55.0    44.0    51.0    59.0    56.0    53.0\n153.0   149.0   175.0   172.0   169.0   163.0   161.0   165.0   165.0   160.0\n328.0   331.0   328.0   313.0   336.0   334.0   343.0   339.0   334.0   358.0\n467.0   455.0   463.0   472.0   482.0   483.0   451.0   460.0   516.0   501.0\n1002.0  975.0   918.0   933.0   960.0   933.0   995.0   969.0   964.0   919.0\n</code></pre>"},{"location":"tutorial/output/dedup-count/#read_umi_trajectory_files","title":"Read UMI trajectory files","text":"<p>The UMI trajectories across PCR amplification cycles are represented to be in a Python multiple dimensional dictionary. From outer to inner, the keys stand for:</p> <p>Permutation number sequencing condition :material-arrow-ri  ght-thin: connected component  UMI that are merging other UMIs  tuple (1. origin (UMI identity before PCR amplification); 2. merged UMIs that have the same origin; 3. merged UMIs that originate differently.</p> <pre><code>{\"0\": {\"1e-05\": {\"cc0\": {}, \"cc1\": {}, \"cc2\": {}, \"cc3\": {}, \"cc4\": {}, \"cc5\": {}, \"cc6\": {}, \"cc7\": {}, \"cc8\": {}, \"cc9\": {}, \"cc10\": {}, \"cc11\": {}, \"cc12\": {}, \"cc13\": {}, \"cc14\": {}, \"cc15\": {}, \"cc16\": {}, \"cc17\": {}, \"cc18\": {}, \"cc19\": {}, \"cc20\": {}, \"cc21\": {}, \"cc22\": {}, \"cc23\": {}, \"cc24\": {}, \"cc25\": {}, \"cc26\": {}, \"cc27\": {}, \"cc28\": {}, \"cc29\": {}, \"cc30\": {}, \"cc31\": {}, \"cc32\": {}, \"cc33\": {}, \"cc34\": {}, \"cc35\": {}, \"cc36\": {}, \"cc37\": {}, \"cc38\": {}, \"cc39\": {}, \"cc40\": {}, \"cc41\": {}, \"cc42\": {}, \"cc43\": {}, \"cc44\": {}, \"cc45\": {}, \"cc46\": {}, \"cc47\": {}, \"cc48\": {}, \"cc49\": {}}, \"2.5e-05\": {\"cc0\": {}, \"cc1\": {}, \"cc2\": {}, \"cc3\": {}, \"cc4\": {}, \"cc5\": {}, \"cc6\": {}, \"cc7\": {}, \"cc8\": {}, \"cc9\": {}, \"cc10\": {}, \"cc11\": {}, \"cc12\": {}, \"cc13\": {}, \"cc14\": {}, \"cc15\": {}, \"cc16\": {}, \"cc17\": {}, \"cc18\": {}, \"cc19\": {}, \"cc20\": {}, \"cc21\": {}, \"cc22\": {}, \"cc23\": {}, \"cc24\": {}, \"cc25\": {}, \"cc26\": {\"26\": {\"ori\": 41, \"same\": [50, 26], \"diff\": []}}, \"cc27\": {}, \"cc28\": {}, \"cc29\": {}, \"cc30\": {}, \"cc31\": {}, \"cc32\": {}, \"cc33\": {}, \"cc34\": {}, \"cc35\": {}, \"cc36\": {}, \"cc37\": {}, \"cc38\": {}, \"cc39\": {}, \"cc40\": {}, \"cc41\": {}, \"cc42\": {}, \"cc43\": {}, \"cc44\": {}, \"cc45\": {}, \"cc46\": {}, \"cc47\": {}, \"cc48\": {}, \"cc49\": {}}, \"5e-05\": {\"cc0\": {}, \"cc1\": {}, \"cc2\": {}, \"cc3\": {}, \"cc4\": {}, \"cc5\": {}, \"cc6\": {}, \"cc7\": {}, \"cc8\": {}, \"cc9\": {}, \"cc10\": {}, \"cc11\": {}, \"cc12\": {}, \"cc13\": {}, \"cc14\": {}, \"cc15\": {}, \"cc16\": {}, \"cc17\": {}, \"cc18\": {}, \"cc19\": {}, \"cc20\": {}, \"cc21\": {}, \"cc22\": {}, \"cc23\": {}, \"cc24\": {}, \"cc25\": {\"25\": {\"ori\": 25, \"same\": [51, 25], \"diff\": []}},\n</code></pre>"}]}